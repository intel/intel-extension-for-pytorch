<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Auto Mixed Precision (AMP) &mdash; intel_extension_for_pytorch 1.12.0+cpu documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Graph Optimization" href="graph_optimization.html" />
    <link rel="prev" title="Channels Last" href="nhwc.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> intel_extension_for_pytorch
          </a>
              <div class="version">
                <a href="../../../../versions.html">1.12.0+cpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../features.html">Features</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../features.html#ease-of-use-python-api">Ease-of-use Python API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#isa-dynamic-dispatching">ISA Dynamic Dispatching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#channels-last">Channels Last</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../features.html#auto-mixed-precision-amp">Auto Mixed Precision (AMP)</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Auto Mixed Precision (AMP)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#use-case">Use Case</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#default-precision">Default Precision</a></li>
<li class="toctree-l5"><a class="reference internal" href="#inference-with-imperative-path">Inference with Imperative Path</a></li>
<li class="toctree-l5"><a class="reference internal" href="#inference-with-torchscript-path">Inference with TorchScript Path</a></li>
<li class="toctree-l5"><a class="reference internal" href="#training-support">Training Support</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#autocast-op-reference">Autocast Op Reference</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#op-eligibility">Op Eligibility</a></li>
<li class="toctree-l5"><a class="reference internal" href="#op-specific-behavior">Op-Specific Behavior</a><ul>
<li class="toctree-l6"><a class="reference internal" href="#ops-that-can-autocast-to-bfloat16">Ops that can autocast to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code></a></li>
<li class="toctree-l6"><a class="reference internal" href="#ops-that-can-autocast-to-float32">Ops that can autocast to <code class="docutils literal notranslate"><span class="pre">float32</span></code></a></li>
<li class="toctree-l6"><a class="reference internal" href="#ops-that-promote-to-the-widest-input-type">Ops that promote to the widest input type</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#graph-optimization">Graph Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#operator-optimization">Operator Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#optimizer-optimization">Optimizer Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#runtime-extension">Runtime Extension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#int8-quantization">INT8 Quantization</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance_tuning.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">intel_extension_for_pytorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../features.html">Features</a> &raquo;</li>
      <li>Auto Mixed Precision (AMP)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/features/amp.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="auto-mixed-precision-amp">
<h1>Auto Mixed Precision (AMP)<a class="headerlink" href="#auto-mixed-precision-amp" title="Permalink to this headline"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline"></a></h2>
<p><code class="docutils literal notranslate"><span class="pre">torch.cpu.amp</span></code> provides convenience for auto data type conversion at runtime. Deep learning workloads can benefit from lower-precision floating point data types such as <code class="docutils literal notranslate"><span class="pre">torch.float16</span></code> or <code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code>, because of its lighter calculation workload and smaller memory usage. Accuracy is sacrificed when using lower-precision floating point data types so there’s a trade-off between accuracy and performance. Thus, some operations should use the slower but more accurate<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>, while others can be converted to use the faster but less accurate <code class="docutils literal notranslate"><span class="pre">torch.float16</span></code> data type. The Auto Mixed Precision (AMP) feature automates the tuning of data type conversions over all operators.</p>
<p><code class="docutils literal notranslate"><span class="pre">torch.cpu.amp</span></code> only supports <code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code>. It is the default lower precision floating point data type when <code class="docutils literal notranslate"><span class="pre">torch.cpu.amp</span></code> is enabled. <code class="docutils literal notranslate"><span class="pre">torch.cpu.amp</span></code> primarily benefits when running on Intel CPU with BFloat16 instruction set support.</p>
</section>
<section id="use-case">
<h2>Use Case<a class="headerlink" href="#use-case" title="Permalink to this headline"></a></h2>
<p>The following simple network should show a speedup with mixed precision.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">SimpleNet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<section id="default-precision">
<h3>Default Precision<a class="headerlink" href="#default-precision" title="Permalink to this headline"></a></h3>
<p>Without <code class="docutils literal notranslate"><span class="pre">torch.cpu.amp</span></code>, the network executes all operators with default precision (<code class="docutils literal notranslate"><span class="pre">torch.float32</span></code>).</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNet</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="inference-with-imperative-path">
<h3>Inference with Imperative Path<a class="headerlink" href="#inference-with-imperative-path" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">torch.cpu.amp.autocast</span></code> is designed to be a context manager that allow scopes of your script to run with mixed precision. In these scopes, operations run in a data type chosen by the <code class="docutils literal notranslate"><span class="pre">autocast</span></code> class to improve performance while maintaining accuracy. See the operations category section for details on what precision the <code class="docutils literal notranslate"><span class="pre">autocast</span></code> class chooses for each operator, and under what circumstances.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNet</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">():</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="inference-with-torchscript-path">
<h3>Inference with TorchScript Path<a class="headerlink" href="#inference-with-torchscript-path" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">torch.cpu.amp.autocast</span></code> can be used with <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code> to apply graph optimization. Due to PyTorch limitation, only <code class="docutils literal notranslate"><span class="pre">torch.jit.trace</span></code> is supported.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNet</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-support">
<h3>Training Support<a class="headerlink" href="#training-support" title="Permalink to this headline"></a></h3>
<p><code class="docutils literal notranslate"><span class="pre">torch.cpu.amp.autocast</span></code> can be used in training to improve performance.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNet</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.001</span><span class="p">)</span>
<span class="k">for</span> <span class="n">images</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">train_loader</span><span class="p">():</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">():</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">),</span> <span class="n">label</span><span class="p">)</span>
    <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>
<section id="autocast-op-reference">
<h2>Autocast Op Reference<a class="headerlink" href="#autocast-op-reference" title="Permalink to this headline"></a></h2>
<section id="op-eligibility">
<h3>Op Eligibility<a class="headerlink" href="#op-eligibility" title="Permalink to this headline"></a></h3>
<p>Ops that run in <code class="docutils literal notranslate"><span class="pre">float64</span></code> or non-floating-point dtypes are not eligible for mixed precision, and will run in these types whether or not autocast is enabled.</p>
<p>Only out-of-place ops and Tensor methods are eligible for mixed precision. In-place variants and calls that explicitly supply an <code class="docutils literal notranslate"><span class="pre">out=...</span></code> Tensor
are allowed in autocast-enabled regions, but won’t go through autocasting. For example, in an autocast-enabled region <code class="docutils literal notranslate"><span class="pre">a.addmm(b,</span> <span class="pre">c)</span></code> can autocast, but <code class="docutils literal notranslate"><span class="pre">a.addmm_(b,</span> <span class="pre">c)</span></code> and <code class="docutils literal notranslate"><span class="pre">a.addmm(b,</span> <span class="pre">c,</span> <span class="pre">out=d)</span></code> cannot. For best performance and stability, use out-of-place ops in autocast-enabled regions.</p>
</section>
<section id="op-specific-behavior">
<h3>Op-Specific Behavior<a class="headerlink" href="#op-specific-behavior" title="Permalink to this headline"></a></h3>
<p>The following lists describe the behavior of eligible ops in autocast-enabled regions. These ops always go through autocasting whether they are invoked as part of a <code class="docutils literal notranslate"><span class="pre">torch.nn.Module</span></code>, as a function, or as a <code class="docutils literal notranslate"><span class="pre">torch.Tensor</span></code> method. If functions are exposed in multiple namespaces, they go through autocasting regardless of the namespace.</p>
<p>Ops not listed below do not go through autocasting. They run in the type defined by their inputs. However, autocasting may still change the type in which unlisted ops run if they’re downstream from autocasted ops.</p>
<p>If an op is unlisted, we assume it’s numerically stable in <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code>. If you believe that an unlisted op is numerically unstable in <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code>, file a <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/issues">GitHub issue</a>.</p>
<section id="ops-that-can-autocast-to-bfloat16">
<h4>Ops that can autocast to <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code><a class="headerlink" href="#ops-that-can-autocast-to-bfloat16" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">conv1d</span></code>, <code class="docutils literal notranslate"><span class="pre">conv2d</span></code>, <code class="docutils literal notranslate"><span class="pre">conv3d</span></code>, <code class="docutils literal notranslate"><span class="pre">bmm</span></code>, <code class="docutils literal notranslate"><span class="pre">mm</span></code>, <code class="docutils literal notranslate"><span class="pre">baddbmm</span></code>, <code class="docutils literal notranslate"><span class="pre">addmm</span></code>, <code class="docutils literal notranslate"><span class="pre">addbmm</span></code>, <code class="docutils literal notranslate"><span class="pre">linear</span></code>, <code class="docutils literal notranslate"><span class="pre">matmul</span></code>, <code class="docutils literal notranslate"><span class="pre">conv_tbc</span></code>, <code class="docutils literal notranslate"><span class="pre">group_norm</span></code>, <code class="docutils literal notranslate"><span class="pre">_native_multi_head_attention</span></code></p>
</section>
<section id="ops-that-can-autocast-to-float32">
<h4>Ops that can autocast to <code class="docutils literal notranslate"><span class="pre">float32</span></code><a class="headerlink" href="#ops-that-can-autocast-to-float32" title="Permalink to this headline"></a></h4>
<p><code class="docutils literal notranslate"><span class="pre">conv_transpose1d</span></code>, <code class="docutils literal notranslate"><span class="pre">conv_transpose2d</span></code>, <code class="docutils literal notranslate"><span class="pre">conv_transpose3d</span></code>, <code class="docutils literal notranslate"><span class="pre">mish</span></code>, <code class="docutils literal notranslate"><span class="pre">avg_pool3d</span></code>, <code class="docutils literal notranslate"><span class="pre">binary_cross_entropy</span></code>, <code class="docutils literal notranslate"><span class="pre">grid_sampler</span></code>, <code class="docutils literal notranslate"><span class="pre">polar</span></code>, <code class="docutils literal notranslate"><span class="pre">prod</span></code>, <code class="docutils literal notranslate"><span class="pre">quantile</span></code>, <code class="docutils literal notranslate"><span class="pre">nanquantile</span></code>, <code class="docutils literal notranslate"><span class="pre">stft</span></code>, <code class="docutils literal notranslate"><span class="pre">cdist</span></code>, <code class="docutils literal notranslate"><span class="pre">trace</span></code>, <code class="docutils literal notranslate"><span class="pre">view_as_complex</span></code>, <code class="docutils literal notranslate"><span class="pre">cholesky</span></code>, <code class="docutils literal notranslate"><span class="pre">cholesky_inverse</span></code>, <code class="docutils literal notranslate"><span class="pre">cholesky_solve</span></code>, <code class="docutils literal notranslate"><span class="pre">inverse</span></code>, <code class="docutils literal notranslate"><span class="pre">lu_solve</span></code>, <code class="docutils literal notranslate"><span class="pre">matrix_rank</span></code>, <code class="docutils literal notranslate"><span class="pre">orgqr</span></code>, <code class="docutils literal notranslate"><span class="pre">ormqr</span></code>, <code class="docutils literal notranslate"><span class="pre">pinverse</span></code>, <code class="docutils literal notranslate"><span class="pre">max_pool3d</span></code>, <code class="docutils literal notranslate"><span class="pre">max_unpool2d</span></code>, <code class="docutils literal notranslate"><span class="pre">max_unpool3d</span></code>, <code class="docutils literal notranslate"><span class="pre">adaptive_avg_pool3d</span></code>, <code class="docutils literal notranslate"><span class="pre">reflection_pad1d</span></code>, <code class="docutils literal notranslate"><span class="pre">reflection_pad2d</span></code>, <code class="docutils literal notranslate"><span class="pre">replication_pad1d</span></code>, <code class="docutils literal notranslate"><span class="pre">replication_pad2d</span></code>, <code class="docutils literal notranslate"><span class="pre">replication_pad3d</span></code>, <code class="docutils literal notranslate"><span class="pre">mse_loss</span></code>, <code class="docutils literal notranslate"><span class="pre">ctc_loss</span></code>, <code class="docutils literal notranslate"><span class="pre">kl_div</span></code>, <code class="docutils literal notranslate"><span class="pre">multilabel_margin_loss</span></code>, <code class="docutils literal notranslate"><span class="pre">fft_fft</span></code>, <code class="docutils literal notranslate"><span class="pre">fft_ifft</span></code>, <code class="docutils literal notranslate"><span class="pre">fft_fft2</span></code>, <code class="docutils literal notranslate"><span class="pre">fft_ifft2</span></code>, <code class="docutils literal notranslate"><span class="pre">fft_fftn</span></code>, <code class="docutils literal notranslate"><span class="pre">fft_ifftn</span></code>, <code class="docutils literal notranslate"><span class="pre">fft_rfft</span></code>, <code class="docutils literal notranslate"><span class="pre">fft_irfft</span></code>, <code class="docutils literal notranslate"><span class="pre">fft_rfft2</span></code>, <code class="docutils literal notranslate"><span class="pre">fft_irfft2</span></code>, <code class="docutils literal notranslate"><span class="pre">fft_rfftn</span></code>, <code class="docutils literal notranslate"><span class="pre">fft_irfftn</span></code>, <code class="docutils literal notranslate"><span class="pre">fft_hfft</span></code>, <code class="docutils literal notranslate"><span class="pre">fft_ihfft</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_matrix_norm</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_cond</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_matrix_rank</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_solve</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_cholesky</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_svdvals</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_eigvals</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_eigvalsh</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_inv</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_householder_product</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_tensorinv</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_tensorsolve</span></code>, <code class="docutils literal notranslate"><span class="pre">fake_quantize_per_tensor_affine</span></code>, <code class="docutils literal notranslate"><span class="pre">eig</span></code>, <code class="docutils literal notranslate"><span class="pre">geqrf</span></code>, <code class="docutils literal notranslate"><span class="pre">lstsq</span></code>, <code class="docutils literal notranslate"><span class="pre">_lu_with_info</span></code>, <code class="docutils literal notranslate"><span class="pre">qr</span></code>, <code class="docutils literal notranslate"><span class="pre">svd</span></code>, <code class="docutils literal notranslate"><span class="pre">symeig</span></code>, <code class="docutils literal notranslate"><span class="pre">triangular_solve</span></code>, <code class="docutils literal notranslate"><span class="pre">fractional_max_pool2d</span></code>, <code class="docutils literal notranslate"><span class="pre">fractional_max_pool3d</span></code>, <code class="docutils literal notranslate"><span class="pre">adaptive_max_pool3d</span></code>, <code class="docutils literal notranslate"><span class="pre">multilabel_margin_loss_forward</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_qr</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_cholesky_ex</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_svd</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_eig</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_eigh</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_lstsq</span></code>, <code class="docutils literal notranslate"><span class="pre">linalg_inv_ex</span></code></p>
</section>
<section id="ops-that-promote-to-the-widest-input-type">
<h4>Ops that promote to the widest input type<a class="headerlink" href="#ops-that-promote-to-the-widest-input-type" title="Permalink to this headline"></a></h4>
<p>These ops don’t require a particular dtype for stability, but take multiple inputs and require that the inputs’ dtypes match.  If all of the inputs are <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code>, the op runs in <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code>.  If any of the inputs is <code class="docutils literal notranslate"><span class="pre">float32</span></code>, autocast casts all inputs to <code class="docutils literal notranslate"><span class="pre">float32</span></code> and runs the op in <code class="docutils literal notranslate"><span class="pre">float32</span></code>.</p>
<p><code class="docutils literal notranslate"><span class="pre">cat</span></code>, <code class="docutils literal notranslate"><span class="pre">stack</span></code>, <code class="docutils literal notranslate"><span class="pre">index_copy</span></code></p>
<p>Some ops not listed here (e.g., binary ops like <code class="docutils literal notranslate"><span class="pre">add</span></code>) natively promote inputs without autocasting’s intervention.  If inputs are a mixture of <code class="docutils literal notranslate"><span class="pre">bfloat16</span></code> and <code class="docutils literal notranslate"><span class="pre">float32</span></code>, these ops run in <code class="docutils literal notranslate"><span class="pre">float32</span></code> and produce <code class="docutils literal notranslate"><span class="pre">float32</span></code> output, regardless of whether autocast is enabled.</p>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="nhwc.html" class="btn btn-neutral float-left" title="Channels Last" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="graph_optimization.html" class="btn btn-neutral float-right" title="Graph Optimization" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Intel(R).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>