

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Codeless Optimization (Prototype) &mdash; Intel&amp;#174 Extension for PyTorch* 2.8.0+cpu documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=01a6a0bb" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-pytorch"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Graph Capture (Prototype)" href="graph_capture.html" />
    <link rel="prev" title="Smooth Quant Recipe Tuning API (Prototype)" href="sq_recipe_tuning_api.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel&#174 Extension for PyTorch*
          </a>
<div class="version">
            <a href="../../../../">2.8.0+cpu ▼</a>
            <p>Click link above to switch version</p>
          </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ABOUT</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../features.html">Features</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../features.html#easy-to-use-python-api">Easy-to-use Python API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#large-language-models-llm-new-feature-from-2-1-0">Large Language Models (LLM, <em>NEW feature from 2.1.0</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#torch-compile-beta-new-feature-from-2-0-0">torch.compile (Beta, <em>NEW feature from 2.0.0</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#isa-dynamic-dispatching">ISA Dynamic Dispatching</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#auto-channels-last">Auto Channels Last</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#auto-mixed-precision-amp">Auto Mixed Precision (AMP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#graph-optimization">Graph Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#operator-optimization">Operator Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#runtime-extension">Runtime Extension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#int8-quantization">INT8 Quantization</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../features.html#codeless-optimization-prototype-new-feature-from-1-13-0">Codeless Optimization (Prototype, <em>NEW feature from 1.13.0</em>)</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Codeless Optimization (Prototype)</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#motivation">Motivation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#example-usage-with-huggingface">Example Usage with HuggingFace</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#the-origin-command-with-ipex-launch">The origin command with ipex launch</a></li>
<li class="toctree-l5"><a class="reference internal" href="#command-to-apply-ipex-optimization-for-fp32">Command to apply ipex optimization for FP32</a></li>
<li class="toctree-l5"><a class="reference internal" href="#command-to-apply-ipex-optimization-for-bf16">Command to apply ipex optimization for BF16</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#use-case-not-supported">Use Case not supported</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#module-uses-forward-method-explicitly-instead-of-the-call-attr">Module uses forward method explicitly instead of the <code class="docutils literal notranslate"><span class="pre">__call__</span></code> attr</a></li>
<li class="toctree-l5"><a class="reference internal" href="#already-using-ipex-optimize">Already using <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code></a></li>
<li class="toctree-l5"><a class="reference internal" href="#already-using-jit-trace">Already using Jit Trace</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#graph-capture-prototype-new-feature-from-1-13-0">Graph Capture (Prototype, <em>NEW feature from 1.13.0</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#hypertune-prototype-new-feature-from-1-13-0">HyperTune (Prototype, <em>NEW feature from 1.13.0</em>)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#fast-bert-optimization-prototype-new-feature-from-2-0-0">Fast BERT Optimization (Prototype, <em>NEW feature from 2.0.0</em>)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../llm.html">Large Language Models (LLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../known_issues.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheat_sheet.html">Cheat Sheet</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PERFORMANCE TUNING</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../performance_tuning/tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance_tuning/launch_script.html">Launch Script Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance_tuning/torchserve.html">TorchServe with Intel® Extension for PyTorch*</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CONTRIBUTING GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel&#174 Extension for PyTorch*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../features.html">Features</a></li>
      <li class="breadcrumb-item active">Codeless Optimization (Prototype)</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/features/codeless_optimization.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="codeless-optimization-prototype">
<h1>Codeless Optimization (Prototype)<a class="headerlink" href="#codeless-optimization-prototype" title="Link to this heading"></a></h1>
<p>This feature aims to get inference performance benefits from Intel® Extension for PyTorch* without changing code in your python scripts, which can raise Out-of-Box (OOB) experience to get started with Intel® Extension for PyTorch* easily. Users who already known how to apply optimizations with Intel® Extension for PyTorch* APIs are not targeted for this feature, due to the inevitable overhead and limitations we mentioned below.</p>
<section id="motivation">
<h2>Motivation<a class="headerlink" href="#motivation" title="Link to this heading"></a></h2>
<p>A typical use case of inference as in <a class="reference external" href="https://github.com/huggingface/transformers/blob/v4.21.1/src/transformers/trainer.py#L3187">transformer</a> can be simplified as the code snippet below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">():</span>
        <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p>To utilize optimizations of Intel® Extension for PyTorch* for optimum performance, several lines code changes are required/recommended.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span> <span class="c1"># clause added</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimization</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>          <span class="c1"># clause added</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
  <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">():</span>          <span class="c1"># clause added for running with BFloat16 (Optional)</span>
    <span class="nb">input</span> <span class="o">=</span> <span class="o">...</span>                           <span class="c1"># clause added for TorchScript (Optional, but recommended) </span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>        <span class="c1"># clause added for TorchScript (Optional, but recommended) </span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>            <span class="c1"># clause added for TorchScript (Optional, but recommended) </span>
    <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">():</span>
      <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p>With this feature, code changes above done manually are not required any more. Intel® Extension for PyTorch* optimizations will be applied automatically during execution in a monkey patch way.</p>
<ul class="simple">
<li><p>Automatically import <code class="docutils literal notranslate"><span class="pre">intel_extension_for_pytorch</span></code> package: It applies Intel® Extension for PyTorch* optimizations, such as: <code class="docutils literal notranslate"><span class="pre">torch.embedding_bag</span></code>, <code class="docutils literal notranslate"><span class="pre">torch.cpu.amp.autocast</span></code>. It also registers Intel® Extension for PyTorch* JIT fusion pass and thus benefits the graph mode inference performance.</p></li>
<li><p>Automatically apply <code class="docutils literal notranslate"><span class="pre">ipex.optimize()</span></code> function. Only features enabled by default parameter values are supported, such as:</p>
<ul>
<li><p>Auto generate FX or Jit Graph.</p></li>
<li><p>Auto Channel Last convert.</p></li>
<li><p>Conv-Bn folding.</p></li>
<li><p>Weight prepack.</p></li>
<li><p>Replace dropout with identity.</p></li>
<li><p>Optimize LSTM.</p></li>
</ul>
</li>
<li><p>Automatically apply <code class="docutils literal notranslate"><span class="pre">torch.cpu.amp.autocast</span></code> with BFloat16 data type for inference.</p></li>
</ul>
</section>
<section id="example-usage-with-huggingface">
<h2>Example Usage with HuggingFace<a class="headerlink" href="#example-usage-with-huggingface" title="Link to this heading"></a></h2>
<p>Let’s take the <a class="reference external" href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering">QA case</a> in HuggingFace as an example.</p>
<section id="the-origin-command-with-ipex-launch">
<h3>The origin command with ipex launch<a class="headerlink" href="#the-origin-command-with-ipex-launch" title="Link to this heading"></a></h3>
<p>Here is the command to run with <a class="reference internal" href="../performance_tuning/launch_script.html"><span class="doc">ipexrun</span></a>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">clear</span> <span class="o">&amp;&amp;</span> <span class="n">ipexrun</span> <span class="o">--</span><span class="n">memory</span><span class="o">-</span><span class="n">allocator</span> <span class="n">default</span> <span class="o">--</span><span class="n">ninstances</span> <span class="mi">2</span> <span class="o">--</span><span class="n">ncores</span><span class="o">-</span><span class="n">per</span><span class="o">-</span><span class="n">instance</span> <span class="mi">28</span> <span class="n">run_qa</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">model_name_or_path</span> <span class="n">bert</span><span class="o">-</span><span class="n">base</span><span class="o">-</span><span class="n">uncased</span> <span class="o">--</span><span class="n">dataset_name</span> <span class="n">squad</span> <span class="o">--</span><span class="n">do_eval</span> <span class="o">--</span><span class="n">per_device_train_batch_size</span> <span class="mi">12</span> <span class="o">--</span><span class="n">learning_rate</span> <span class="mf">3e-5</span> <span class="o">--</span><span class="n">num_train_epochs</span> <span class="mi">2</span> <span class="o">--</span><span class="n">max_seq_length</span> <span class="mi">384</span> <span class="o">--</span><span class="n">doc_stride</span> <span class="mi">128</span> <span class="o">--</span><span class="n">output_dir</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">debug_squad</span><span class="o">/</span>
</pre></div>
</div>
</section>
<section id="command-to-apply-ipex-optimization-for-fp32">
<h3>Command to apply ipex optimization for FP32<a class="headerlink" href="#command-to-apply-ipex-optimization-for-fp32" title="Link to this heading"></a></h3>
<p>Added <code class="docutils literal notranslate"><span class="pre">--auto-ipex</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">clear</span> <span class="o">&amp;&amp;</span> <span class="n">ipexrun</span> <span class="o">--</span><span class="n">memory</span><span class="o">-</span><span class="n">allocator</span> <span class="n">default</span> <span class="o">--</span><span class="n">ninstances</span> <span class="mi">2</span> <span class="o">--</span><span class="n">ncores</span><span class="o">-</span><span class="n">per</span><span class="o">-</span><span class="n">instance</span> <span class="mi">28</span> <span class="o">--</span><span class="n">auto</span><span class="o">-</span><span class="n">ipex</span> <span class="n">run_qa</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">model_name_or_path</span> <span class="n">bert</span><span class="o">-</span><span class="n">base</span><span class="o">-</span><span class="n">uncased</span> <span class="o">--</span><span class="n">dataset_name</span> <span class="n">squad</span> <span class="o">--</span><span class="n">do_eval</span> <span class="o">--</span><span class="n">per_device_train_batch_size</span> <span class="mi">12</span> <span class="o">--</span><span class="n">learning_rate</span> <span class="mf">3e-5</span> <span class="o">--</span><span class="n">num_train_epochs</span> <span class="mi">2</span> <span class="o">--</span><span class="n">max_seq_length</span> <span class="mi">384</span> <span class="o">--</span><span class="n">doc_stride</span> <span class="mi">128</span> <span class="o">--</span><span class="n">output_dir</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">debug_squad</span><span class="o">/</span>
</pre></div>
</div>
</section>
<section id="command-to-apply-ipex-optimization-for-bf16">
<h3>Command to apply ipex optimization for BF16<a class="headerlink" href="#command-to-apply-ipex-optimization-for-bf16" title="Link to this heading"></a></h3>
<p>Added <code class="docutils literal notranslate"><span class="pre">--auto-ipex</span> <span class="pre">--dtype</span> <span class="pre">bfloat16</span></code></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">clear</span> <span class="o">&amp;&amp;</span> <span class="n">ipexrun</span> <span class="o">--</span><span class="n">memory</span><span class="o">-</span><span class="n">allocator</span> <span class="n">default</span> <span class="o">--</span><span class="n">ninstances</span> <span class="mi">2</span> <span class="o">--</span><span class="n">ncores</span><span class="o">-</span><span class="n">per</span><span class="o">-</span><span class="n">instance</span> <span class="mi">28</span> <span class="o">--</span><span class="n">auto</span><span class="o">-</span><span class="n">ipex</span> <span class="o">--</span><span class="n">dtype</span> <span class="n">bfloat16</span> <span class="n">run_qa</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">model_name_or_path</span> <span class="n">bert</span><span class="o">-</span><span class="n">base</span><span class="o">-</span><span class="n">uncased</span> <span class="o">--</span><span class="n">dataset_name</span> <span class="n">squad</span> <span class="o">--</span><span class="n">do_eval</span> <span class="o">--</span><span class="n">per_device_train_batch_size</span> <span class="mi">12</span> <span class="o">--</span><span class="n">learning_rate</span> <span class="mf">3e-5</span> <span class="o">--</span><span class="n">num_train_epochs</span> <span class="mi">2</span> <span class="o">--</span><span class="n">max_seq_length</span> <span class="mi">384</span> <span class="o">--</span><span class="n">doc_stride</span> <span class="mi">128</span> <span class="o">--</span><span class="n">output_dir</span> <span class="o">/</span><span class="n">tmp</span><span class="o">/</span><span class="n">debug_squad</span><span class="o">/</span>
</pre></div>
</div>
</section>
</section>
<section id="use-case-not-supported">
<h2>Use Case not supported<a class="headerlink" href="#use-case-not-supported" title="Link to this heading"></a></h2>
<section id="module-uses-forward-method-explicitly-instead-of-the-call-attr">
<h3>Module uses forward method explicitly instead of the <code class="docutils literal notranslate"><span class="pre">__call__</span></code> attr<a class="headerlink" href="#module-uses-forward-method-explicitly-instead-of-the-call-attr" title="Link to this heading"></a></h3>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="k">class</span><span class="w"> </span><span class="nc">DummyModule</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DummyModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">7</span><span class="p">,</span> <span class="mi">7</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-05</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">affine</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">track_running_stats</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">customized_forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="c1"># Method1 will success</span>
<span class="n">DummyModule</span><span class="p">()(</span><span class="nb">input</span><span class="p">)</span>
<span class="c1"># Method2 will fail to apply ipex.optimize in the top-level model</span>
<span class="n">DummyModule</span><span class="p">()</span><span class="o">.</span><span class="n">customized_forward</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p>If a model uses forward method explicitly instead of the <code class="docutils literal notranslate"><span class="pre">__call__</span></code> attr, we are unable to hook the execution of this model. As result, we are unable to auto apply the optimizations to this <code class="docutils literal notranslate"><span class="pre">DummyModule()</span></code>.</p>
</section>
<section id="already-using-ipex-optimize">
<h3>Already using <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code><a class="headerlink" href="#already-using-ipex-optimize" title="Link to this heading"></a></h3>
<p>User already invokes <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> in script is not targeted for this feature. The behaviour as repeated invoking of <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> is not defined. The second invoking of <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> for the same module will fail with error message to avoid this behaviour.</p>
</section>
<section id="already-using-jit-trace">
<h3>Already using Jit Trace<a class="headerlink" href="#already-using-jit-trace" title="Link to this heading"></a></h3>
<p>For Jit trace case (as below example code) is not planned to support at first stage:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">traced_model</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="nb">input</span> <span class="ow">in</span> <span class="n">dataloader</span><span class="p">():</span>
        <span class="n">traced_model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p>For 2 reasons:</p>
<ul class="simple">
<li><p>The auto graph mode support has already been included in <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> with graph first API in 1.13.</p></li>
<li><p>Extra launch parameters and Monkey patches are needed to support above case. We will focus on the feasibility of first use case in TorchVision and HuggingFace workloads.</p></li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="sq_recipe_tuning_api.html" class="btn btn-neutral float-left" title="Smooth Quant Recipe Tuning API (Prototype)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="graph_capture.html" class="btn btn-neutral float-right" title="Graph Capture (Prototype)" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7cbf911091e0> 
<p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a><a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a><a href='/#' data-wap_ref='dns' id='wap_dns'><small>| Your Privacy Choices</small><span style='height:10px;width:28px;display:inline-block;position:relative;'><svg style='position:absolute;width:28px;bottom:-2px;' version='1.1' id='Layer_1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' x='0px' y='0px' viewBox='0 0 30 14' xml:space='preserve'><title>California Consumer Privacy Act (CCPA) Opt-Out Icon</title><style type='text/css'> .st0 { fill-rule: evenodd; clip-rule: evenodd; fill: #FFFFFF; } .st1 { fill-rule: evenodd; clip-rule: evenodd; fill: #0066FF; } .st2 { fill: #FFFFFF; } .st3 { fill: #0066FF; } </style><g><g id='final---dec.11-2020_1_'><g id='_x30_208-our-toggle_2_' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2_2_' transform='translate(1275.000000, 200.000000)'><path class='st0' d='M7.4,12.8h6.8l3.1-11.6H7.4C4.2,1.2,1.6,3.8,1.6,7S4.2,12.8,7.4,12.8z'></path></g></g></g><g id='final---dec.11-2020'><g id='_x30_208-our-toggle' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2' transform='translate(1275.000000, 200.000000)'><path class='st1' d='M22.6,0H7.4c-3.9,0-7,3.1-7,7s3.1,7,7,7h15.2c3.9,0,7-3.1,7-7S26.4,0,22.6,0z M1.6,7c0-3.2,2.6-5.8,5.8-5.8 h9.9l-3.1,11.6H7.4C4.2,12.8,1.6,10.2,1.6,7z'></path><path id='x' class='st2' d='M24.6,4c0.2,0.2,0.2,0.6,0,0.8l0,0L22.5,7l2.2,2.2c0.2,0.2,0.2,0.6,0,0.8c-0.2,0.2-0.6,0.2-0.8,0 l0,0l-2.2-2.2L19.5,10c-0.2,0.2-0.6,0.2-0.8,0c-0.2-0.2-0.2-0.6,0-0.8l0,0L20.8,7l-2.2-2.2c-0.2-0.2-0.2-0.6,0-0.8 c0.2-0.2,0.6-0.2,0.8,0l0,0l2.2,2.2L23.8,4C24,3.8,24.4,3.8,24.6,4z'></path><path id='y' class='st3' d='M12.7,4.1c0.2,0.2,0.3,0.6,0.1,0.8l0,0L8.6,9.8C8.5,9.9,8.4,10,8.3,10c-0.2,0.1-0.5,0.1-0.7-0.1l0,0 L5.4,7.7c-0.2-0.2-0.2-0.6,0-0.8c0.2-0.2,0.6-0.2,0.8,0l0,0L8,8.6l3.8-4.5C12,3.9,12.4,3.9,12.7,4.1z'></path></g></g></g></g></svg></span></a><a href=https://www.intel.com/content/www/us/en/privacy/privacy-residents-certain-states.html data-wap_ref='nac' id='wap_nac'><small>| Notice at Collection</small></a></div><p></p><div>&copy; Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others. No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (OBSD), <a href='http://opensource.org/licenses/0BSD'>http://opensource.org/licenses/0BSD</a>.</div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>