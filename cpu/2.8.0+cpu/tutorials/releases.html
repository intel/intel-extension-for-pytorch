

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Releases &mdash; Intel&amp;#174 Extension for PyTorch* 2.8.0+cpu documentation</title>
      <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=01a6a0bb" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-pytorch"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Troubleshooting" href="known_issues.html" />
    <link rel="prev" title="Performance" href="performance.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Intel&#174 Extension for PyTorch*
          </a>
<div class="version">
            <a href="../../../">2.8.0+cpu ▼</a>
            <p>Click link above to switch version</p>
          </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ABOUT</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="llm.html">Large Language Models (LLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance.html">Performance</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Releases</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id1">2.8.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#highlights">Highlights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id2">2.7.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id3">Highlights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id4">2.6.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id5">Highlights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id6">2.5.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id7">Highlights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id8">2.4.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id9">Highlights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id10">2.3.100</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id11">Highlights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id12">2.3.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id13">Highlights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id14">2.2.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id15">Highlights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id16">2.1.100</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id17">Highlights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id18">2.1.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id19">Highlights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id20">2.0.100</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id21">Highlights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id22">2.0.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id23">Highlights</a></li>
<li class="toctree-l3"><a class="reference internal" href="#known-issues">Known Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id24">1.13.100</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id25">Highlights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id26">1.13.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id27">Highlights</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id28">Known Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id29">1.12.300</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id30">Highlights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id31">1.12.100</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id32">1.12.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id33">Highlights</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id34">Known Issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id35">1.11.200</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id36">Highlights</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id37">1.11.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id38">Highlights</a></li>
<li class="toctree-l3"><a class="reference internal" href="#what-s-changed">What’s Changed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id39">1.10.100</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id40">1.10.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id41">Highlights</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id42">Known Issues</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id43">What’s Changed</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id44">1.9.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#what-s-new">What’s New</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id45">1.8.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id46">What’s New</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id47">1.2.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id48">What’s New</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performance-improvement">Performance Improvement</a></li>
<li class="toctree-l3"><a class="reference internal" href="#others">Others</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id49">Known issues</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id50">1.1.0</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id51">What’s New</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performance">Performance</a></li>
<li class="toctree-l3"><a class="reference internal" href="#known-issue">Known issue</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id52">1.0.2</a></li>
<li class="toctree-l2"><a class="reference internal" href="#alpha">1.0.1-Alpha</a></li>
<li class="toctree-l2"><a class="reference internal" href="#id53">1.0.0-Alpha</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id54">What’s New</a></li>
<li class="toctree-l3"><a class="reference internal" href="#performance-result">Performance Result</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id55">Known issue</a></li>
<li class="toctree-l3"><a class="reference internal" href="#note">NOTE</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="known_issues.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="cheat_sheet.html">Cheat Sheet</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_doc.html">API Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PERFORMANCE TUNING</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning/tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning/launch_script.html">Launch Script Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning/torchserve.html">TorchServe with Intel® Extension for PyTorch*</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CONTRIBUTING GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contribution.html">Contribution</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Intel&#174 Extension for PyTorch*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Releases</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorials/releases.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="releases">
<h1>Releases<a class="headerlink" href="#releases" title="Link to this heading"></a></h1>
<section id="id1">
<h2>2.8.0<a class="headerlink" href="#id1" title="Link to this heading"></a></h2>
<p>We are excited to announce the release of Intel® Extension for PyTorch* 2.8.0+cpu which accompanies PyTorch 2.8. This release mainly brings you new LLM model optimization including Qwen3 and Whisper large-v3, enhancement of API for multi-LoRA inference kernels and optimizations of LLM generation sampler. This release also includes a set of bug fixing and small optimizations. We want to sincerely thank our dedicated community for your contributions.</p>
<p>Besides providing optimization in Intel® Extension for PyTorch*, over the past years, we have also upstreamed most of our features and optimizations for Intel® platforms into PyTorch* and will continue pushing remaining ones into PyTorch* in future. Moving forward, we will change our working model to prioritize developing new features and optimization directly in PyTorch*, and de-prioritize development in Intel® Extension for PyTorch*, effective after 2.8 release. We will continue providing critical bug fixes and security patches if needed throughout the PyTorch* 2.9 timeframe to ensure a smooth transition for our partners and community.</p>
<section id="highlights">
<h3>Highlights<a class="headerlink" href="#highlights" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Qwen3 support</p></li>
</ul>
<p><a class="reference external" href="https://qwenlm.github.io/blog/qwen3/">Qwen3</a> has recently been released, the latest addition to the Qwen family of large language models. Intel® Extension for PyTorch* provides <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-qwen3-large-language-models.html">support of Qwen3</a> since its launch date with early release version for MoE models like <a class="reference external" href="https://huggingface.co/Qwen/Qwen3-30B-A3B">Qwen3-30B</a> and middle-size dense model like <a class="reference external" href="https://huggingface.co/Qwen/Qwen3-14B">Qwen3-14B</a>. Related optimizations have been included in this official release.</p>
<ul class="simple">
<li><p>Whisper large-v3 support</p></li>
</ul>
<p>Intel® Extension for PyTorch* provides optimization for <a class="reference external" href="https://huggingface.co/openai/whisper-large-v3">whisper-large-v3</a>, a state-of-the-art model for automatic speech recognition (ASR) and speech translation. Key improvements include replacing the cross-attention mechanism with the Indirect Access Key-Value (IAKV) Cache kernel, bringing you well-performing experience with weight-only INT8 quantization on Intel® Xeon® processors.</p>
<ul class="simple">
<li><p>General Large Language Model (LLM) optimization</p></li>
</ul>
<p>Intel® Extension for PyTorch* provides sgmv support in the API for multi-LoRA inference kernels for LLM serving frameworks and optimizes the LLM generation sampler. A full list of optimized models can be found at <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/v2.8.0+cpu/examples/cpu/llm/inference">LLM optimization</a>.</p>
<ul class="simple">
<li><p>Bug fixing and other optimization</p>
<ul>
<li><p>Optimized the performance of LLM <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/9659e2e76c610c4a01d5579a9775c6a071679cb6">#3688</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/84834fbc1747458660b837491316b10e9a43e6d5">#3708</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/2978fd620fbdf347264a5e0d502cd019e1ab639b">#3754</a></p></li>
<li><p>Removed the dependency on torch-ccl and oneCCL <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/4d00e5a49b38257f28d13b076f2c8564740afa71">#3690</a></p></li>
</ul>
</li>
</ul>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v2.7.0+cpu…v2.8.0+cpu</p>
</section>
</section>
<section id="id2">
<h2>2.7.0<a class="headerlink" href="#id2" title="Link to this heading"></a></h2>
<p>We are excited to announce the release of Intel® Extension for PyTorch* 2.7.0+cpu which accompanies PyTorch 2.7. This release mainly brings you new LLM model optimization including DeepSeek-R1-671B and Phi-4, new APIs for LLM serving frameworks including sliding window and softcap support in PagedAttention APIs, MambaMixer API for Jamba and Mamba model and API for multi-LoRA inference kernels. This release also includes a set of bug fixing and small optimizations. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try this release and feedback as to improve further on this product.</p>
<section id="id3">
<h3>Highlights<a class="headerlink" href="#id3" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>DeepSeek-R1 support</p></li>
</ul>
<p>Intel® Extension for PyTorch* provides optimization for the hot <a class="reference external" href="https://github.com/deepseek-ai/DeepSeek-R1">DeepSeek-R1-671B</a> model. A few optimizations including Multi-Head Latent Attention (MLA), fused MoE, fused-shared-expert and MoEGate, brings you well-performing experience with INT8 precision on Intel® Xeon®.</p>
<ul class="simple">
<li><p>Phi-4 support</p></li>
</ul>
<p>Microsoft has recently released <a class="reference external" href="https://aka.ms/phi4-feb2025">Phi-4</a>, including <a class="reference external" href="https://ai.azure.com/explore/models/Phi-4-mini-instruct/version/1/registry/azureml">Phi-4-mini</a> (3.8B dense decoder-only transformer model) and <a class="reference external" href="https://ai.azure.com/explore/models/Phi-4-multimodal-instruct/version/1/registry/azureml">Phi-4-multimodal</a> (5.6B multimodal model). Intel® Extension for PyTorch* provides <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/accelerate-microsoft-phi-4-small-language-models.html">support of Phi-4</a> since its launch date with early release version, and the related optimizations are included in this official release.</p>
<ul class="simple">
<li><p>General Large Language Model (LLM) optimizations</p></li>
</ul>
<p>Intel® Extension for PyTorch* provides sliding window and softcap support in PagedAttention APIs, MambaMixer API for Jamba and Mamba model and API for multi-LoRA inference kernels for LLM serving frameworks. For user experience improvements, Intel® Extension for PyTorch* supports running INT4 workloads with only INT4 weights, removing the need of downloading the original high precision weights. A full list of optimized models can be found at <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/v2.7.0+cpu/examples/cpu/llm/inference">LLM optimization</a>.</p>
<ul class="simple">
<li><p>Bug fixing and other optimization</p>
<ul>
<li><p>Optimized the performance of LLM <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/30ecffa4db675594670be94e40167543729641a6">#3537</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/e11294db8fe76330965dbce3d94a7666811a6415">#3611</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/f20e0a251fefd55dad36ca21f3f027cfd2d50a44">#3549</a></p></li>
<li><p>Handled new linear modules in DeepSpeed v0.16.5 <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/78694e40c66747cc9ec34f9d043cb522188916ff">#3622</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/ec282f6d5e36b5904197cf7de22e421d0db61a61">#3638</a></p></li>
<li><p>Fixed PagedAttention kernel to avoid the graph break when using <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/c12230bd8c6a0223d7cbd27ef19549cc3bcd87df">#3641</a></p></li>
<li><p>Added user guide for running DeepSeek-R1<a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/94f887994846ae1fd4be691fd26f6308574a9ca9">#3660</a> and multimodal models <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/c2ccab24217858592029f70b5b3db9ff8f2287d0">#3649</a></p></li>
<li><p>Upgraded oneDNN to v3.7.2 <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/68ce64fa1cc28ec7119c163e1a29077f0bda3f43">#3582</a></p></li>
</ul>
</li>
</ul>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v2.6.0+cpu…v2.7.0+cpu</p>
</section>
</section>
<section id="id4">
<h2>2.6.0<a class="headerlink" href="#id4" title="Link to this heading"></a></h2>
<p>We are excited to announce the release of Intel® Extension for PyTorch* 2.6.0+cpu which accompanies PyTorch 2.6. This release mainly brings you full optimization on latest Intel® Xeon® 6 P-core platform, new LLM model support including Falcon3/Jamba/DeepSeek V2.5, and latest LLM optimization including FP8 KV cache, GPTQ/AWQ support under Tensor Parallel mode, and INT8 computation for WOQ. This release also includes a set of bug fixing and small optimizations. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try this release and feedback as to improve further on this product.</p>
<section id="id5">
<h3>Highlights<a class="headerlink" href="#id5" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Comprehensive optimization for Intel® Xeon® 6
Intel® Xeon® 6 deliver new degrees of performance with more cores, a choice of microarchitecture, additional memory bandwidth, and exceptional input/output (I/O) across a range of workloads. Intel® Extension for PyTorch* v2.5 introduced the basic optimization for Intel® Xeon® 6, while in this new release, we added more comprehensive optimization as reflected with a set of typical AI models like DLRM/Bert-Large/ViT/Stable Diffusion/LCM/GPT-J/Llama, etc.</p></li>
<li><p>Large Language Model (LLM) optimization:
Intel® Extension for PyTorch* provides more feature support of the weight only quantization including INT8 based computation by leveraging AMX-INT8 from Intel® Xeon® 6, and GPTQ/AWQ support under Tensor Parallel mode.  FP8 KV cache and FP16 general datatype support in LLM module API, etc.. These features enable better adoption of community model weight and provides better performance for low-precision scenarios. This release also extended the optimized models to include newly published models like Falcon3, DeepSeek V2.5 and Jamba. A full list of optimized models can be found at <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/v2.6.0+cpu/examples/cpu/llm/inference">LLM optimization</a>.</p></li>
<li><p>Bug fixing and other optimization</p>
<ul>
<li><p>Optimized the performance of LLM <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/db3e22beeee026cc07f547be2f4fafd2e370561c">#3420</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/18eeefa762433b9b2731cf55a916cfdb4c8ca4c4">#3441</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/950e509f73a28d1b9d480e397768fbeb931b82ab">#3406</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/fbd0c0334e18760dc7d626673da1d338471dfb80">#3376</a></p></li>
<li><p>Supported loading INT4 checkpoint with Tensor Parallel <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/ae09c58b33e7b052b7f8b7de19992341bdc0aa5d">#3328</a></p></li>
<li><p>Enabled TP=3 with INT4 checkpoint for Weight Only Quantization <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/3e27750ee03674a054f093a707e46a16727b3931">#3400</a></p></li>
<li><p>Supported sharding checkpoint with GPTQ policy <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/0e7ff79d4459a52dea57d35042f8c66c35bc295d">#3423</a></p></li>
<li><p>Enabled lowp-mode=INT8 for NF4 weight <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/1be1dc86a1bba2a03549f5529e7ddce971c4115b">#3395</a></p></li>
<li><p>Fixed the correctness issue in the Weight Only Quantization kernel for Llama3-11b-vision <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/4d0ff47291d35205dd2300b9cbdbdb03af3161d5">#3469</a></p></li>
<li><p>Upgraded oneDNN to v3.6.2 <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/9eeaac672431cfdc5835decc72b00c073e842bd4">#3399</a></p></li>
</ul>
</li>
</ul>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v2.5.0+cpu…v2.6.0+cpu</p>
</section>
</section>
<section id="id6">
<h2>2.5.0<a class="headerlink" href="#id6" title="Link to this heading"></a></h2>
<p>We are excited to announce the release of Intel® Extension for PyTorch* 2.5.0+cpu which accompanies PyTorch 2.5. This release mainly brings you the support for Llama3.2, optimization on newly launched Intel® Xeon® 6 P-core platform, GPTQ/AWQ format support, and latest optimization to push better performance for LLM models. This release also includes a set of bug fixing and small optimizations. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try this release and feedback as to improve further on this product.</p>
<section id="id7">
<h3>Highlights<a class="headerlink" href="#id7" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Llama 3.2 support</p></li>
</ul>
<p>Meta has newly released <a class="reference external" href="https://ai.meta.com/blog/llama-3-2-connect-2024-vision-edge-mobile-devices/">Llama 3.2</a>, which includes small and medium-sized vision LLMs (11B and 90B), and lightweight, text-only models (1B and 3B). Intel® Extension for PyTorch* provides <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-ai-solutions-support-the-new-llama-3-2-model.html">support of Llama 3.2</a> since its launch date with early release version, and now support with this official release.</p>
<ul class="simple">
<li><p>Optimization for Intel® Xeon® 6
Intel® Xeon® 6 deliver new degrees of performance with more cores, a choice of microarchitecture, additional memory bandwidth, and exceptional input/output (I/O) across a range of workloads. Intel® Extension for PyTorch* provides dedicated optimization on this new processor family for features like Multiplexed Rank DIMM (MRDIMM), SNC=3 scenario, etc..</p></li>
<li><p>Large Language Model (LLM) optimization:
Intel® Extension for PyTorch* provides more feature support of the weight only quantization including GPTQ/AWQ format support, symmetric quantization of activation and weight, and added chunked prefill/prefix prefill support in LLM module API, etc.. These features enable better adoption of community model weight and provides better performance for low-precision scenarios. This release also extended the optimized models to include newly published Llama 3.2 vision models. A full list of optimized models can be found at <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/v2.5.0+cpu/examples/cpu/llm/inference">LLM optimization</a>.</p></li>
<li><p>Bug fixing and other optimization</p>
<ul>
<li><p>Optimized the performance of the IndirectAccessKVCacheAttention kernel
<a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/8572e1faf97998783ea2a7fc6ee3094090feebc4">#3185</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/65e96630a2e17f7b762c5c765f10264ad08db098">#3209</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/a04214f7ab4e43648d75abdcf0fae53e5076be2b">#3214</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/f219012ab1babbc67c9b545fa7251cd981a2a3a2">#3218</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/9f6178eb028d36b3ed1f5985e57b7cf160acf38a">#3248</a></p></li>
<li><p>Fixed the Segmentation fault in the IndirectAccessKVCacheAttention kernel <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/bee5ab644086c9b25eb61916c6773932c74667d3">#3246</a></p></li>
<li><p>Fixed the correctness issue in the PagedAttention kernel for Llama-68M-Chat-v1 <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/638a7d26acb33af450ea9869b5b43ccdbe0e962b">#3307</a></p></li>
<li><p>Fixed the support in <code class="docutils literal notranslate"><span class="pre">ipex.llm.optimize</span></code> to ensure <code class="docutils literal notranslate"><span class="pre">model.generate</span></code> returns the correct output type when <code class="docutils literal notranslate"><span class="pre">return_dict_in_generate</span></code> is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>. <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/584a4e2e2c6193b926554f951d2608489cac5d7a">#3333</a></p></li>
<li><p>Optimized the performance of the Flash Attention kernel <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/8fb43ec45ed93b62efef07f4b2e8dcd7dd502b8b">#3291</a></p></li>
<li><p>Upgraded oneDNN to v3.6 <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/91639fa0812ee3c12c672002c2bf5cf1cac4bc0a">#3305</a></p></li>
</ul>
</li>
</ul>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v2.4.0+cpu…v2.5.0+cpu</p>
</section>
</section>
<section id="id8">
<h2>2.4.0<a class="headerlink" href="#id8" title="Link to this heading"></a></h2>
<p>We are excited to announce the release of Intel® Extension for PyTorch* 2.4.0+cpu which accompanies PyTorch 2.4. This release mainly brings you the support for Llama3.1, basic support for LLM serving frameworks like vLLM/TGI, and a set of optimization to push better performance for LLM models. This release also extends the list of optimized LLM models to a broader level and includes a set of bug fixing and small optimizations. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try this release and feedback as to improve further on this product.</p>
<section id="id9">
<h3>Highlights<a class="headerlink" href="#id9" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Llama 3.1 support</p></li>
</ul>
<p>Meta has newly released <a class="reference external" href="https://ai.meta.com/blog/meta-llama-3-1/">Llama 3.1</a> with new features like longer context length (128K) support. Intel® Extension for PyTorch*  provides <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-ai-solutions-support-meta-llama-3-1-launch.html">support of Llama 3.1</a> since its launch date with early release version, and now support with this official release.</p>
<ul class="simple">
<li><p>Serving framework support</p></li>
</ul>
<p>Typical LLM serving frameworks including vLLM, TGI can co-work with Intel® Extension for PyTorch* now which provides optimized performance for Xeon® Scalable CPUs. Besides the integration of LLM serving frameworks with ipex.llm module level APIs, we also continue optimizing the performance and quality of underneath Intel® Extension for PyTorch* operators such as paged attention and flash attention. We also provide new support in ipex.llm module level APIs for 4bits AWQ quantization based on weight only quantization, and distributed communications with shared memory optimization.</p>
<ul class="simple">
<li><p>Large Language Model (LLM) optimization:</p></li>
</ul>
<p>Intel® Extension for PyTorch* further optimized the performance of the weight only quantization kernels, enabled more fusion pattern variants for LLMs and extended the optimized models to include whisper, falcon-11b, Qwen2, and definitely Llama 3.1, etc. A full list of optimized models can be found at <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/v2.4.0+cpu/examples/cpu/llm/inference">LLM optimization</a>.</p>
<ul class="simple">
<li><p>Bug fixing and other optimization</p>
<ul>
<li><p>Fixed the quantization with auto-mixed-precision (AMP) mode of Qwen-7b <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/ad29b2346fe0b26e87e1aefc15e1eb25fb4b9b4d">#3030</a></p></li>
<li><p>Fixed the illegal memory access issue in the Flash Attention kernel <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/620a9bfd9db42813931a857e78fa3f5d298be200">#2987</a></p></li>
<li><p>Re-structured the paths of LLM example scripts <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/bee4a423d99b4dea7362d8cb31b1d48e38344a8f">#3080</a></p></li>
<li><p>Upgraded oneDNN to v3.5.2 <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/7911528f0fef4e1b493cb0b363bf76de2eb6a9ca">#3143</a></p></li>
<li><p>Misc fix and enhancement <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/e74d7a97186e6cafc8e41c2b40f03e95fe6c8060">#3079</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/76dfb92af8aa4778aff09a089bde70f614712b33">#3116</a></p></li>
</ul>
</li>
</ul>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v2.3.0+cpu…v2.4.0+cpu</p>
</section>
</section>
<section id="id10">
<h2>2.3.100<a class="headerlink" href="#id10" title="Link to this heading"></a></h2>
<section id="id11">
<h3>Highlights<a class="headerlink" href="#id11" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Added the optimization for Phi-3: <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/5fde074252d9b61dd0d410832724cbbec882cb96">#2883</a></p></li>
<li><p>Fixed the <code class="docutils literal notranslate"><span class="pre">state_dict</span></code> method patched by <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> to support DistributedDataParallel <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/9a192efa4cf9a9a2dabac19e57ec5d81f9f5d22c">#2910</a></p></li>
<li><p>Fixed the linking issue in CPPSDK <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/38573f2938061620f072346d2b3345b69454acbc">#2911</a></p></li>
<li><p>Fixed the ROPE kernel for cases where the batch size is larger than one <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/2d02768af957011244dd9ca89186cc1318466d6c">#2928</a></p></li>
<li><p>Upgraded deepspeed to v0.14.3 to include the support for Phi-3 <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/73105990e551656f79104dd93adc4a8020978947">#2985</a></p></li>
</ul>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v2.3.0+cpu…v2.3.100+cpu</p>
</section>
</section>
<section id="id12">
<h2>2.3.0<a class="headerlink" href="#id12" title="Link to this heading"></a></h2>
<p>We are excited to announce the release of Intel® Extension for PyTorch* 2.3.0+cpu which accompanies PyTorch 2.3. This release mainly brings you the new feature on Large Language Model (LLM) called module level LLM optimization API, which provides module level optimizations for commonly used LLM modules and functionalities, and targets to optimize customized LLM modeling for scenarios like private models, self-customized models, LLM serving frameworks, etc. This release also extends the list of optimized LLM models to a broader level and includes a set of bug fixing and small optimizations. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try this release and feedback as to improve further on this product.</p>
<section id="id13">
<h3>Highlights<a class="headerlink" href="#id13" title="Link to this heading"></a></h3>
<ul>
<li><p>Large Language Model (LLM) optimization</p>
<p><a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch">Intel® Extension for PyTorch*</a> provides a new feature called module level LLM optimization API, which provides module level optimizations for commonly used LLM modules and functionalities. LLM creators can then use this new API set to replace related parts in models by themselves, with which to reach peak performance.</p>
<p>There are 3 categories of module level LLM optimization APIs in general:</p>
<ul class="simple">
<li><p>Linear post-op APIs</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># using module init and forward</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">linearMul</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">linearGelu</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">linearNewGelu</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">linearAdd</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">linearAddAdd</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">linearSilu</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">linearSiluMul</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">linear2SiluMul</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">linearRelu</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Attention related APIs</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># using module init and forward</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">RotaryEmbedding</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">RMSNorm</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">FastLayerNorm</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">VarlenAttention</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">PagedAttention</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">IndirectAccessKVCacheAttention</span>

<span class="c1"># using as functions</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">rotary_embedding</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">rms_norm</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">fast_layer_norm</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">indirect_access_kv_cache_attention</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">varlen_attention</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Generation related APIs</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># using for optimizing huggingface generation APIs with prompt sharing</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">generation</span><span class="o">.</span><span class="n">hf_beam_sample</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">generation</span><span class="o">.</span><span class="n">hf_beam_search</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">generation</span><span class="o">.</span><span class="n">hf_greedy_search</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">generation</span><span class="o">.</span><span class="n">hf_sample</span>
</pre></div>
</div>
<p>More detailed introduction on how to apply this API set and example code walking you through can be found <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/release/2.3/examples/cpu/inference/python/llm-modeling">here</a>.</p>
</li>
<li><p>Bug fixing and other optimization</p>
<ul class="simple">
<li><p>Optimized the performance of LLM <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/ade45387ecc4e707754de9db6fc2be0af186e2ba">#2561</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/05d07645e1ae5eeeff15abda31a6ba5806dd2bb2">#2584</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/adb563834a4f6bd327d7307c493c8fe1648e6211">#2617</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/214dea0c8e7b2864a0c2d1a1c32fb7815ca68070">#2663</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/f5b941c3b7ea8fe1a387617a9329467d1e1b544a">#2733</a></p></li>
<li><p>Supported Act Order of GPTQ <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/be636289eef628b995e79a475c58f8a4d93e4890">#2550</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/9fcc4897492333330fb6bd156b1178d55347d292">#2568</a></p></li>
<li><p>Improved the warning and the logging information for better user experience <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/e0bf673cf3ea4063a7e168ec221f421fbd378fb3">#2641</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/770275a755ea0445675720a3f6f14e77c491fceb">#2675</a></p></li>
<li><p>Added TorchServe CPU Example <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/1f6fe6423dde7ccecc1565e73dc81d9cb281bc1f">#2613</a></p></li>
<li><p>Upgraded oneDNN to v3.4.1 <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/e2a9af49874fcf39097036c08848cd37cadc0084">#2747</a></p></li>
<li><p>Misc fix and enhancement <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/f88a7d127a6a3017db508454c7d332d7b2ad83f6">#2468</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/bc32ea463084d711e4a9aae85e38dd5d7d427849">#2627</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/f55a2bfa5d505fb7c7a6225c1c6206b5926777ab">#2631</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/eae477f76356b5a83640941787a168f680334775">#2704</a></p></li>
</ul>
</li>
</ul>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v2.2.0+cpu…v2.3.0+cpu</p>
</section>
</section>
<section id="id14">
<h2>2.2.0<a class="headerlink" href="#id14" title="Link to this heading"></a></h2>
<p>We are excited to announce the release of Intel® Extension for PyTorch* 2.2.0+cpu which accompanies PyTorch 2.2. This release mainly brings in our latest optimization on Large Language Model (LLM) including new dedicated API set (<code class="docutils literal notranslate"><span class="pre">ipex.llm</span></code>), new capability for auto-tuning accuracy recipe for LLM, and a broader list of optimized LLM models, together with a set of bug fixing and small optimization. We want to sincerely thank our dedicated community for your contributions. As always, we encourage you to try this release and feedback as to improve further on this product.</p>
<section id="id15">
<h3>Highlights<a class="headerlink" href="#id15" title="Link to this heading"></a></h3>
<ul>
<li><p>Large Language Model (LLM) optimization</p>
<p>Intel® Extension for PyTorch* provides a new dedicated module, <code class="docutils literal notranslate"><span class="pre">ipex.llm</span></code>, to host for Large Language Models (LLMs) specific APIs. With <code class="docutils literal notranslate"><span class="pre">ipex.llm</span></code>, Intel® Extension for PyTorch* provides comprehensive LLM optimization cross various popular datatypes including FP32/BF16/INT8/INT4. Specifically for low precision, both SmoothQuant and Weight-Only quantization are supported for various scenarios. And user can also run Intel® Extension for PyTorch* with Tensor Parallel to fit in the multiple ranks or multiple nodes scenarios to get even better performance.</p>
<p>A typical API under this new module is <code class="docutils literal notranslate"><span class="pre">ipex.llm.optimize</span></code>, which is designed to optimize transformer-based models within frontend Python modules, with a particular focus on Large Language Models (LLMs). It provides optimizations for both model-wise and content-generation-wise. <code class="docutils literal notranslate"><span class="pre">ipex.llm.optimize</span></code> is an upgrade API to replace previous <code class="docutils literal notranslate"><span class="pre">ipex.optimize_transformers</span></code>, which will bring you more consistent LLM experience and performance. Below shows a simple example of <code class="docutils literal notranslate"><span class="pre">ipex.llm.optimize</span></code> for fp32 or bf16 inference:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">transformers</span>

<span class="n">model</span><span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">AutoModelForCausalLM</span><span class="p">(</span><span class="n">model_name_or_path</span><span class="p">)</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span> <span class="c1"># or torch.bfloat16</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">llm</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">generate</span><span class="p">(</span><span class="n">YOUR_GENERATION_PARAMS</span><span class="p">)</span>
</pre></div>
</div>
<p>More examples of this API can be found at <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/v2.2.0%2Bcpu/docs/tutorials/llm/llm_optimize.md">LLM optimization API</a>.</p>
<p>Besides the new optimization API for LLM inference, Intel® Extension for PyTorch* also provides new capability for users to auto-tune a good quantization recipe for running SmoothQuant INT8 with good accuracy. SmoothQuant is a popular method to improve the accuracy of int8 quantization. The new auto-tune API allows automatic global alpha tuning, and automatic layer-by-layer alpha tuning provided by Intel® Neural Compressor for the best INT8 accuracy. More details can be found at <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/v2.2.0%2Bcpu/docs/tutorials/features/sq_recipe_tuning_api.md">SmoothQuant Recipe Tuning API Introduction</a>.</p>
<p>Intel® Extension for PyTorch* newly optimized many more LLM models including more llama2 variance like llama2-13b/llama2-70b, encoder-decoder model like T5, code generation models like starcoder/codegen, and more like Baichuan, Baichuan2, ChatGLM2, ChatGLM3, mistral, mpt, dolly, etc.. A full list of optimized models can be found at <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/v2.2.0%2Bcpu/examples/cpu/inference/python/llm">LLM Optimization</a>.</p>
</li>
<li><p>Bug fixing and other optimization</p>
<ul class="simple">
<li><p>Further optimized the performance of LLMs <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/d6d591938aefb9020a8a542a160abe4aeb6b238c">#2349</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/e0399108856c826ad609e5f421021945de30a4bf#diff-11f6a633ad677c6a8b6e8e4462afbe836a853a284e362ba794a8fcbceebc9dc5">#2412</a>, <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/aeaeba47bc722d9b18f13f8a78e02092c0a6bb5b">#2469</a>, <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/c95eb77398fa131e4ef60be65841ca09a284115d">#2476</a></p></li>
<li><p>Optimized the Flash Attention Operator <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/8d0426c1aebc85620fd417fa7fd4e0f1b357fa3d">#2317</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/efab335b427daf76e01836d520b1d7981de59595">#2334</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/5ed3a2413db5f0a5e53bcca0b3e84a814d87bb50">#2392</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/df2387e976461f6c42e0b90b3544ea76d3132694">#2480</a></p></li>
<li><p>Fixed the static quantization of the ELSER model <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/ac613a73fb395836b210710a6fefdf6d32df3386">#2491</a></p></li>
<li><p>Switched deepspeed to the public release version on PyPI <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/dba7b8c5fc9bfd8e7aa9431efe63499014acd722">#2473</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/94c31ecb3b6f6e77f595ce94dd6d6cbae1db1210">#2511</a></p></li>
<li><p>Upgrade oneDNN to v3.3.4 <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/af9b096070e81b46250172174bb9d12e3e1c6acf">#2433</a></p></li>
</ul>
</li>
</ul>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v2.1.100+cpu…v2.2.0+cpu</p>
</section>
</section>
<section id="id16">
<h2>2.1.100<a class="headerlink" href="#id16" title="Link to this heading"></a></h2>
<section id="id17">
<h3>Highlights<a class="headerlink" href="#id17" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Improved the performance of BF16 LLM generation inference: <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/99aa54f757de6c7d98f704edc6f8a83650fb1541">#2253</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/1d5e83d85c3aaf7c00323d7cb4019b40849dd2ed">#2251</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/be349962f3362f8afde4f083ec04d335245992bb">#2236</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/066c3bff417df084fa8e1d48375c0e1404320e95">#2278</a></p></li>
<li><p>Added the optimization for Codegen: <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/7c598e42e5b7899f284616c05c6896bf9d8bd2b8">#2257</a></p></li>
<li><p>Provided the dockerfile and updated the related doc to improve the UX for LLM users: <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/11484c3ebad9f868d0179a46de3d1330d9011822">#2229</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/0cd25021952bddcf5a364da45dfbefd4a0c77af4">#2195</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/76a42e516a68539752a3a8ab9aeb814d28c44cf8">#2299</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/4091bb5c0bf5f3c3ce5fbece291b44159a7fbf5c">#2315</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/e5ed8270d4d89bf68757f967676db57292c71920">#2283</a></p></li>
<li><p>Improved the accuracy of the quantization path of LLMs: <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/abc4c4e160cec3c792f5316e358173b8722a786e">#2280</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/4e212e41affa2ed07ffaf57bf10e9781113bc101">#2292</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/ed5957eb3b6190ad0be728656674f0a2a3b89158">#2275</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/1dae69de39408bc0ad245f4914d5f60e008a6eb3">#2319</a></p></li>
<li><p>Misc fix and enhancement: <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/ed1deccb86403e12e895227045d558117c5ea0fe">#2198</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/5dedcd6eb7bbf70dc92f0c20962fb2340e42e76f">#2264</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/c6e46cecd899317acfd2bd2a44a3f17b3cc1ce69">#2290</a></p></li>
</ul>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v2.1.0+cpu…v2.1.100+cpu</p>
</section>
</section>
<section id="id18">
<h2>2.1.0<a class="headerlink" href="#id18" title="Link to this heading"></a></h2>
<section id="id19">
<h3>Highlights<a class="headerlink" href="#id19" title="Link to this heading"></a></h3>
<ul>
<li><p><strong>Large Language Model (LLM) optimization (Experimental)</strong>: Intel® Extension for PyTorch* provides a lot of specific optimizations for LLMs in this new release. In operator level, we provide highly efficient GEMM kernel to speedup Linear layer and customized operators to reduce the memory footprint. To better trade-off the performance and accuracy, different low-precision solutions e.g., smoothQuant for INT8 and weight-only-quantization for INT4 and INT8 are also enabled. Besides, tensor parallel can also be adopt to get lower latency for LLMs.</p>
<p>A new API function, <code class="docutils literal notranslate"><span class="pre">ipex.optimize_transformers</span></code>, is designed to optimize transformer-based models within frontend Python modules, with a particular focus on Large Language Models (LLMs). It provides optimizations for both model-wise and content-generation-wise. You just need to invoke the <code class="docutils literal notranslate"><span class="pre">ipex.optimize_transformers</span></code> function instead of the <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> function to apply all optimizations transparently. More detailed information can be found at <a class="reference internal" href="./llm.html">Large Language Model optimizations overview</a>.</p>
<p>Specifically, this new release includes the support of <a class="reference external" href="https://arxiv.org/abs/2211.10438">SmoothQuant</a> and weight only quantization (both INT8 weight and INT4 weight) as to provide better performance and accuracy for low precision scenarios.</p>
<p>A typical usage of this new feature is quite simple as below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="o">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize_transformers</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>torch.compile backend optimization with PyTorch Inductor (Experimental)</strong>: We optimized Intel® Extension for PyTorch to leverage PyTorch Inductor’s capability when working as a backend of torch.compile, which can better utilize torch.compile’s power of graph capture, Inductor’s scalable fusion capability, and still keep customized optimization from Intel® Extension for PyTorch.</p></li>
<li><p><strong>performance optimization of static quantization under dynamic shape</strong>: We optimized the static quantization performance of Intel® Extension for PyTorch for dynamic shapes. The usage is the same as the workflow of running static shapes while inputs of variable shapes could be provided during runtime.</p></li>
<li><p><strong>Bug fixing and other optimization</strong></p>
<ul class="simple">
<li><p>Optimized the runtime memory usage <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/a821c0aef97ee6252d2bfbe6a75b6085f78bcc59">#1563</a></p></li>
<li><p>Fixed the excessive size of the saved model <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/39f2d0f4e91c6007cb58566b63e06b72d7b17ce4">#1677</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/58adee5b043a52e0c0a60320d48eae82de557074">#1688</a></p></li>
<li><p>Supported shared parameters in <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/4fa37949385db88b854eb60ab6de7178706cdcfe">#1664</a></p></li>
<li><p>Enabled the optimization of LARS fusion <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/e5b169e8d1e06558bb366eeaf4c793a382bc2d62">#1695</a></p></li>
<li><p>Supported dictionary input in <code class="docutils literal notranslate"><span class="pre">ipex.quantization.prepare</span></code> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/30b70e4b0bd8c3d1b2be55147ebd74fbfebe6093">#1682</a></p></li>
<li><p>Updated oneDNN to v3.3 <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/4dc4bb5f9d1cfb9f958893a410f7332be4b5f783">#2137</a></p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="id20">
<h2>2.0.100<a class="headerlink" href="#id20" title="Link to this heading"></a></h2>
<section id="id21">
<h3>Highlights<a class="headerlink" href="#id21" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Enhanced the functionality of Intel® Extension for PyTorch as a backend of <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>: <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/881c6fe0e6f8ab84a564b02216ddb96a3589363e">#1568</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/f5ce6193496ae68a57d688a3b3bbff541755e4ce">#1585</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/d8723df73358ae495ae5f62b5cdc90ae08920d27">#1590</a></p></li>
<li><p>Fixed the Stable Diffusion fine-tuning accuracy issue <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/bc76ab133b7330852931db9cda8dca7c69a0b594">#1587</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/b2983b4d35fc0ea7f5bdaf37f6e269256f8c36c4">#1594</a></p></li>
<li><p>Fixed the ISA check on old hypervisor based VM <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/a34eab577c4efa1c336b1f91768075bb490c1f14">#1513</a></p></li>
<li><p>Addressed the excessive memory usage in weight prepack <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/ee7dc343790d1d63bab1caf71e57dd3f7affdce9">#1593</a></p></li>
<li><p>Fixed the weight prepack of convolution when <code class="docutils literal notranslate"><span class="pre">padding_mode</span></code> is not <code class="docutils literal notranslate"><span class="pre">'zeros'</span></code> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/02449ccb3a6b475643116532a4cffbe1f974c1d9">#1580</a></p></li>
<li><p>Optimized the INT8 LSTM performance <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/fed42b17391fed477ae8adec83d920f8f8fb1a80">#1566</a></p></li>
<li><p>Fixed TransNetV2 calibration failure <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/046f7dfbaa212389ac58ae219597c16403e66bad">#1564</a></p></li>
<li><p>Fixed BF16 RNN-T inference when <code class="docutils literal notranslate"><span class="pre">AVX512_CORE_VNNI</span></code> ISA is used <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/023c104ab5953cf63b84efeb5176007d876015a2">#1592</a></p></li>
<li><p>Fixed the ROIAlign operator <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/6beb3d4661f09f55d031628ebe9fa6d63f04cab1">#1589</a></p></li>
<li><p>Enabled execution on designated numa nodes with launch script <a class="reference external" href="https://github.com/intel-innersource/frameworks.ai.pytorch.ipex-cpu/commit/2ab3693d50d6edd4bfae766f75dc273396a79488">#1517</a></p></li>
</ul>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v2.0.0+cpu…v2.0.100+cpu</p>
</section>
</section>
<section id="id22">
<h2>2.0.0<a class="headerlink" href="#id22" title="Link to this heading"></a></h2>
<p>We are pleased to announce the release of Intel® Extension for PyTorch* 2.0.0-cpu which accompanies PyTorch 2.0. This release mainly brings in our latest optimization on NLP (BERT), support of PyTorch 2.0’s hero API –- torch.compile as one of its backend, together with a set of bug fixing and small optimization.</p>
<section id="id23">
<h3>Highlights<a class="headerlink" href="#id23" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>Fast BERT optimization (Experimental)</strong>: Intel introduced a new technique to speed up BERT workloads. Intel® Extension for PyTorch* integrated this implementation, which benefits BERT model especially training. A new API <code class="docutils literal notranslate"><span class="pre">ipex.fast_bert</span></code> is provided to try this new optimization. More detailed information can be found at <a class="reference internal" href="features/fast_bert.html"><span class="doc">Fast Bert Feature</span></a>.</p></li>
<li><p><strong>MHA optimization with Flash Attention</strong>: Intel optimized MHA module with Flash Attention technique as inspired by <a class="reference external" href="https://arxiv.org/abs/2205.14135">Stanford paper</a>. This brings less memory consumption for LLM, and also provides better inference performance for models like BERT, Stable Diffusion, etc.</p></li>
<li><p><strong>Work with torch.compile as an backend (Experimental)</strong>: PyTorch 2.0 introduces a new feature, <code class="docutils literal notranslate"><span class="pre">torch.compile</span></code>, to speed up PyTorch execution. We’ve enabled Intel® Extension for PyTorch as a backend of torch.compile, which can leverage this new PyTorch API’s power of graph capture and provide additional optimization based on these graphs.
The usage of this new feature is quite simple as below:</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="o">...</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">backend</span><span class="o">=</span><span class="s1">&#39;ipex&#39;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p><strong>Bug fixing and other optimization</strong></p>
<ul>
<li><p>Supported <a class="reference external" href="https://arxiv.org/abs/1910.07467">RMSNorm</a> which is widely used in the t5 model of huggingface <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/d1de1402a8d6b9ca49b9c9a45a92899f7566866a">#1341</a></p></li>
<li><p>Optimized InstanceNorm <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/8b97d2998567cc2fda6eb008194cd64f624e857f">#1330</a></p></li>
<li><p>Fixed the quantization of LSTM <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/a4f93c09855679d2b424ca5be81930e3a4562cef">#1414</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/5b44996dc0fdb5c45995d403e18a44f2e1a11b3d">#1473</a></p></li>
<li><p>Fixed the correctness issue of unpacking non-contiguous Linear weight <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/84d413d6c10e16c025c407b68652b1769597e016">#1419</a></p></li>
<li><p>oneDNN update <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/fd5c10b664d19c87f8d94cf293077f65f78c3937">#1488</a></p></li>
</ul>
</li>
</ul>
</section>
<section id="known-issues">
<h3>Known Issues<a class="headerlink" href="#known-issues" title="Link to this heading"></a></h3>
<p>Please check at <a class="reference internal" href="known_issues.html"><span class="doc">Known Issues webpage</span></a>.</p>
</section>
</section>
<section id="id24">
<h2>1.13.100<a class="headerlink" href="#id24" title="Link to this heading"></a></h2>
<section id="id25">
<h3>Highlights<a class="headerlink" href="#id25" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Quantization optimization with more fusion, op and auto channels last support <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/5dd3a6ed9017197dea5c05c3af6d330336ed8eff">#1318</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/461b867021e1471c93a1a2a96255247c9d2ab45b">#1353</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/ff3f527025d2102898df9d02977df955e31ddf69">#1328</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/d21111565a179bb8f7ef6db3c04fafbe94871b61">#1355</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/2b898a935e597cfa92ee01a064a626763657c952">#1367</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/a81bd7023e9a119d1ce5f86307865b443034909e">#1384</a></p></li>
<li><p>Installation and build enhancement <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/9da7844b75b7cf22d9f4f5401178948919c40914">#1295</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/ef12c70c3ed496e723ac087ea5703dae7df0358d">#1392</a></p></li>
<li><p>OneDNN graph and OneDNN update <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/dab9dc18659da53e624637166283ccc8db1373f9">#1376</a></p></li>
<li><p>Misc fix and enhancement <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/085ba5d93773ab283e954a4fce75468708b74d3a">#1373</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/0bdf4b27dc445eb8fd0d59f46d157949db597953">#1338</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/2e8289967472553a049158d55e60835371829925">#1391</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/f69492345eb8a9383a67d9416146c2b73de19d8d">#1322</a></p></li>
</ul>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v1.13.0+cpu…v1.13.100+cpu</p>
</section>
</section>
<section id="id26">
<h2>1.13.0<a class="headerlink" href="#id26" title="Link to this heading"></a></h2>
<p>We are pleased to announce the release of Intel® Extension for PyTorch* 1.13.0-cpu which accompanies PyTorch 1.13. This release is highlighted with quite a few usability features which help users to get good performance and accuracy on CPU with less effort. We also added a couple of performance features as always. Check out the feature summary below.</p>
<ul class="simple">
<li><p>Usability Features</p></li>
</ul>
<ol class="simple">
<li><p><strong>Automatic channels last format conversion</strong>: Channels last conversion is now applied automatically to PyTorch modules with <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> by default. Users don’t have to explicitly convert input and weight for CV models.</p></li>
<li><p><strong>Code-free optimization</strong> (experimental): <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> is automatically applied to PyTorch modules without the need of code changes when the PyTorch program is started with the Intel® Extension for PyTorch* launcher via the new <code class="docutils literal notranslate"><span class="pre">--auto-ipex</span></code> option.</p></li>
<li><p><strong>Graph capture mode</strong> of <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> (experimental): A new boolean flag <code class="docutils literal notranslate"><span class="pre">graph_mode</span></code> (default off) was added to <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code>, when turned on, converting the eager-mode PyTorch module into graph(s) to get the best of graph optimization.</p></li>
<li><p><strong>INT8 quantization accuracy autotune</strong> (experimental): A new quantization API <code class="docutils literal notranslate"><span class="pre">ipex.quantization.autotune</span></code> was added to refine the default Intel® Extension for PyTorch* quantization recipe via autotuning algorithms for better accuracy.</p></li>
<li><p><strong>Hypertune</strong> (experimental) is a new tool added on top of Intel® Extension for PyTorch* launcher to automatically identify the good configurations for best throughput via hyper-parameter tuning.</p></li>
<li><p><strong>ipexrun</strong>: The counterpart of <strong>torchrun</strong>, is a shortcut added for invoking Intel® Extension for PyTorch* launcher.</p></li>
</ol>
<ul class="simple">
<li><p>Performance Features</p></li>
</ul>
<ol class="simple">
<li><p>Packed MKL SGEMM landed as the default kernel option for FP32 Linear, bringing up-to 20% geomean speedup for real-time NLP tasks.</p></li>
<li><p>DL compiler is now turned on by default with oneDNN fusion and gives additional performance boost for INT8 models.</p></li>
</ol>
<section id="id27">
<h3>Highlights<a class="headerlink" href="#id27" title="Link to this heading"></a></h3>
<ul>
<li><p><strong>Automatic channels last format conversion</strong>: Channels last conversion is now applied to PyTorch modules automatically with <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> by default for both training and inference scenarios. Users don’t have to explicitly convert input and weight for CV models.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="c1"># No need to do explicitly format conversion</span>
<span class="c1"># m = m.to(format=torch.channels_last)</span>
<span class="c1"># x = x.to(format=torch.channels_last)</span>
<span class="c1"># for inference</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">m</span><span class="p">)</span>
<span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="c1"># for training</span>
<span class="n">m</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span>
<span class="n">m</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Code-free optimization</strong> (experimental): <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> is automatically applied to PyTorch modules without the need of code changes when the PyTorch program is started with the Intel® Extension for PyTorch* launcher via the new <code class="docutils literal notranslate"><span class="pre">--auto-ipex</span></code> option.</p>
<p>Example: QA case in HuggingFace</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span><span class="c1"># original command</span>
ipexrun<span class="w"> </span>--use_default_allocator<span class="w"> </span>--ninstance<span class="w"> </span><span class="m">2</span><span class="w"> </span>--ncore_per_instance<span class="w"> </span><span class="m">28</span><span class="w"> </span>run_qa.py<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model_name_or_path<span class="w"> </span>bert-base-uncased<span class="w"> </span>--dataset_name<span class="w"> </span>squad<span class="w"> </span>--do_eval<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--per_device_train_batch_size<span class="w"> </span><span class="m">12</span><span class="w"> </span>--learning_rate<span class="w"> </span>3e-5<span class="w"> </span>--num_train_epochs<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--max_seq_length<span class="w"> </span><span class="m">384</span><span class="w"> </span>--doc_stride<span class="w"> </span><span class="m">128</span><span class="w"> </span>--output_dir<span class="w"> </span>/tmp/debug_squad/

<span class="c1"># automatically apply bfloat16 optimization (--auto-ipex --dtype bfloat16)</span>
ipexrun<span class="w"> </span>--use_default_allocator<span class="w"> </span>--ninstance<span class="w"> </span><span class="m">2</span><span class="w"> </span>--ncore_per_instance<span class="w"> </span><span class="m">28</span><span class="w"> </span>--auto_ipex<span class="w"> </span>--dtype<span class="w"> </span>bfloat16<span class="w"> </span>run_qa.py<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--model_name_or_path<span class="w"> </span>bert-base-uncased<span class="w"> </span>--dataset_name<span class="w"> </span>squad<span class="w"> </span>--do_eval<span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--per_device_train_batch_size<span class="w"> </span><span class="m">12</span><span class="w"> </span>--learning_rate<span class="w"> </span>3e-5<span class="w"> </span>--num_train_epochs<span class="w"> </span><span class="m">2</span><span class="w"> </span><span class="se">\</span>
<span class="w">  </span>--max_seq_length<span class="w"> </span><span class="m">384</span><span class="w"> </span>--doc_stride<span class="w"> </span><span class="m">128</span><span class="w"> </span>--output_dir<span class="w"> </span>/tmp/debug_squad/
</pre></div>
</div>
</li>
<li><p><strong>Graph capture mode</strong> of <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> (experimental): A new boolean flag <code class="docutils literal notranslate"><span class="pre">graph_mode</span></code> (default off) was added to <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code>, when turned on, converting the eager-mode PyTorch module into graph(s) to get the best of graph optimization. Under the hood, it combines the goodness of both TorchScript tracing and TorchDynamo to get as max graph scope as possible. Currently, it only supports FP32 and BF16 inference. INT8 inference and training support are under way.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="n">model</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">optimized_model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">graph_mode</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>INT8 quantization accuracy autotune</strong> (experimental): A new quantization API <code class="docutils literal notranslate"><span class="pre">ipex.quantization.autotune</span></code> was added to refine the default Intel® Extension for PyTorch* quantization recipe via autotuning algorithms for better accuracy. This is an optional API to invoke (after <code class="docutils literal notranslate"><span class="pre">prepare</span></code> and before <code class="docutils literal notranslate"><span class="pre">convert</span></code>) for scenarios when the accuracy of default quantization recipe of Intel® Extension for PyTorch* cannot meet the requirement. The current implementation is powered by Intel® Neural Compressor.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="c1"># Calibrate the model</span>
<span class="n">qconfig</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">default_static_qconfig</span>
<span class="n">calibrated_model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">model_to_be_calibrated</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">,</span> <span class="n">example_inputs</span><span class="o">=</span><span class="n">example_inputs</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">calibration_data_set</span><span class="p">:</span>
    <span class="n">calibrated_model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="c1"># Autotune the model</span>
<span class="n">calib_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">eval_func</span><span class="p">(</span><span class="n">model</span><span class="p">):</span>
    <span class="c1"># Return accuracy value</span>
    <span class="o">...</span>
    <span class="k">return</span> <span class="n">accuracy</span>
<span class="n">tuned_model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">autotune</span><span class="p">(</span>
                 <span class="n">calibrated_model</span><span class="p">,</span> <span class="n">calib_dataloader</span><span class="p">,</span> <span class="n">eval_func</span><span class="p">,</span>
                 <span class="n">sampling_sizes</span><span class="o">=</span><span class="p">[</span><span class="mi">100</span><span class="p">],</span> <span class="n">accuracy_criterion</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;relative&#39;</span><span class="p">:</span> <span class="mf">0.01</span><span class="p">},</span> <span class="n">tuning_time</span><span class="o">=</span><span class="mi">0</span>
              <span class="p">)</span>
<span class="c1"># Convert the model to jit model</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">tuned_model</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="n">example_input</span><span class="p">)</span>
    <span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">traced_model</span><span class="p">)</span>
<span class="c1"># Do inference</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">traced_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p><strong>Hypertune</strong> (experimental) is a new tool added on top of Intel® Extension for PyTorch* launcher to automatically identify the good configurations for best throughput via hyper-parameter tuning.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python<span class="w"> </span>-m<span class="w"> </span>intel_extension_for_pytorch.cpu.launch.hypertune<span class="w"> </span>--conf_file<span class="w"> </span>&lt;your_conf_file&gt;<span class="w"> </span>&lt;your_python_script&gt;<span class="w"> </span><span class="o">[</span>args<span class="o">]</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="id28">
<h3>Known Issues<a class="headerlink" href="#id28" title="Link to this heading"></a></h3>
<p>Please check at <a class="reference internal" href="known_issues.html"><span class="doc">Known Issues webpage</span></a>.</p>
</section>
</section>
<section id="id29">
<h2>1.12.300<a class="headerlink" href="#id29" title="Link to this heading"></a></h2>
<section id="id30">
<h3>Highlights<a class="headerlink" href="#id30" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Optimize BF16 MHA fusion to avoid transpose overhead to boost BERT-* BF16 performance <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/7076524601f42a9b60402019af21b32782c2c203">#992</a></p></li>
<li><p>Remove 64bytes alignment constraint for FP32 and BF16 AddLayerNorm fusion <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/7076524601f42a9b60402019af21b32782c2c203">#992</a></p></li>
<li><p>Fix INT8 RetinaNet accuracy issue <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/e0c719be8246041f8b7bc5feca9cf9c2f599210a">#1032</a></p></li>
<li><p>Fix <code class="docutils literal notranslate"><span class="pre">Cat.out</span></code> issue that does not update the <code class="docutils literal notranslate"><span class="pre">out</span></code> tensor (#1053) <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/4381f9126bbb65aab2daf034299c3bf3d307e6e2">#1074</a></p></li>
</ul>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v1.12.100…v1.12.300</p>
</section>
</section>
<section id="id31">
<h2>1.12.100<a class="headerlink" href="#id31" title="Link to this heading"></a></h2>
<p>This is a patch release to fix the AVX2 issue that blocks running on non-AVX512 platforms.</p>
</section>
<section id="id32">
<h2>1.12.0<a class="headerlink" href="#id32" title="Link to this heading"></a></h2>
<p>We are excited to bring you the release of Intel® Extension for PyTorch* 1.12.0-cpu, by tightly following PyTorch <a class="reference external" href="https://github.com/pytorch/pytorch/releases/tag/v1.12.0">1.12</a> release. In this release, we matured the automatic int8 quantization and made it a stable feature. We stabilized runtime extension and brought about a MultiStreamModule feature to further boost throughput in offline inference scenario. We also brought about various enhancements in operation and graph which are positive for performance of broad set of workloads.</p>
<p>Highlights include:</p>
<ul class="simple">
<li><p>Automatic INT8 quantization became a stable feature baking into a well-tuned default quantization recipe, supporting both static and dynamic quantization and a wide range of calibration algorithms.</p></li>
<li><p>Runtime Extension, featured MultiStreamModule, became a stable feature, could further enhance throughput in offline inference scenario.</p></li>
<li><p>More optimizations in graph and operations to improve performance of broad set of models, examples include but not limited to wave2vec, T5, Albert etc.</p></li>
<li><p>Pre-built experimental binary with oneDNN Graph Compiler tuned on would deliver additional performance gain for Bert, Albert, Roberta in INT8 inference.</p></li>
</ul>
<section id="id33">
<h3>Highlights<a class="headerlink" href="#id33" title="Link to this heading"></a></h3>
<ul>
<li><p>Matured automatic INT8 quantization feature baking into a well-tuned default quantization recipe. We facilitated the user experience and provided a wide range of calibration algorithms like Histogram, MinMax, MovingAverageMinMax, etc. Meanwhile, We polished the static quantization with better flexibility and enabled dynamic quantization as well. Compared to the previous version, the brief changes are as follows. Refer to <a class="reference internal" href="features/int8_overview.html"><span class="doc">tutorial page</span></a> for more details.</p>
<table align="center">
<tbody>
<tr>
<td>v1.11.0-cpu</td>
<td>v1.12.0-cpu</td>
</tr>
<tr>
<td valign="top"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="c1"># Calibrate the model</span>
<span class="n">qconfig</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantConf</span><span class="p">(</span><span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">calibration_data_set</span><span class="p">:</span>
    <span class="k">with</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">calibrate</span><span class="p">(</span><span class="n">qconfig</span><span class="p">):</span>
        <span class="n">model_to_be_calibrated</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">qconfig</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;qconfig.json&#39;</span><span class="p">)</span>
<span class="c1"># Convert the model to jit model</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantConf</span><span class="p">(</span><span class="s1">&#39;qconfig.json&#39;</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">traced_model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">conf</span><span class="p">,</span> <span class="n">example_input</span><span class="p">)</span>
<span class="c1"># Do inference </span>
<span class="n">y</span> <span class="o">=</span> <span class="n">traced_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td valign="top"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="c1"># Calibrate the model</span>
<span class="n">qconfig</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">default_static_qconfig</span> <span class="c1"># Histogram calibration algorithm and</span>
<span class="n">calibrated_model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span><span class="n">model_to_be_calibrated</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">,</span> <span class="n">example_inputs</span><span class="o">=</span><span class="n">example_inputs</span><span class="p">)</span>
<span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">calibration_data_set</span><span class="p">:</span>
    <span class="n">calibrated_model</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
<span class="c1"># Convert the model to jit model</span>
<span class="n">quantized_model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">calibrated_model</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">quantized_model</span><span class="p">,</span> <span class="n">example_input</span><span class="p">)</span>
    <span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">traced_model</span><span class="p">)</span>
<span class="c1"># Do inference</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">traced_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table></li>
<li><p>Runtime Extension, featured MultiStreamModule, became a stable feature. In this release, we enhanced the heuristic rule to further enhance throughput in offline inference scenario. Meanwhile, we also provide the <code class="docutils literal notranslate"><span class="pre">ipex.cpu.runtime.MultiStreamModuleHint</span></code> to custom how to split the input into streams and concat the output for each steam.</p>
<table align="center">
<tbody>
<tr>
<td>v1.11.0-cpu</td>
<td>v1.12.0-cpu</td>
</tr>
<tr>
<td valign="top"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="c1"># Create CPU pool</span>
<span class="n">cpu_pool</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">CPUPool</span><span class="p">(</span><span class="n">node_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Create multi-stream model</span>
<span class="n">multi_Stream_model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">MultiStreamModule</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_streams</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cpu_pool</span><span class="o">=</span><span class="n">cpu_pool</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td valign="top"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="c1"># Create CPU pool</span>
<span class="n">cpu_pool</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">CPUPool</span><span class="p">(</span><span class="n">node_id</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Optional</span>
<span class="n">multi_stream_input_hint</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">MultiStreamModuleHint</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">multi_stream_output_hint</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">MultiStreamModuleHint</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Create multi-stream model</span>
<span class="n">multi_Stream_model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">runtime</span><span class="o">.</span><span class="n">MultiStreamModule</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">num_streams</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">cpu_pool</span><span class="o">=</span><span class="n">cpu_pool</span><span class="p">,</span>
  <span class="n">multi_stream_input_hint</span><span class="p">,</span>   <span class="c1"># optional</span>
  <span class="n">multi_stream_output_hint</span> <span class="p">)</span> <span class="c1"># optional</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table></li>
<li><p>Polished the <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> to accept the input shape information which would conclude the optimal memory layout for better kernel efficiency.</p>
<table align="center">
<tbody>
<tr>
<td>v1.11.0-cpu</td>
<td>v1.12.0-cpu</td>
</tr>
<tr>
<td valign="top"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="n">model</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">optimized_model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td valign="top"><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="n">model</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">model</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">PATH</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">optimized_model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">,</span> <span class="n">sample_input</span><span class="o">=</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table></li>
<li><p>Provided more optimizations in graph and operations</p>
<ul class="simple">
<li><p>Fuse Adam to improve training performance <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/d3f714e54dc8946675259ea7a445b26a2460b523">#822</a></p></li>
<li><p>Enable Normalization operators to support channels-last 3D <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/ae268ac1760d598a29584de5c99bfba46c6554ae">#642</a></p></li>
<li><p>Support Deconv3D to serve most models and implement most fusions like Conv</p></li>
<li><p>Enable LSTM to support static and dynamic quantization <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/2bf8dba0c380a26bbb385e253adbfaa2a033a785">#692</a></p></li>
<li><p>Enable Linear to support dynamic quantization <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/ff231fb55e33c37126a0ef7f0e739cd750d1ef6c">#787</a></p></li>
<li><p>Fusions.</p>
<ul>
<li><p>Fuse <code class="docutils literal notranslate"><span class="pre">Add</span></code> + <code class="docutils literal notranslate"><span class="pre">Swish</span></code> to accelerate FSI Riskful model <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/cc855ff2bafd245413a6111f3d21244d0bcbb6f6">#551</a></p></li>
<li><p>Fuse <code class="docutils literal notranslate"><span class="pre">Conv</span></code> + <code class="docutils literal notranslate"><span class="pre">LeakyReLU</span></code> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/dc6ed1a5967c644b03874fd1f8a503f0b80be6bd">#589</a></p></li>
<li><p>Fuse <code class="docutils literal notranslate"><span class="pre">BMM</span></code> + <code class="docutils literal notranslate"><span class="pre">Add</span></code> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/d1379aa565cc84b4a61b537ba2c9a046b7652f1a">#407</a></p></li>
<li><p>Fuse <code class="docutils literal notranslate"><span class="pre">Concat</span></code> + <code class="docutils literal notranslate"><span class="pre">BN</span></code> + <code class="docutils literal notranslate"><span class="pre">ReLU</span></code> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/cad3f82f6b7efed0c08b2f0c11117a4720f58df4">#647</a></p></li>
<li><p>Optimize <code class="docutils literal notranslate"><span class="pre">Convolution1D</span></code> to support channels last memory layout and fuse <code class="docutils literal notranslate"><span class="pre">GeLU</span></code> as its post operation. <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/a0c063bdf4fd1a7e66f8a23750ac0c2fe471a559">#657</a></p></li>
<li><p>Fuse <code class="docutils literal notranslate"><span class="pre">Einsum</span></code> + <code class="docutils literal notranslate"><span class="pre">Add</span></code> to boost Alphafold2 <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/3094f346a67c81ad858ad2a80900fab4c3b4f4e9">#674</a></p></li>
<li><p>Fuse <code class="docutils literal notranslate"><span class="pre">Linear</span></code> + <code class="docutils literal notranslate"><span class="pre">Tanh</span></code> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/b24cc530b1fd29cb161a76317891e361453333c9">#711</a></p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="id34">
<h3>Known Issues<a class="headerlink" href="#id34" title="Link to this heading"></a></h3>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">RuntimeError:</span> <span class="pre">Overflow</span> <span class="pre">when</span> <span class="pre">unpacking</span> <span class="pre">long</span></code> when a tensor’s min max value exceeds int range while performing int8 calibration. Please customize QConfig to use min-max calibration method.</p></li>
<li><p>Calibrating with quantize_per_tensor, when benchmarking with 1 OpenMP* thread, results might be incorrect with large tensors (find more detailed info <a class="reference external" href="https://github.com/pytorch/pytorch/issues/80501">here</a>. Editing your code following the pseudocode below can workaround this issue, if you do need to explicitly set OMP_NUM_THREAEDS=1 for benchmarking. However, there could be a performance regression if oneDNN graph compiler prototype feature is utilized.</p>
<p>Workaround pseudocode:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># perform convert/trace/freeze with omp_num_threads &gt; 1(N)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">prepared_model</span> <span class="o">=</span> <span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="n">converted_model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">prepared_model</span><span class="p">)</span>
<span class="n">traced_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">converted_model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
<span class="n">freezed_model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">traced_model</span><span class="p">)</span>
<span class="c1"># run freezed model to apply optimization pass</span>
<span class="n">freezed_model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>

<span class="c1"># benchmarking with omp_num_threads = 1</span>
<span class="n">torch</span><span class="o">.</span><span class="n">set_num_threads</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">run_benchmark</span><span class="p">(</span><span class="n">freezed_model</span><span class="p">,</span> <span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>Low performance with INT8 support for dynamic shapes
The support for dynamic shapes in Intel® Extension for PyTorch* INT8 integration is still work in progress. When the input shapes are dynamic, for example inputs of variable image sizes in an object detection task or of variable sequence lengths in NLP tasks, the Intel® Extension for PyTorch* INT8 path may slow down the model inference. In this case, use stock PyTorch INT8 functionality.
<strong>Note</strong>: Using Runtime Extension feature if batch size cannot be divided by number of streams, because mini batch size on each stream are not equivalent, scripts run into this issues.</p></li>
<li><p>BF16 AMP(auto-mixed-precision) runs abnormally with the extension on the AVX2-only machine if the topology contains <code class="docutils literal notranslate"><span class="pre">Conv</span></code>, <code class="docutils literal notranslate"><span class="pre">Matmul</span></code>, <code class="docutils literal notranslate"><span class="pre">Linear</span></code>, and <code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code></p></li>
<li><p>Runtime extension of MultiStreamModule doesn’t support DLRM inference, since the input of DLRM (EmbeddingBag specifically) can’t be simplely batch split.</p></li>
<li><p>Runtime extension of MultiStreamModule has poor performance of RNNT Inference comparing with native throughput mode. Only part of the RNNT models (joint_net specifically) can be jit traced into graph. However, in one batch inference, <code class="docutils literal notranslate"><span class="pre">joint_net</span></code> is invoked multi times. It increases the overhead of MultiStreamModule as input batch split, thread synchronization and output concat.</p></li>
<li><p>Incorrect Conv and Linear result if the number of OMP threads is changed at runtime
The oneDNN memory layout depends on the number of OMP threads, which requires the caller to detect the changes for the # of OMP threads while this release has not implemented it yet.</p></li>
<li><p>Low throughput with DLRM FP32 Train
A ‘Sparse Add’ <a class="reference external" href="https://github.com/pytorch/pytorch/pull/23057">PR</a> is pending on review. The issue will be fixed when the PR is merged.</p></li>
<li><p>If inference is done with a custom function, <code class="docutils literal notranslate"><span class="pre">conv+bn</span></code> folding feature of the <code class="docutils literal notranslate"><span class="pre">ipex.optimize()</span></code> function doesn’t work.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_pytorch_extension</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="k">class</span><span class="w"> </span><span class="nc">Module</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Module</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">5</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">bn</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">BatchNorm2d</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>
    <span class="k">def</span><span class="w"> </span><span class="nf">inference</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">Module</span><span class="p">()</span>
    <span class="n">m</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="n">m</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">optimize</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">level</span><span class="o">=</span><span class="s2">&quot;O0&quot;</span><span class="p">)</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">112</span><span class="p">,</span> <span class="mi">112</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
      <span class="n">m</span><span class="o">.</span><span class="n">inference</span><span class="p">(</span><span class="n">d</span><span class="p">)</span>
</pre></div>
</div>
<p>This is a PyTorch FX limitation. You can avoid this error by calling <code class="docutils literal notranslate"><span class="pre">m</span> <span class="pre">=</span> <span class="pre">ipex.optimize(m,</span> <span class="pre">level=&quot;O0&quot;)</span></code>, which doesn’t apply ipex optimization, or disable <code class="docutils literal notranslate"><span class="pre">conv+bn</span></code> folding by calling <code class="docutils literal notranslate"><span class="pre">m</span> <span class="pre">=</span> <span class="pre">ipex.optimize(m,</span> <span class="pre">level=&quot;O1&quot;,</span> <span class="pre">conv_bn_folding=False)</span></code>.</p>
</li>
</ul>
</section>
</section>
<section id="id35">
<h2>1.11.200<a class="headerlink" href="#id35" title="Link to this heading"></a></h2>
<section id="id36">
<h3>Highlights<a class="headerlink" href="#id36" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Enable more fused operators to accelerate particular models.</p></li>
<li><p>Fuse <code class="docutils literal notranslate"><span class="pre">Convolution</span></code> and <code class="docutils literal notranslate"><span class="pre">LeakyReLU</span></code> (<a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/d7603133f37375b3aba7bf744f1095b923ba979e">#648</a>)</p></li>
<li><p>Support <a class="reference external" href="https://pytorch.org/docs/stable/generated/torch.einsum.html"><code class="docutils literal notranslate"><span class="pre">torch.einsum</span></code></a> and fuse it with <code class="docutils literal notranslate"><span class="pre">add</span></code> (<a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/b66d6d8d0c743db21e534d13be3ee75951a3771d">#684</a>)</p></li>
<li><p>Fuse <code class="docutils literal notranslate"><span class="pre">Linear</span></code> and <code class="docutils literal notranslate"><span class="pre">Tanh</span></code> (<a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/f0f2bae96162747ed2a0002b274fe7226a8eb200">#685</a>)</p></li>
<li><p>In addition to the original installation methods, this release provides Docker installation from <a class="reference external" href="https://hub.docker.com/">DockerHub</a>.</p></li>
<li><p>Provided the <a class="reference external" href="installation.html#installation_onednn_graph_compiler">evaluation wheel packages</a> that could boost performance for selective topologies on top of oneDNN graph compiler prototype feature.
<em><strong>NOTE</strong></em>: This is still at an early development stage and not fully mature yet, but feel free to reach out through <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/issues">GitHub issues</a> if you have any suggestions.</p></li>
</ul>
<p><strong><a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/compare/v1.11.0...v1.11.200">Full Changelog</a></strong></p>
</section>
</section>
<section id="id37">
<h2>1.11.0<a class="headerlink" href="#id37" title="Link to this heading"></a></h2>
<p>We are excited to announce Intel® Extension for PyTorch* 1.11.0-cpu release by tightly following PyTorch 1.11 release. Along with extension 1.11, we focused on continually improving OOB user experience and performance. Highlights include:</p>
<ul class="simple">
<li><p>Support a single binary with runtime dynamic dispatch based on AVX2/AVX512 hardware ISA detection</p></li>
<li><p>Support install binary from <code class="docutils literal notranslate"><span class="pre">pip</span></code> with package name only (without the need of specifying the URL)</p></li>
<li><p>Provide the C++ SDK installation to facilitate ease of C++ app development and deployment</p></li>
<li><p>Add more optimizations, including graph fusions for speeding up Transformer-based models and CNN, etc</p></li>
<li><p>Reduce the binary size for both the PIP wheel and C++ SDK (2X to 5X reduction from the previous version)</p></li>
</ul>
<section id="id38">
<h3>Highlights<a class="headerlink" href="#id38" title="Link to this heading"></a></h3>
<ul>
<li><p>Combine the AVX2 and AVX512 binary as a single binary and automatically dispatch to different implementations based on hardware ISA detection at runtime. The typical case is to serve the data center that mixtures AVX2-only and AVX512 platforms. It does not need to deploy the different ISA binary now compared to the previous version</p>
<p><em><strong>NOTE</strong></em>:  The extension uses the oneDNN library as the backend. However, the BF16 and INT8 operator sets and features are different between AVX2 and AVX512. Refer to <a class="reference external" href="https://oneapi-src.github.io/oneDNN/dev_guide_int8_computations.html#processors-with-the-intel-avx2-or-intel-avx-512-support">oneDNN document</a> for more details.</p>
<blockquote>
<div><p>When one input is of type u8, and the other one is of type s8, oneDNN assumes the user will choose the quantization parameters so no overflow/saturation occurs. For instance, a user can use u7 [0, 127] instead of u8 for the unsigned input, or s7 [-64, 63] instead of the s8 one. It is worth mentioning that this is required only when the Intel AVX2 or Intel AVX512 Instruction Set is used.</p>
</div></blockquote>
</li>
<li><p>The extension wheel packages have been uploaded to <a class="reference external" href="https://pypi.org/project/intel-extension-for-pytorch/">pypi.org</a>. The user could directly install the extension by <code class="docutils literal notranslate"><span class="pre">pip/pip3</span></code> without explicitly specifying the binary location URL.</p></li>
</ul>
<table align="center">
<tbody>
<tr>
<td>v1.10.100-cpu</td>
<td>v1.11.0-cpu</td>
</tr>
<tr>
<td><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="o">-</span><span class="n">m</span> <span class="n">pip</span> <span class="n">install</span> <span class="n">intel_extension_for_pytorch</span><span class="o">==</span><span class="mf">1.10.100</span> <span class="o">-</span><span class="n">f</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">software</span><span class="o">.</span><span class="n">intel</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">ipex</span><span class="o">-</span><span class="n">whl</span><span class="o">-</span><span class="n">stable</span>
</pre></div>
</div>
</td>
<td><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="n">intel_extension_for_pytorch</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Compared to the previous version, this release provides a dedicated installation file for the C++ SDK. The installation file automatically detects the PyTorch C++ SDK location and installs the extension C++ SDK files to the PyTorch C++ SDK. The user does not need to manually add the extension C++ SDK source files and CMake to the PyTorch SDK. In addition to that, the installation file reduces the C++ SDK binary size from ~220MB to ~13.5MB.</p></li>
</ul>
<table align="center">
<tbody>
<tr>
<td>v1.10.100-cpu</td>
<td>v1.11.0-cpu</td>
</tr>
<tr>
<td><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">intel</span><span class="o">-</span><span class="n">ext</span><span class="o">-</span><span class="n">pt</span><span class="o">-</span><span class="n">cpu</span><span class="o">-</span><span class="n">libtorch</span><span class="o">-</span><span class="n">shared</span><span class="o">-</span><span class="k">with</span><span class="o">-</span><span class="n">deps</span><span class="o">-</span><span class="mf">1.10.0</span><span class="o">+</span><span class="n">cpu</span><span class="o">.</span><span class="n">zip</span> <span class="p">(</span><span class="mi">220</span><span class="n">M</span><span class="p">)</span>
<span class="n">intel</span><span class="o">-</span><span class="n">ext</span><span class="o">-</span><span class="n">pt</span><span class="o">-</span><span class="n">cpu</span><span class="o">-</span><span class="n">libtorch</span><span class="o">-</span><span class="n">cxx11</span><span class="o">-</span><span class="n">abi</span><span class="o">-</span><span class="n">shared</span><span class="o">-</span><span class="k">with</span><span class="o">-</span><span class="n">deps</span><span class="o">-</span><span class="mf">1.10.0</span><span class="o">+</span><span class="n">cpu</span><span class="o">.</span><span class="n">zip</span> <span class="p">(</span><span class="mi">224</span><span class="n">M</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td><div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">libintel</span><span class="o">-</span><span class="n">ext</span><span class="o">-</span><span class="n">pt</span><span class="o">-</span><span class="mf">1.11.0</span><span class="o">+</span><span class="n">cpu</span><span class="o">.</span><span class="n">run</span> <span class="p">(</span><span class="mf">13.7</span><span class="n">M</span><span class="p">)</span>
<span class="n">libintel</span><span class="o">-</span><span class="n">ext</span><span class="o">-</span><span class="n">pt</span><span class="o">-</span><span class="n">cxx11</span><span class="o">-</span><span class="n">abi</span><span class="o">-</span><span class="mf">1.11.0</span><span class="o">+</span><span class="n">cpu</span><span class="o">.</span><span class="n">run</span> <span class="p">(</span><span class="mf">13.5</span><span class="n">M</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Add more optimizations, including more custom operators and fusions.</p>
<ul>
<li><p>Fuse the QKV linear operators as a single Linear to accelerate the Transformer*(BERT-*) encoder part  - <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/0f27c269cae0f902973412dc39c9a7aae940e07b">#278</a>.</p></li>
<li><p>Remove Multi-Head-Attention fusion limitations to support the 64bytes unaligned tensor shape. <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/dbb10fedb00c6ead0f5b48252146ae9d005a0fad">#531</a></p></li>
<li><p>Fold the binary operator to Convolution and Linear operator to reduce computation. <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/564588561fa5d45b8b63e490336d151ff1fc9cbc">#432</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/b4e7dacf08acd849cecf8d143a11dc4581a3857f">#438</a> <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/74aa21262938b923d3ed1e6929e7d2b629b3ff27">#602</a></p></li>
<li><p>Replace the outplace operators with their corresponding in-place version to reduce memory footprint. The extension currently supports the operators including <code class="docutils literal notranslate"><span class="pre">sliu</span></code>, <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code>, <code class="docutils literal notranslate"><span class="pre">tanh</span></code>, <code class="docutils literal notranslate"><span class="pre">hardsigmoid</span></code>, <code class="docutils literal notranslate"><span class="pre">hardswish</span></code>, <code class="docutils literal notranslate"><span class="pre">relu6</span></code>, <code class="docutils literal notranslate"><span class="pre">relu</span></code>, <code class="docutils literal notranslate"><span class="pre">selu</span></code>, <code class="docutils literal notranslate"><span class="pre">softmax</span></code>. <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/38647677e8186a235769ea519f4db65925eca33c">#524</a></p></li>
<li><p>Fuse the Concat + BN + ReLU as a single operator. <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/275ff503aea780a6b741f04db5323d9529ee1081">#452</a></p></li>
<li><p>Optimize Conv3D for both imperative and JIT by enabling NHWC and pre-packing the weight. <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/commit/ae33faf62bb63b204b0ee63acb8e29e24f6076f3">#425</a></p></li>
</ul>
</li>
<li><p>Reduce the binary size. C++ SDK is reduced from ~220MB to ~13.5MB while the wheel packaged is reduced from ~100MB to ~40MB.</p></li>
<li><p>Update oneDNN and oneDNN graph to <a class="reference external" href="https://github.com/oneapi-src/oneDNN/releases/tag/v2.5.2">2.5.2</a> and <a class="reference external" href="https://github.com/oneapi-src/oneDNN/releases/tag/graph-v0.4.2">0.4.2</a> respectively.</p></li>
</ul>
</section>
<section id="what-s-changed">
<h3>What’s Changed<a class="headerlink" href="#what-s-changed" title="Link to this heading"></a></h3>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v1.10.100…v1.11.0</p>
</section>
</section>
<section id="id39">
<h2>1.10.100<a class="headerlink" href="#id39" title="Link to this heading"></a></h2>
<p>This release is meant to fix the following issues:</p>
<ul class="simple">
<li><p>Resolve the issue that the PyTorch Tensor Expression(TE) did not work after importing the extension.</p></li>
<li><p>Wraps the BactchNorm(BN) as another operator to break the TE’s BN-related fusions. Because the BatchNorm performance of PyTorch Tensor Expression can not achieve the same performance as PyTorch ATen BN.</p></li>
<li><p>Update the <a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/">documentation</a></p>
<ul>
<li><p>Fix the INT8 quantization example issue #205</p></li>
<li><p>Polish the installation guide</p></li>
</ul>
</li>
</ul>
</section>
<section id="id40">
<h2>1.10.0<a class="headerlink" href="#id40" title="Link to this heading"></a></h2>
<p>The Intel® Extension for PyTorch* 1.10 is on top of PyTorch 1.10. In this release, we polished the front end APIs. The APIs are more simple, stable, and straightforward now. According to PyTorch community recommendation, we changed the underhood device from <code class="docutils literal notranslate"><span class="pre">XPU</span></code> to <code class="docutils literal notranslate"><span class="pre">CPU</span></code>. With this change, the model and tensor does not need to be converted to the extension device to get performance improvement. It simplifies the model changes.</p>
<p>Besides that, we continuously optimize the Transformer* and CNN models by fusing more operators and applying NHWC. We measured the 1.10 performance on Torchvison and HugginFace. As expected, 1.10 can speed up the two model zones.</p>
<section id="id41">
<h3>Highlights<a class="headerlink" href="#id41" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Change the package name to <code class="docutils literal notranslate"><span class="pre">intel_extension_for_pytorch</span></code> while the original package name is <code class="docutils literal notranslate"><span class="pre">intel_pytorch_extension</span></code>. This change targets to avoid any potential legal issues.</p></li>
</ul>
<table align="center">
<tbody>
<tr>
<td>v1.9.0-cpu</td>
<td>v1.10.0-cpu</td>
</tr>
<tr>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>The underhood device is changed from the extension-specific device(<code class="docutils literal notranslate"><span class="pre">XPU</span></code>) to the standard CPU device that aligns with the PyTorch CPU device design, regardless of the dispatch mechanism and operator register mechanism. The means the model does not need to be converted to the extension device explicitly.</p></li>
</ul>
<table align="center">
<tbody>
<tr>
<td>v1.9.0-cpu</td>
<td>v1.10.0-cpu</td>
</tr>
<tr>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">models</span>

<span class="c1"># Import the extension</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>

<span class="n">resnet18</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="c1"># Explicitly convert the model to the extension device</span>
<span class="n">resnet18_xpu</span> <span class="o">=</span> <span class="n">resnet18</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ipex</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">models</span>

<span class="c1"># Import the extension</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>

<span class="n">resnet18</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet18</span><span class="p">(</span><span class="n">pretrained</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Compared to v1.9.0, v1.10.0 follows PyTorch AMP API(<code class="docutils literal notranslate"><span class="pre">torch.cpu.amp</span></code>) to support auto-mixed-precision. <code class="docutils literal notranslate"><span class="pre">torch.cpu.amp</span></code> provides convenience for auto data type conversion at runtime. Currently, <code class="docutils literal notranslate"><span class="pre">torch.cpu.amp</span></code> only supports <code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code>. It is the default lower precision floating point data type when <code class="docutils literal notranslate"><span class="pre">torch.cpu.amp</span></code> is enabled. <code class="docutils literal notranslate"><span class="pre">torch.cpu.amp</span></code> primarily benefits on Intel CPU with BFloat16 instruction set support.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="k">class</span><span class="w"> </span><span class="nc">SimpleNet</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SimpleNet</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">,</span> <span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<table align="center">
<tbody>
<tr>
<td>v1.9.0-cpu</td>
<td>v1.10.0-cpu</td>
</tr>
<tr>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the extension</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>

<span class="c1"># Automatically mix precision</span>
<span class="n">ipex</span><span class="o">.</span><span class="n">enable_auto_mixed_precision</span><span class="p">(</span><span class="n">mixed_dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bfloat16</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNet</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the extension</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">SimpleNet</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">cpu</span><span class="o">.</span><span class="n">amp</span><span class="o">.</span><span class="n">autocast</span><span class="p">(),</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>The 1.10 release provides the INT8 calibration as an experimental feature while it only supports post-training static quantization now. Compared to 1.9.0, the fronted APIs for quantization is more straightforward and ease-of-use.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch.nn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">nn</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>

<span class="k">class</span><span class="w"> </span><span class="nc">MyModel</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span><span class="w"> </span><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MyModel</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>

    <span class="k">def</span><span class="w"> </span><span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">x</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">MyModel</span><span class="p">()</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># user dataset for calibration.</span>
<span class="n">xx_c</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="c1"># user dataset for validation.</span>
<span class="n">xx_v</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">28</span><span class="p">,</span> <span class="mi">28</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">))</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Clibration</p></li>
</ul>
<table align="center">
<tbody>
<tr>
<td>v1.9.0-cpu</td>
<td>v1.10.0-cpu</td>
</tr>
<tr>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the extension</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>

<span class="c1"># Convert the model to the Extension device</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ipex</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">)</span>

<span class="c1"># Create a configuration file to save quantization parameters.</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">AmpConf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xx_c</span><span class="p">:</span>
        <span class="c1"># Run the model under calibration mode to collect quantization parameters</span>
        <span class="k">with</span> <span class="n">ipex</span><span class="o">.</span><span class="n">AutoMixPrecision</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span> <span class="n">running_mode</span><span class="o">=</span><span class="s1">&#39;calibration&#39;</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ipex</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">))</span>
<span class="c1"># Save the configuration file</span>
<span class="n">conf</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;configure.json&#39;</span><span class="p">)</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the extension</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>

<span class="n">conf</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantConf</span><span class="p">(</span><span class="n">qscheme</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">per_tensor_affine</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xx_c</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">calibrate</span><span class="p">(</span><span class="n">conf</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="n">conf</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="s1">&#39;configure.json&#39;</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Inference</p></li>
</ul>
 <table align="center">
<tbody>
<tr>
<td>v1.9.0-cpu</td>
<td>v1.10.0-cpu</td>
</tr>
<tr>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the extension</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>

<span class="c1"># Convert the model to the Extension device</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ipex</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">)</span>
<span class="n">conf</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">AmpConf</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">int8</span><span class="p">,</span> <span class="s1">&#39;configure.json&#39;</span><span class="p">)</span>
<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">cali_dataset</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">ipex</span><span class="o">.</span><span class="n">AutoMixPrecision</span><span class="p">(</span><span class="n">conf</span><span class="p">,</span> <span class="n">running_mode</span><span class="o">=</span><span class="s1">&#39;inference&#39;</span><span class="p">):</span>
            <span class="n">y</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">ipex</span><span class="o">.</span><span class="n">DEVICE</span><span class="p">))</span>
</pre></div>
</div>
</td>
<td><div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># Import the extension</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>

<span class="n">conf</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">QuantConf</span><span class="p">(</span><span class="s1">&#39;configure.json&#39;</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">trace_model</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">conf</span><span class="p">,</span> <span class="n">example_input</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">xx_v</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">trace_model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>This release introduces the <code class="docutils literal notranslate"><span class="pre">optimize</span></code> API at python front end to optimize the model and optimizer for training. The new API both supports FP32 and BF16, inference and training.</p></li>
<li><p>Runtime Extension (Experimental) provides a runtime CPU pool API to bind threads to cores. It also features async tasks. <strong>Note</strong>: Intel® Extension for PyTorch* Runtime extension is still in the <strong>experimental</strong> stage. The API is subject to change. More detailed descriptions are available in the extension documentation.</p></li>
</ul>
</section>
<section id="id42">
<h3>Known Issues<a class="headerlink" href="#id42" title="Link to this heading"></a></h3>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">omp_set_num_threads</span></code> function failed to change OpenMP threads number of oneDNN operators if it was set before.</p>
<p><code class="docutils literal notranslate"><span class="pre">omp_set_num_threads</span></code> function is provided in Intel® Extension for PyTorch* to change the number of threads used with OpenMP. However, it failed to change the number of OpenMP threads if it was set before.</p>
<p>pseudo-code:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">omp_set_num_threads</span><span class="p">(</span><span class="mi">6</span><span class="p">)</span>
<span class="n">model_execution</span><span class="p">()</span>
<span class="n">omp_set_num_threads</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="n">same_model_execution_again</span><span class="p">()</span>
</pre></div>
</div>
<p><strong>Reason:</strong> oneDNN primitive descriptor stores the omp number of threads. Current oneDNN integration caches the primitive descriptor in IPEX. So if we use runtime extension with oneDNN based pytorch/ipex operation, the runtime extension fails to change the used omp number of threads.</p>
</li>
<li><p>Low performance with INT8 support for dynamic shapes</p>
<p>The support for dynamic shapes in Intel® Extension for PyTorch* INT8 integration is still work in progress. When the input shapes are dynamic, for example, inputs of variable image sizes in an object detection task or of variable sequence lengths in NLP tasks, the Intel® Extension for PyTorch* INT8 path may slow down the model inference. In this case, use stock PyTorch INT8 functionality.</p>
</li>
<li><p>Low throughput with DLRM FP32 Train</p>
<p>A ‘Sparse Add’ <a class="reference external" href="https://github.com/pytorch/pytorch/pull/23057">PR</a> is pending review. The issue will be fixed when the PR is merged.</p>
</li>
</ul>
</section>
<section id="id43">
<h3>What’s Changed<a class="headerlink" href="#id43" title="Link to this heading"></a></h3>
<p><strong>Full Changelog</strong>: https://github.com/intel/intel-extension-for-pytorch/compare/v1.9.0…v1.10.0+cpu-rc3</p>
</section>
</section>
<section id="id44">
<h2>1.9.0<a class="headerlink" href="#id44" title="Link to this heading"></a></h2>
<section id="what-s-new">
<h3>What’s New<a class="headerlink" href="#what-s-new" title="Link to this heading"></a></h3>
<ul>
<li><p>Rebased the Intel Extension for Pytorch from PyTorch-1.8.0 to the official PyTorch-1.9.0 release.</p></li>
<li><p>Support binary installation.</p>
<p><code class="docutils literal notranslate"><span class="pre">python</span> <span class="pre">-m</span> <span class="pre">pip</span> <span class="pre">install</span> <span class="pre">torch_ipex==1.9.0</span> <span class="pre">-f</span> <span class="pre">https://software.intel.com/ipex-whl-stable</span></code></p>
</li>
<li><p>Support the C++ library. The third party App can link the Intel-Extension-for-PyTorch C++ library to enable the particular optimizations.</p></li>
</ul>
</section>
</section>
<section id="id45">
<h2>1.8.0<a class="headerlink" href="#id45" title="Link to this heading"></a></h2>
<section id="id46">
<h3>What’s New<a class="headerlink" href="#id46" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Rebased the Intel Extension for Pytorch from Pytorch -1.7.0 to the official Pytorch-1.8.0 release. The new XPU device type has been added into Pytorch-1.8.0(49786), don’t need to patch PyTorch to enable Intel Extension for Pytorch anymore</p></li>
<li><p>Upgraded the oneDNN from v1.5-rc to v1.8.1</p></li>
<li><p>Updated the README file to add the sections to introduce supported customized operators, supported fusion patterns, tutorials, and joint blogs with stakeholders</p></li>
</ul>
</section>
</section>
<section id="id47">
<h2>1.2.0<a class="headerlink" href="#id47" title="Link to this heading"></a></h2>
<section id="id48">
<h3>What’s New<a class="headerlink" href="#id48" title="Link to this heading"></a></h3>
<ul>
<li><p>We rebased the Intel Extension for pytorch from Pytorch -1.5rc3 to the official Pytorch-1.7.0 release. It will have performance improvement with the new Pytorch-1.7 support.</p></li>
<li><p>Device name was changed from DPCPP to XPU.</p>
<p>We changed the device name from DPCPP to XPU to align with the future Intel GPU product for heterogeneous computation.</p>
</li>
<li><p>Enabled the launcher for end users.</p></li>
<li><p>We enabled the launch script that helps users launch the program for training and inference, then automatically setup the strategy for multi-thread, multi-instance, and memory allocator. Refer to the launch script comments for more details.</p></li>
</ul>
</section>
<section id="performance-improvement">
<h3>Performance Improvement<a class="headerlink" href="#performance-improvement" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>This upgrade provides better INT8 optimization with refined auto mixed-precision API.</p></li>
<li><p>More operators are optimized for the int8 inference and bfp16 training of some key workloads, like MaskRCNN, SSD-ResNet34, DLRM, RNNT.</p></li>
</ul>
</section>
<section id="others">
<h3>Others<a class="headerlink" href="#others" title="Link to this heading"></a></h3>
<ul>
<li><p>Bug fixes</p>
<ul class="simple">
<li><p>This upgrade fixes the issue that saving the model trained by Intel extension for PyTorch caused errors.</p></li>
<li><p>This upgrade fixes the issue that Intel extension for PyTorch was slower than pytorch proper for Tacotron2.</p></li>
</ul>
</li>
<li><p>New custom operators</p>
<p>This upgrade adds several custom operators: ROIAlign, RNN, FrozenBatchNorm, nms.</p>
</li>
<li><p>Optimized operators/fusion</p>
<p>This upgrade optimizes several operators: tanh, log_softmax, upsample, and embeddingbad and enables int8 linear fusion.</p>
</li>
<li><p>Performance</p>
<p>The release has daily automated testing for the supported models: ResNet50, ResNext101, Huggingface Bert, DLRM, Resnext3d, MaskRNN, SSD-ResNet34. With the extension imported, it can bring up to 2x INT8 over FP32 inference performance improvements on the 3rd Gen Intel Xeon scalable processors (formerly codename Cooper Lake).</p>
</li>
</ul>
</section>
<section id="id49">
<h3>Known issues<a class="headerlink" href="#id49" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Multi-node training still encounter hang issues after several iterations. The fix will be included in the next official release.</p></li>
</ul>
</section>
</section>
<section id="id50">
<h2>1.1.0<a class="headerlink" href="#id50" title="Link to this heading"></a></h2>
<section id="id51">
<h3>What’s New<a class="headerlink" href="#id51" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Added optimization for training with FP32 data type &amp; BF16 data type. All the optimized FP32/BF16 backward operators include:</p>
<ul>
<li><p>Conv2d</p></li>
<li><p>Relu</p></li>
<li><p>Gelu</p></li>
<li><p>Linear</p></li>
<li><p>Pooling</p></li>
<li><p>BatchNorm</p></li>
<li><p>LayerNorm</p></li>
<li><p>Cat</p></li>
<li><p>Softmax</p></li>
<li><p>Sigmoid</p></li>
<li><p>Split</p></li>
<li><p>Embedding_bag</p></li>
<li><p>Interaction</p></li>
<li><p>MLP</p></li>
</ul>
</li>
<li><p>More fusion patterns are supported and validated in the release, see table:</p></li>
</ul>
<table border="1" class="docutils">
<thead>
<tr>
<th>Fusion Patterns</th>
<th>Release</th>
</tr>
</thead>
<tbody>
<tr>
<td>Conv + Sum</td>
<td>v1.0</td>
</tr>
<tr>
<td>Conv + BN</td>
<td>v1.0</td>
</tr>
<tr>
<td>Conv + Relu</td>
<td>v1.0</td>
</tr>
<tr>
<td>Linear + Relu</td>
<td>v1.0</td>
</tr>
<tr>
<td>Conv + Eltwise</td>
<td>v1.1</td>
</tr>
<tr>
<td>Linear + Gelu</td>
<td>v1.1</td>
</tr>
</tbody>
</table><ul class="simple">
<li><p>Add docker support</p></li>
<li><p>[Alpha] Multi-node training with oneCCL support.</p></li>
<li><p>[Alpha] INT8 inference optimization.</p></li>
</ul>
</section>
<section id="performance">
<h3>Performance<a class="headerlink" href="#performance" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>The release has daily automated testing for the supported models: ResNet50, ResNext101, <a class="reference external" href="https://github.com/huggingface/transformers">Huggingface Bert</a>, <a class="reference external" href="https://github.com/intel/optimized-models/tree/master/pytorch/dlrm">DLRM</a>, <a class="reference external" href="https://github.com/XiaobingSuper/Resnext3d-for-video-classification">Resnext3d</a>, <a class="reference external" href="https://github.com/pytorch/fairseq/blob/master/fairseq/models/transformer.py">Transformer</a>. With the extension imported, it can bring up to 1.2x~1.7x BF16 over FP32 training performance improvements on the 3rd Gen Intel Xeon scalable processors (formerly codename Cooper Lake).</p></li>
</ul>
</section>
<section id="known-issue">
<h3>Known issue<a class="headerlink" href="#known-issue" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p>Some workloads may crash after several iterations on the extension with <a class="reference external" href="https://github.com/jemalloc/jemalloc">jemalloc</a> enabled.</p></li>
</ul>
</section>
</section>
<section id="id52">
<h2>1.0.2<a class="headerlink" href="#id52" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Rebase torch CCL patch to PyTorch 1.5.0-rc3</p></li>
</ul>
</section>
<section id="alpha">
<h2>1.0.1-Alpha<a class="headerlink" href="#alpha" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p>Static link oneDNN library</p></li>
<li><p>Check AVX512 build option</p></li>
<li><p>Fix the issue that cannot normally invoke <code class="docutils literal notranslate"><span class="pre">enable_auto_optimization</span></code></p></li>
</ul>
</section>
<section id="id53">
<h2>1.0.0-Alpha<a class="headerlink" href="#id53" title="Link to this heading"></a></h2>
<section id="id54">
<h3>What’s New<a class="headerlink" href="#id54" title="Link to this heading"></a></h3>
<ul>
<li><p>Auto Operator Optimization</p>
<p>Intel Extension for PyTorch will automatically optimize the operators of PyTorch when importing its python package. It will significantly improve the computation performance if the input tensor and the model is converted to the extension device.</p>
</li>
<li><p>Auto Mixed Precision
Currently, the extension has supported bfloat16. It streamlines the work to enable a bfloat16 model. The feature is controlled by <code class="docutils literal notranslate"><span class="pre">enable_auto_mix_precision</span></code>. If you enable it, the extension will run the operator with bfloat16 automatically to accelerate the operator computation.</p></li>
</ul>
</section>
<section id="performance-result">
<h3>Performance Result<a class="headerlink" href="#performance-result" title="Link to this heading"></a></h3>
<p>We collected the performance data of some models on the Intel Cooper Lake platform with 1 socket and 28 cores. Intel Cooper Lake introduced AVX512 BF16 instructions that could improve the bfloat16 computation significantly. The detail is as follows (The data is the speedup ratio and the baseline is upstream PyTorch).</p>
<table border="1" class="docutils">
<thead>
<tr>
<th style="text-align: center;"></th>
<th style="text-align: center;">Imperative - Operator Injection</th>
<th style="text-align: center;">Imperative - Mixed Precision</th>
<th style="text-align: center;">JIT- Operator Injection</th>
<th style="text-align: center;">JIT - Mixed Precision</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: center;">RN50</td>
<td style="text-align: center;">2.68</td>
<td style="text-align: center;">5.01</td>
<td style="text-align: center;">5.14</td>
<td style="text-align: center;">9.66</td>
</tr>
<tr>
<td style="text-align: center;">ResNet3D</td>
<td style="text-align: center;">3.00</td>
<td style="text-align: center;">4.67</td>
<td style="text-align: center;">5.19</td>
<td style="text-align: center;">8.39</td>
</tr>
<tr>
<td style="text-align: center;">BERT-LARGE</td>
<td style="text-align: center;">0.99</td>
<td style="text-align: center;">1.40</td>
<td style="text-align: center;">N/A</td>
<td style="text-align: center;">N/A</td>
</tr>
</tbody>
</table><p>We also measured the performance of ResNeXt101, Transformer-FB, DLRM, and YOLOv3 with the extension. We observed that the performance could be significantly improved by the extension as expected.</p>
</section>
<section id="id55">
<h3>Known issue<a class="headerlink" href="#id55" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/issues/10">#10</a> All data types have not been registered for DPCPP</p></li>
<li><p><a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/issues/37">#37</a> MaxPool can’t get nan result when input’s value is nan</p></li>
</ul>
</section>
<section id="note">
<h3>NOTE<a class="headerlink" href="#note" title="Link to this heading"></a></h3>
<p>The extension supported PyTorch v1.5.0-rc3. Support for other PyTorch versions is working in progress.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="performance.html" class="btn btn-neutral float-left" title="Performance" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="known_issues.html" class="btn btn-neutral float-right" title="Troubleshooting" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7cbf91ad4eb0> 
<p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a><a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a><a href='/#' data-wap_ref='dns' id='wap_dns'><small>| Your Privacy Choices</small><span style='height:10px;width:28px;display:inline-block;position:relative;'><svg style='position:absolute;width:28px;bottom:-2px;' version='1.1' id='Layer_1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' x='0px' y='0px' viewBox='0 0 30 14' xml:space='preserve'><title>California Consumer Privacy Act (CCPA) Opt-Out Icon</title><style type='text/css'> .st0 { fill-rule: evenodd; clip-rule: evenodd; fill: #FFFFFF; } .st1 { fill-rule: evenodd; clip-rule: evenodd; fill: #0066FF; } .st2 { fill: #FFFFFF; } .st3 { fill: #0066FF; } </style><g><g id='final---dec.11-2020_1_'><g id='_x30_208-our-toggle_2_' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2_2_' transform='translate(1275.000000, 200.000000)'><path class='st0' d='M7.4,12.8h6.8l3.1-11.6H7.4C4.2,1.2,1.6,3.8,1.6,7S4.2,12.8,7.4,12.8z'></path></g></g></g><g id='final---dec.11-2020'><g id='_x30_208-our-toggle' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2' transform='translate(1275.000000, 200.000000)'><path class='st1' d='M22.6,0H7.4c-3.9,0-7,3.1-7,7s3.1,7,7,7h15.2c3.9,0,7-3.1,7-7S26.4,0,22.6,0z M1.6,7c0-3.2,2.6-5.8,5.8-5.8 h9.9l-3.1,11.6H7.4C4.2,12.8,1.6,10.2,1.6,7z'></path><path id='x' class='st2' d='M24.6,4c0.2,0.2,0.2,0.6,0,0.8l0,0L22.5,7l2.2,2.2c0.2,0.2,0.2,0.6,0,0.8c-0.2,0.2-0.6,0.2-0.8,0 l0,0l-2.2-2.2L19.5,10c-0.2,0.2-0.6,0.2-0.8,0c-0.2-0.2-0.2-0.6,0-0.8l0,0L20.8,7l-2.2-2.2c-0.2-0.2-0.2-0.6,0-0.8 c0.2-0.2,0.6-0.2,0.8,0l0,0l2.2,2.2L23.8,4C24,3.8,24.4,3.8,24.6,4z'></path><path id='y' class='st3' d='M12.7,4.1c0.2,0.2,0.3,0.6,0.1,0.8l0,0L8.6,9.8C8.5,9.9,8.4,10,8.3,10c-0.2,0.1-0.5,0.1-0.7-0.1l0,0 L5.4,7.7c-0.2-0.2-0.2-0.6,0-0.8c0.2-0.2,0.6-0.2,0.8,0l0,0L8,8.6l3.8-4.5C12,3.9,12.4,3.9,12.7,4.1z'></path></g></g></g></g></svg></span></a><a href=https://www.intel.com/content/www/us/en/privacy/privacy-residents-certain-states.html data-wap_ref='nac' id='wap_nac'><small>| Notice at Collection</small></a></div><p></p><div>&copy; Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others. No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (OBSD), <a href='http://opensource.org/licenses/0BSD'>http://opensource.org/licenses/0BSD</a>.</div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>