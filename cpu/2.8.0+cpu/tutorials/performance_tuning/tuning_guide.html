

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Performance Tuning Guide &mdash; Intel&amp;#174 Extension for PyTorch* 2.8.0+cpu documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=01a6a0bb" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-pytorch"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Launch Script Usage Guide" href="launch_script.html" />
    <link rel="prev" title="API Documentation" href="../api_doc.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel&#174 Extension for PyTorch*
          </a>
<div class="version">
            <a href="../../../../">2.8.0+cpu ▼</a>
            <p>Click link above to switch version</p>
          </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ABOUT</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm.html">Large Language Models (LLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../known_issues.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheat_sheet.html">Cheat Sheet</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PERFORMANCE TUNING</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Performance Tuning Guide</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#overview">Overview</a></li>
<li class="toctree-l2"><a class="reference internal" href="#contents-of-this-document">Contents of this Document</a></li>
<li class="toctree-l2"><a class="reference internal" href="#hardware-configuration">Hardware Configuration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#intel-cpu-structure">Intel CPU Structure</a></li>
<li class="toctree-l3"><a class="reference internal" href="#non-uniform-memory-access-numa">Non-Uniform Memory Access (NUMA)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#software-configuration">Software Configuration</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#channels-last">Channels Last</a></li>
<li class="toctree-l3"><a class="reference internal" href="#numactl">Numactl</a></li>
<li class="toctree-l3"><a class="reference internal" href="#openmp">OpenMP</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#omp-num-threads">OMP_NUM_THREADS</a></li>
<li class="toctree-l4"><a class="reference internal" href="#omp-thread-limit">OMP_THREAD_LIMIT</a></li>
<li class="toctree-l4"><a class="reference internal" href="#gnu-openmp">GNU OpenMP</a></li>
<li class="toctree-l4"><a class="reference internal" href="#intel-openmp">Intel OpenMP</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#memory-allocator">Memory Allocator</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#jemalloc">Jemalloc</a></li>
<li class="toctree-l4"><a class="reference internal" href="#tcmalloc">TCMalloc</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#denormal-number">Denormal Number</a></li>
<li class="toctree-l3"><a class="reference internal" href="#onednn-primitive-cache">OneDNN primitive cache</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="launch_script.html">Launch Script Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="torchserve.html">TorchServe with Intel® Extension for PyTorch*</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CONTRIBUTING GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel&#174 Extension for PyTorch*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Performance Tuning Guide</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/performance_tuning/tuning_guide.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="performance-tuning-guide">
<h1>Performance Tuning Guide<a class="headerlink" href="#performance-tuning-guide" title="Link to this heading"></a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading"></a></h2>
<p>Intel® Extension for PyTorch* is a Python package to extend official PyTorch. It makes the out-of-box user experience of PyTorch CPU better while achieving good performance. To fully utilize the power of Intel® architecture and thus yield high performance, PyTorch, as well as Intel® Extension for PyTorch*, are powered by <a class="reference external" href="https://github.com/oneapi-src/oneDNN">oneAPI Deep Neural Network Library (oneDNN)</a>, an open-source cross-platform performance library of basic building blocks for deep learning applications. It is developed and optimized for Intel Architecture Processors, Intel Processor Graphics, and Xe architecture-based Graphics.</p>
<p>Although default primitives of PyTorch and Intel® Extension for PyTorch* are highly optimized, there are things users can do improve performance. Most optimized configurations can be automatically set by the launcher script. This article introduces common methods recommended by Intel developers.</p>
</section>
<section id="contents-of-this-document">
<h2>Contents of this Document<a class="headerlink" href="#contents-of-this-document" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="#hardware-configuration">Hardware Configuration</a></p>
<ul>
<li><p><a class="reference external" href="#intel-cpu-structure">Intel CPU Structure</a></p></li>
<li><p><a class="reference external" href="#non-uniform-memory-access-numa">Non-Uniform Memory Access (NUMA)</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#software-configuration">Software Configuration</a></p>
<ul>
<li><p><a class="reference external" href="#channels-last">Channels Last</a></p></li>
<li><p><a class="reference external" href="#numactl">Numactl</a></p></li>
<li><p><a class="reference external" href="#openmp">OpenMP</a></p>
<ul>
<li><p><a class="reference external" href="#omp-num-threads">OMP_NUM_THREADS</a></p></li>
<li><p><a class="reference external" href="#omp-thread-limit">OMP_THREAD_LIMIT</a></p></li>
<li><p><a class="reference external" href="#gnu-openmp">GNU OpenMP</a></p></li>
<li><p><a class="reference external" href="#intel-openmp">Intel OpenMP</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#memory-allocator">Memory Allocator</a></p>
<ul>
<li><p><a class="reference external" href="#jemalloc">Jemalloc</a></p></li>
<li><p><a class="reference external" href="#tcmalloc">TCMalloc</a></p></li>
</ul>
</li>
<li><p><a class="reference external" href="#denormal-number">Denormal Number</a></p></li>
<li><p><a class="reference external" href="#onednn-primitive-cache">OneDNN primitive cache</a></p></li>
</ul>
</li>
</ul>
</section>
<section id="hardware-configuration">
<h2>Hardware Configuration<a class="headerlink" href="#hardware-configuration" title="Link to this heading"></a></h2>
<p>This section briefly introduces the structure of Intel CPUs, as well as concept of Non-Uniform Memory Access (NUMA).</p>
<section id="intel-cpu-structure">
<h3>Intel CPU Structure<a class="headerlink" href="#intel-cpu-structure" title="Link to this heading"></a></h3>
<p>There are many families of Intel CPUs. We’ll use Intel® Xeon® processor Scalable family as an example to discuss an Intel CPU and how it works. Understanding this background knowledge is helpful to understand the PyTorch optimization methodologies that Intel engineers recommend.</p>
<p>On the Intel® Xeon® Scalable Processors with Intel® C620 Series Chipsets, (formerly Purley) platform, each chip provides up to 28 cores. Each core has a non-inclusive last-level cache and an 1MB L2 cache. The CPU features fast 2666 MHz DDR4 memory, six memory channels per CPU, Intel Ultra Path Interconnect (UPI) high speed point-to-point processor interconnect, and more. Figure 1 shows microarchitecture of the Intel® Xeon® processor Scalable family chips. Each CPU chip consists of a number of cores, along with core-specific cache. 6 channels of DDR4 memory are connected to the chip directly. Meanwhile, chips communicates through the Intel UPI interconnect, which features a transfer speed of up to 10.4 GT/s.</p>
<div align="center"><p><img alt="Block Diagram of the Intel® Xeon® processor Scalable family microarchitecture" src="../../_images/block_diagram_xeon_architecture.png" /></p>
<p>Figure 1: Block Diagram of the Intel® Xeon® processor Scalable family microarchitecture.</p>
</div><p>Usually, a CPU chip is called a socket. A typical two-socket configuration is illustrated as Figure 2. Two CPU sockets are equipped on one motherboard. Each socket is connected to up to 6 channels of memory, called its local memory, from socket perspective. Sockets are connected to each other via Intel UPI. It is possible for each socket to access memories attached on other sockets, usually called remote memory access. Local memory access is always faster than remote memory access. Meanwhile, cores on one socket share a space of high speed cache memory, which is much faster than communication via Intel UPI. Figure 3 shows an ASUS Z11PA-D8 Intel® Xeon® server motherboard, equipping with two sockets for Intel® Xeon® processor Scalable family CPUs.</p>
<div align="center"><p><img alt="Typical two-socket configuration" src="../../_images/two_socket_config.png" /></p>
<p>Figure 2: Typical two-socket configuration.</p>
<p><img alt="ASUS Z11PA-D8 Intel® Xeon® server motherboard" src="https://dlcdnimgs.asus.com/websites/global/products/MCCApMgGOdr9WJxN/MB-Z11PAD8-overview-01-s.jpg" /></p>
<p>Figure 3: An ASUS Z11PA-D8 Intel® Xeon® server motherboard. It contains two sockets for Intel® Xeon® processor Scalable family CPUs.</p>
</div></section>
<section id="non-uniform-memory-access-numa">
<h3>Non-Uniform Memory Access (NUMA)<a class="headerlink" href="#non-uniform-memory-access-numa" title="Link to this heading"></a></h3>
<p>It is a good thing that more and more CPU cores are provided to users in one socket, because this brings more computation resources. However, this also brings memory access competitions. Program can stall because memory is busy to visit. To address this problem, Non-Uniform Memory Access (NUMA) was introduced. Comparing to Uniform Memory Access (UMA), in which scenario all memories are connected to all cores equally, NUMA tells memories into multiple groups. Certain number of memories are directly attached to one socket’s integrated memory controller to become local memory of this socket. As described in the previous section, local memory access is much faster than remote memory access.</p>
<p>Users can get CPU information with <code class="docutils literal notranslate"><span class="pre">lscpu</span></code> command on Linux to learn how many cores, sockets there on the machine. Also, NUMA information like how CPU cores are distributed can also be retrieved. The following is an example of <code class="docutils literal notranslate"><span class="pre">lscpu</span></code> execution on a machine with two Intel(R) Xeon(R) Platinum 8180M CPUs. 2 sockets were detected. Each socket has 28 physical cores onboard. Since Hyper-Threading is enabled, each core can run 2 threads. I.e. each socket has another 28 logical cores. Thus, there are 112 CPU cores on service. When indexing CPU cores, usually physical cores are indexed before logical core. In this case, the first 28 cores (0-27) are physical cores on the first NUMA socket (node), the second 28 cores (28-55) are physical cores on the second NUMA socket (node). Logical cores are indexed afterward. 56-83 are 28 logical cores on the first NUMA socket (node), 84-111 are the second 28 logical cores on the second NUMA socket (node). Typically, running Intel® Extension for PyTorch* should avoid using logical cores to get a good performance.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ lscpu
...
CPU(s):              112
On-line CPU(s) list: 0-111
Thread(s) per core:  2
Core(s) per socket:  28
Socket(s):           2
NUMA node(s):        2
...
Model name:          Intel(R) Xeon(R) Platinum 8180M CPU @ 2.50GHz
...
NUMA node0 CPU(s):   0-27,56-83
NUMA node1 CPU(s):   28-55,84-111
...
</pre></div>
</div>
</section>
</section>
<section id="software-configuration">
<h2>Software Configuration<a class="headerlink" href="#software-configuration" title="Link to this heading"></a></h2>
<p>This section introduces software configurations that helps to boost performance.</p>
<section id="channels-last">
<h3>Channels Last<a class="headerlink" href="#channels-last" title="Link to this heading"></a></h3>
<p>Take advantage of <strong>Channels Last</strong> memory format for image processing tasks. Comparing to PyTorch default NCHW (<code class="docutils literal notranslate"><span class="pre">torch.contiguous_format</span></code>) memory format, NHWC (<code class="docutils literal notranslate"><span class="pre">torch.channels_last</span></code>) is more friendly to Intel platforms, and thus generally yields better performance. More detailed introduction can be found at <a class="reference internal" href="../features/nhwc.html"><span class="doc">Channels Last page</span></a>. You can get sample codes with Resnet50 at <a class="reference internal" href="../examples.html"><span class="doc">Example page</span></a>.</p>
</section>
<section id="numactl">
<h3>Numactl<a class="headerlink" href="#numactl" title="Link to this heading"></a></h3>
<p>Since NUMA largely influences memory access performance, this functionality should also be implemented in software side.</p>
<p>During development of Linux kernels, more and more sophisticated implementations/optimizations/strategies had been brought out. Version 2.5 of the Linux kernel already contained basic NUMA support, which was further improved in subsequent kernel releases. Version 3.8 of the Linux kernel brought a new NUMA foundation that allowed development of more efficient NUMA policies in later kernel releases. Version 3.13 of the Linux kernel brought numerous policies that aim at putting a process near its memory, together with the handling of cases such as having memory pages shared between processes, or the use of transparent huge pages. New sysctl settings allow NUMA balancing to be enabled or disabled, as well as the configuration of various NUMA memory balancing parameters.[1] Behavior of Linux kernels are thus different according to kernel version. Newer Linux kernels may contain further optimizations of NUMA strategies, and thus have better performances. For some workloads, NUMA strategy influences performance great.</p>
<p>Linux provides a tool, <code class="docutils literal notranslate"><span class="pre">numactl</span></code>, that allows user control of NUMA policy for processes or shared memory. It runs processes with a specific NUMA scheduling or memory placement policy. As described in previous section, cores share high-speed cache in one socket, thus it is a good idea to avoid cross socket computations. From a memory access perspective, bounding memory access locally is much faster than accessing remote memories.</p>
<p>The following is an example of numactl usage to run a workload on the Nth socket and limit memory access to its local memories on the Nth socket. More detailed description of numactl command can be found <a class="reference external" href="https://linux.die.net/man/8/numactl">on the numactl man page</a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">numactl</span> <span class="pre">--cpunodebind</span> <span class="pre">N</span> <span class="pre">--membind</span> <span class="pre">N</span> <span class="pre">python</span> <span class="pre">&lt;script&gt;</span></code></p>
<p>Assume core 0-3 are on socket 0, the following command binds script execution on core 0-3, and binds memory access to socket 0 local memories.</p>
<p><code class="docutils literal notranslate"><span class="pre">numactl</span> <span class="pre">--membind</span> <span class="pre">0</span> <span class="pre">-C</span> <span class="pre">0-3</span> <span class="pre">python</span> <span class="pre">&lt;script&gt;</span></code></p>
<p>[1] <a class="reference external" href="https://en.wikipedia.org/wiki/Non-uniform_memory_access">Wikipedia - Non-uniform memory access</a></p>
</section>
<section id="openmp">
<h3>OpenMP<a class="headerlink" href="#openmp" title="Link to this heading"></a></h3>
<p>OpenMP is an implementation of multithreading, a method of parallelizing where a primary thread (a series of instructions executed consecutively) forks a specified number of sub-threads and the system divides a task among them. The threads then run concurrently, with the runtime environment allocating threads to different processors.[2] Figure 4 illustrates fork-join model of OpenMP execution.</p>
<div align="center"><p><img alt="A number of parallel block execution threads are forked from primary thread" src="https://upload.wikimedia.org/wikipedia/commons/thumb/f/f1/Fork_join.svg/1920px-Fork_join.svg.png" /></p>
<p>Figure 4: A number of parallel block execution threads are forked from primary thread.</p>
</div><p>Users can control OpenMP behaviors through some environment variables to fit for their workloads. Also, beside GNU OpenMP library (<a class="reference external" href="https://gcc.gnu.org/onlinedocs/libgomp/">libgomp</a>), Intel provides another OpenMP implementation <a class="reference external" href="https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support.html">libiomp</a> for users to choose from. Environment variables that control behavior of OpenMP threads may differ from libgomp and libiomp. They will be introduced separately in sections below.</p>
<p>GNU OpenMP (libgomp) is the default multi-threading library for both PyTorch and Intel® Extension for PyTorch*.</p>
<p>[2] <a class="reference external" href="https://en.wikipedia.org/wiki/OpenMP">Wikipedia - OpenMP</a></p>
<section id="omp-num-threads">
<h4>OMP_NUM_THREADS<a class="headerlink" href="#omp-num-threads" title="Link to this heading"></a></h4>
<p>Environment variable <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> sets the number of threads used for parallel regions. By default, it is set to be the number of available physical cores. It can be used along with numactl settings, as the following example. If cores 0-3 are on socket 0, this example command runs &lt;script&gt; on cores 0-3, with 4 OpenMP threads.</p>
<p>This environment variable works on both libgomp and libiomp.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">OMP_NUM_THREADS</span><span class="o">=</span><span class="mi">4</span>
<span class="n">numactl</span> <span class="o">-</span><span class="n">C</span> <span class="mi">0</span><span class="o">-</span><span class="mi">3</span> <span class="o">--</span><span class="n">membind</span> <span class="mi">0</span> <span class="n">python</span> <span class="o">&lt;</span><span class="n">script</span><span class="o">&gt;</span>
</pre></div>
</div>
</section>
<section id="omp-thread-limit">
<h4>OMP_THREAD_LIMIT<a class="headerlink" href="#omp-thread-limit" title="Link to this heading"></a></h4>
<p>Environment variable <code class="docutils literal notranslate"><span class="pre">OMP_THREAD_LIMIT</span></code> specifies the number of threads to use for the whole program. The value of this variable shall be a positive integer. If undefined, the number of threads is not limited.</p>
<p>Please make sure <code class="docutils literal notranslate"><span class="pre">OMP_THREAD_LIMIT</span></code> is set to a number equal to or larger than <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code> to avoid backward propagation hanging issues.</p>
</section>
<section id="gnu-openmp">
<h4>GNU OpenMP<a class="headerlink" href="#gnu-openmp" title="Link to this heading"></a></h4>
<p>Beside <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code>, other GNU OpenMP specific environment variables are commonly used to improve performance:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">GOMP_CPU_AFFINITY</span></code>: Binds threads to specific CPUs. The variable should contain a space-separated or comma-separated list of CPUs.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OMP_PROC_BIND</span></code>: Specifies whether threads may be moved between processors. Setting it to CLOSE keeps OpenMP threads close to the primary thread in contiguous place partitions.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">OMP_SCHEDULE</span></code>: Determine how OpenMP threads are scheduled.</p></li>
</ul>
<p>Here are recommended settings of these environment variables:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">GOMP_CPU_AFFINITY</span><span class="o">=</span><span class="s2">&quot;0-3&quot;</span>
<span class="n">export</span> <span class="n">OMP_PROC_BIND</span><span class="o">=</span><span class="n">CLOSE</span>
<span class="n">export</span> <span class="n">OMP_SCHEDULE</span><span class="o">=</span><span class="n">STATIC</span>
</pre></div>
</div>
</section>
<section id="intel-openmp">
<h4>Intel OpenMP<a class="headerlink" href="#intel-openmp" title="Link to this heading"></a></h4>
<p>By default, PyTorch uses GNU OpenMP (GNU libgomp) for parallel computation. On Intel platforms, Intel OpenMP Runtime Library (libiomp) provides OpenMP API specification support. It sometimes brings more performance benefits compared to libgomp. Utilizing environment variable <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> can switch OpenMP library to libiomp:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>pip install intel-openmp=2024.1.2
export LD_PRELOAD=&lt;path&gt;/libiomp5.so:$LD_PRELOAD
</pre></div>
</div>
<p>Similar to GNU OpenMP, beside <code class="docutils literal notranslate"><span class="pre">OMP_NUM_THREADS</span></code>, there are other Intel OpenMP specific environment variables that control behavior of OpenMP threads:</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">KMP_AFFINITY</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">KMP_AFFINITY</span></code> controls how to to bind OpenMP threads to physical processing units. Depending on the system (machine) topology, application, and operating system, thread affinity can have a dramatic effect on the application speed.</p>
<p>A common usage scenario is to bind consecutive threads close together, as is done with KMP_AFFINITY=compact. By doing this, communication overhead, cache line invalidation overhead, and page thrashing are minimized. Now, suppose the application also had a number of parallel regions that did not utilize all of the available OpenMP threads. A thread normally executes faster on a core where it is not competing for resources with another active thread on the same core. It is always good to avoid binding multiple threads to the same core while leaving other cores unused. This can be achieved by the following command. Figure 5 illustrates this strategy.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">KMP_AFFINITY</span><span class="o">=</span><span class="n">granularity</span><span class="o">=</span><span class="n">fine</span><span class="p">,</span><span class="n">compact</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span>
</pre></div>
</div>
<div align="center"><p><img alt="KMP_AFFINITY=granularity=fine,compact,1,0" src="../../_images/kmp_affinity.jpg" /></p>
<p>Figure 5: <em>KMP_AFFINITY=granularity=fine,compact,1,0</em></p>
</div><p>The OpenMP thread n+1 is bound to a thread context as close as possible to OpenMP thread n, but on a different core. Once each core has been assigned one OpenMP thread, the subsequent OpenMP threads are assigned to the available cores in the same order, but they are assigned on different thread contexts.</p>
<p>It is also possible to bind OpenMP threads to certain CPU cores with the following command.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">KMP_AFFINITY</span><span class="o">=</span><span class="n">granularity</span><span class="o">=</span><span class="n">fine</span><span class="p">,</span><span class="n">proclist</span><span class="o">=</span><span class="p">[</span><span class="n">N</span><span class="o">-</span><span class="n">M</span><span class="p">],</span><span class="n">explicit</span>
</pre></div>
</div>
<p>More detailed information about <code class="docutils literal notranslate"><span class="pre">KMP_AFFINITY</span></code> can be found in the <a class="reference external" href="https://software.intel.com/content/www/us/en/develop/documentation/cpp-compiler-developer-guide-and-reference/top/optimization-and-programming-guide/openmp-support/openmp-library-support/thread-affinity-interface-linux-and-windows.html">Intel CPP Compiler Developer Guide</a>.</p>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">KMP_BLOCKTIME</span></code></p>
<p><code class="docutils literal notranslate"><span class="pre">KMP_BLOCKTIME</span></code> sets the time, in milliseconds, that a thread, after completing the execution of a parallel region, should wait before sleeping. The default value is 200ms.</p>
<p>After completing the execution of a parallel region, threads wait for new parallel work to become available. After a certain period of time has elapsed, they stop waiting and sleep. Sleeping allows the threads to be used, until more parallel work becomes available, by non-OpenMP threaded code that may execute between parallel regions, or by other applications. A small <code class="docutils literal notranslate"><span class="pre">KMP_BLOCKTIME</span></code> value may offer better overall performance if application contains non-OpenMP threaded code that executes between parallel regions. A larger <code class="docutils literal notranslate"><span class="pre">KMP_BLOCKTIME</span></code> value may be more appropriate if threads are to be reserved solely for use for OpenMP execution, but may penalize other concurrently-running OpenMP or threaded applications. It is suggested to be set to 0 or 1 for convolutional neural network (CNN) based models.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">KMP_BLOCKTIME</span><span class="o">=</span><span class="mi">0</span> <span class="p">(</span><span class="ow">or</span> <span class="mi">1</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
</section>
</section>
<section id="memory-allocator">
<h3>Memory Allocator<a class="headerlink" href="#memory-allocator" title="Link to this heading"></a></h3>
<p>Memory allocator plays an important role from performance perspective as well. A more efficient memory usage reduces overhead on unnecessary memory allocations or destructions, and thus results in a faster execution. From practical experiences, for deep learning workloads, Jemalloc or TCMalloc can get better performance by reusing memory as much as possible than default malloc function.</p>
<p>It is as simple as adding path of Jemalloc/TCMalloc dynamic library to environment variable <code class="docutils literal notranslate"><span class="pre">LD_PRELOAD</span></code> to switch memory allocator to one of them.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>export LD_PRELOAD=&lt;jemalloc.so/tcmalloc.so&gt;:$LD_PRELOAD
</pre></div>
</div>
<section id="jemalloc">
<h4>Jemalloc<a class="headerlink" href="#jemalloc" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://github.com/jemalloc/jemalloc">Jemalloc</a> is a general purpose malloc implementation that emphasizes fragmentation avoidance and scalable concurrency support. More detailed introduction of performance tuning with Jemalloc can be found at <a class="reference external" href="https://android.googlesource.com/platform/external/jemalloc_new/+/6e6a93170475c05ebddbaf3f0df6add65ba19f01/TUNING.md">Jemalloc tuning guide</a></p>
<p>A recommended setting for <code class="docutils literal notranslate"><span class="pre">MALLOC_CONF</span></code> is <code class="docutils literal notranslate"><span class="pre">oversize_threshold:1,background_thread:true,metadata_thp:auto,dirty_decay_ms:9000000000,muzzy_decay_ms:9000000000</span></code> from performance perspective. However, in some cases the <code class="docutils literal notranslate"><span class="pre">dirty_decay_ms:9000000000,mmuzzy_decay_ms:9000000000</span></code> may cause Out-of-Memory crash. Try <code class="docutils literal notranslate"><span class="pre">oversize_threshold:1,background_thread:true,metadata_thp:auto</span></code> instead in this case.</p>
<p>Getting Jemalloc is straight-forward.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">conda</span><span class="o">-</span><span class="n">forge</span> <span class="n">jemalloc</span>
</pre></div>
</div>
</section>
<section id="tcmalloc">
<h4>TCMalloc<a class="headerlink" href="#tcmalloc" title="Link to this heading"></a></h4>
<p><a class="reference external" href="https://github.com/google/tcmalloc">TCMalloc</a> also features a couple of optimizations to speed up program executions. One of them is holding memory in caches to speed up access of commonly-used objects. Holding such caches even after deallocation also helps avoid costly system calls if such memory is later re-allocated. It is part of <a class="reference external" href="https://github.com/gperftools/gperftools">gpertools</a>, a collection of a high-performance multi-threaded malloc() implementation, plus some pretty nifty performance analysis tools.</p>
<p>Getting TCMalloc is also not complicated.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conda</span> <span class="n">install</span> <span class="o">-</span><span class="n">c</span> <span class="n">conda</span><span class="o">-</span><span class="n">forge</span> <span class="n">gperftools</span>
</pre></div>
</div>
</section>
</section>
<section id="denormal-number">
<h3>Denormal Number<a class="headerlink" href="#denormal-number" title="Link to this heading"></a></h3>
<p><a class="reference external" href="https://en.wikipedia.org/wiki/Denormal_number">Denormal number</a> is used to store extremely small numbers that are close to 0. Computations with denormal numbers are remarkably slower than normalized number. To solve the low performance issue caused by denormal numbers, users can use the following PyTorch API function.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">set_flush_denormal</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="onednn-primitive-cache">
<h3>OneDNN primitive cache<a class="headerlink" href="#onednn-primitive-cache" title="Link to this heading"></a></h3>
<p>Intel® Extension for PyTorch* is using OneDNN backend for those most computing bound PyTorch operators such as Linear and Convolution.</p>
<p>To achieve better performance, OneDNN backend is using its <a class="reference external" href="https://oneapi-src.github.io/oneDNN/dev_guide_primitive_cache.html">primitive cache</a> to store those created primitives for different input shapes during warm-up stage (default primitive cache size is 1024, i.e., 1024 cached primitives). Therefore, when the total size of the primitives created by all the input shapes is within the default threshold, Intel® Extension for PyTorch* could get fully computation performance from OneDNN kernels.</p>
<p>Different input shapes usualy come from dynamic shapes of datasets. Dynamic shapes commonly exist in <a class="reference external" href="https://github.com/matterport/Mask_RCNN">MaskRCNN model</a> (object detection), <a class="reference external" href="https://github.com/huggingface/transformers/">Transformers</a> Wav2vec2 model (speech-recognition) and other speech/text-generation related Transformers models.</p>
<p>However, we might meet the fact that model would need to cache a large amount of various input shapes, which would even exceed the default primitive cache size. In such case, we recommend tuning the OneDNN primitive cache by setting <code class="docutils literal notranslate"><span class="pre">ONEDNN_PRIMITIVE_CACHE_CAPACITY</span></code> environment variable to get better performance (Note that it is at the cost of increased memory usage):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">export</span> <span class="n">ONEDNN_PRIMITIVE_CACHE_CAPACITY</span> <span class="o">=</span> <span class="p">{</span><span class="n">Tuning</span> <span class="n">size</span><span class="p">}</span>
<span class="o">//</span><span class="n">Note</span> <span class="n">that</span> <span class="p">{</span><span class="n">Tuning</span> <span class="n">size</span><span class="p">}</span> <span class="n">has</span> <span class="n">an</span> <span class="n">upper</span> <span class="n">limit</span> <span class="mi">65536</span> <span class="n">cached</span> <span class="n">primitives</span>
</pre></div>
</div>
<p>Take Transformers <a class="reference external" href="https://github.com/huggingface/transformers/tree/main/examples/pytorch/speech-recognition">Wav2vec2 for speech-recognition</a> as an example, the dataset “common voice” used for inference has a large amount of difference shapes for Convolution operator. In our experiment, the best primitive cache size is 4096, and the model runs with its full speed after being warmed up with inputs of all the shape sizes.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../api_doc.html" class="btn btn-neutral float-left" title="API Documentation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="launch_script.html" class="btn btn-neutral float-right" title="Launch Script Usage Guide" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7cbf915f5780> 
<p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a><a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a><a href='/#' data-wap_ref='dns' id='wap_dns'><small>| Your Privacy Choices</small><span style='height:10px;width:28px;display:inline-block;position:relative;'><svg style='position:absolute;width:28px;bottom:-2px;' version='1.1' id='Layer_1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' x='0px' y='0px' viewBox='0 0 30 14' xml:space='preserve'><title>California Consumer Privacy Act (CCPA) Opt-Out Icon</title><style type='text/css'> .st0 { fill-rule: evenodd; clip-rule: evenodd; fill: #FFFFFF; } .st1 { fill-rule: evenodd; clip-rule: evenodd; fill: #0066FF; } .st2 { fill: #FFFFFF; } .st3 { fill: #0066FF; } </style><g><g id='final---dec.11-2020_1_'><g id='_x30_208-our-toggle_2_' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2_2_' transform='translate(1275.000000, 200.000000)'><path class='st0' d='M7.4,12.8h6.8l3.1-11.6H7.4C4.2,1.2,1.6,3.8,1.6,7S4.2,12.8,7.4,12.8z'></path></g></g></g><g id='final---dec.11-2020'><g id='_x30_208-our-toggle' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2' transform='translate(1275.000000, 200.000000)'><path class='st1' d='M22.6,0H7.4c-3.9,0-7,3.1-7,7s3.1,7,7,7h15.2c3.9,0,7-3.1,7-7S26.4,0,22.6,0z M1.6,7c0-3.2,2.6-5.8,5.8-5.8 h9.9l-3.1,11.6H7.4C4.2,12.8,1.6,10.2,1.6,7z'></path><path id='x' class='st2' d='M24.6,4c0.2,0.2,0.2,0.6,0,0.8l0,0L22.5,7l2.2,2.2c0.2,0.2,0.2,0.6,0,0.8c-0.2,0.2-0.6,0.2-0.8,0 l0,0l-2.2-2.2L19.5,10c-0.2,0.2-0.6,0.2-0.8,0c-0.2-0.2-0.2-0.6,0-0.8l0,0L20.8,7l-2.2-2.2c-0.2-0.2-0.2-0.6,0-0.8 c0.2-0.2,0.6-0.2,0.8,0l0,0l2.2,2.2L23.8,4C24,3.8,24.4,3.8,24.6,4z'></path><path id='y' class='st3' d='M12.7,4.1c0.2,0.2,0.3,0.6,0.1,0.8l0,0L8.6,9.8C8.5,9.9,8.4,10,8.3,10c-0.2,0.1-0.5,0.1-0.7-0.1l0,0 L5.4,7.7c-0.2-0.2-0.2-0.6,0-0.8c0.2-0.2,0.6-0.2,0.8,0l0,0L8,8.6l3.8-4.5C12,3.9,12.4,3.9,12.7,4.1z'></path></g></g></g></g></svg></span></a><a href=https://www.intel.com/content/www/us/en/privacy/privacy-residents-certain-states.html data-wap_ref='nac' id='wap_nac'><small>| Notice at Collection</small></a></div><p></p><div>&copy; Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others. No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (OBSD), <a href='http://opensource.org/licenses/0BSD'>http://opensource.org/licenses/0BSD</a>.</div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>