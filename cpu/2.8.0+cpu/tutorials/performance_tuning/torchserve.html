

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>TorchServe with Intel® Extension for PyTorch* &mdash; Intel&amp;#174 Extension for PyTorch* 2.8.0+cpu documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=01a6a0bb" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-pytorch"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Contribution" href="../contribution.html" />
    <link rel="prev" title="Launch Script Usage Guide" href="launch_script.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel&#174 Extension for PyTorch*
          </a>
<div class="version">
            <a href="../../../../">2.8.0+cpu ▼</a>
            <p>Click link above to switch version</p>
          </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ABOUT</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm.html">Large Language Models (LLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../known_issues.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheat_sheet.html">Cheat Sheet</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PERFORMANCE TUNING</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="launch_script.html">Launch Script Usage Guide</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">TorchServe with Intel® Extension for PyTorch*</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#contents-of-this-document">Contents of this Document</a></li>
<li class="toctree-l2"><a class="reference internal" href="#install-intel-extension-for-pytorch">Install Intel® Extension for PyTorch*</a></li>
<li class="toctree-l2"><a class="reference internal" href="#serving-model-with-intel-extension-for-pytorch">Serving model with Intel® Extension for PyTorch*</a></li>
<li class="toctree-l2"><a class="reference internal" href="#torchserve-with-launcher">TorchServe with Launcher</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#launcher-core-pinning-to-boost-performance-of-torchserve-multi-worker-inference">Launcher Core Pinning to Boost Performance of TorchServe Multi Worker Inference</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#scaling-workers">Scaling workers</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#creating-and-exporting-int8-model-for-intel-extension-for-pytorch">Creating and Exporting INT8 model for Intel® Extension for PyTorch*</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#creating-a-serialized-file">1. Creating a serialized file</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#bert">BERT</a></li>
<li class="toctree-l4"><a class="reference internal" href="#resnet50">ResNet50</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="#creating-a-model-archive">2. Creating a Model Archive</a></li>
<li class="toctree-l3"><a class="reference internal" href="#start-torchserve-to-serve-the-model">3. Start TorchServe to serve the model</a></li>
<li class="toctree-l3"><a class="reference internal" href="#registering-and-deploying-model">4. Registering and Deploying model</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#benchmarking-with-launcher">Benchmarking with Launcher</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#benchmarking-with-launcher-core-pinning">Benchmarking with Launcher Core Pinning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#performance-boost-with-intel-extension-for-pytorch-and-launcher">Performance Boost with Intel® Extension for PyTorch* and Launcher</a></li>
</ul>
</li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CONTRIBUTING GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel&#174 Extension for PyTorch*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">TorchServe with Intel® Extension for PyTorch*</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/performance_tuning/torchserve.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="torchserve-with-intel-extension-for-pytorch">
<h1>TorchServe with Intel® Extension for PyTorch*<a class="headerlink" href="#torchserve-with-intel-extension-for-pytorch" title="Link to this heading"></a></h1>
<p>TorchServe can be used with Intel® Extension for PyTorch* to give performance boost on Intel hardware.<sup>1</sup>
Here we show how to use TorchServe with Intel® Extension for PyTorch*.</p>
<p><sup>1. While Intel® Extension for PyTorch* benefits all platforms, platforms with AVX512 benefit the most. </sup></p>
<section id="contents-of-this-document">
<h2>Contents of this Document<a class="headerlink" href="#contents-of-this-document" title="Link to this heading"></a></h2>
<ul class="simple">
<li><p><a class="reference external" href="#install-intel-extension-for-pytorch">Install Intel® Extension for PyTorch*</a></p></li>
<li><p><a class="reference external" href="#serving-model-with-intel-extension-for-pytorch">Serving model with Intel® Extension for PyTorch*</a></p></li>
<li><p><a class="reference external" href="#torchserve-with-launcher">TorchServe with Launcher</a></p></li>
<li><p><a class="reference external" href="#creating-and-exporting-int8-model-for-intel-extension-for-pytorch">Creating and Exporting INT8 model for Intel® Extension for PyTorch*</a></p></li>
<li><p><a class="reference external" href="#benchmarking-with-launcher">Benchmarking with Launcher</a></p></li>
<li><p><a class="reference external" href="#performance-boost-with-intel-extension-for-pytorch-and-launcher">Performance Boost with Intel® Extension for PyTorch* and Launcher</a></p></li>
</ul>
</section>
<section id="install-intel-extension-for-pytorch">
<h2>Install Intel® Extension for PyTorch*<a class="headerlink" href="#install-intel-extension-for-pytorch" title="Link to this heading"></a></h2>
<p>Refer to the documentation <a class="reference internal" href="../installation.html"><span class="doc">here</span></a>.</p>
</section>
<section id="serving-model-with-intel-extension-for-pytorch">
<h2>Serving model with Intel® Extension for PyTorch*<a class="headerlink" href="#serving-model-with-intel-extension-for-pytorch" title="Link to this heading"></a></h2>
<p>After installation, all it needs to use TorchServe with Intel® Extension for PyTorch* is to enable it in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipex_enable</span><span class="o">=</span><span class="n">true</span>
</pre></div>
</div>
<p>Once Intel® Extension for PyTorch* is enabled, deploying PyTorch model follows the same procedure shown <a class="reference external" href="https://pytorch.org/serve/use_cases.html">here</a>. TorchServe with Intel® Extension for PyTorch* can deploy any model and do inference.</p>
</section>
<section id="torchserve-with-launcher">
<h2>TorchServe with Launcher<a class="headerlink" href="#torchserve-with-launcher" title="Link to this heading"></a></h2>
<p>Launcher is a script to automate the process of tunining configuration setting on Intel hardware to boost performance. Tuning configurations such as OMP_NUM_THREADS, thread affinity, memory allocator can have a dramatic effect on performance. Refer to <a class="reference internal" href="tuning_guide.html"><span class="doc">Performance Tuning Guide</span></a> and <a class="reference internal" href="launch_script.html"><span class="doc">Launch Script Usage Guide</span></a> for details on performance tuning with launcher.</p>
<p>All it needs to use TorchServe with launcher is to set its configuration in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code>.</p>
<p>Add the following lines in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code> to use launcher with its default configuration.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipex_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
</pre></div>
</div>
<p>Launcher by default uses <code class="docutils literal notranslate"><span class="pre">numactl</span></code> if it’s installed to ensure socket is pinned and thus memory is allocated from local numa node. To use launcher without numactl, add the following lines in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipex_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_args</span><span class="o">=--</span><span class="n">disable_numactl</span>
</pre></div>
</div>
<p>Launcher by default uses only non-hyperthreaded cores if hyperthreading is present to avoid core compute resource sharing. To use launcher with all cores, both physical and logical, add the following lines in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipex_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_args</span><span class="o">=--</span><span class="n">use_logical_core</span>
</pre></div>
</div>
<p>Below is an example of passing multiple args to <code class="docutils literal notranslate"><span class="pre">cpu_launcher_args</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipex_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_args</span><span class="o">=--</span><span class="n">use_logical_core</span> <span class="o">--</span><span class="n">disable_numactl</span>
</pre></div>
</div>
<p>Below are some useful <code class="docutils literal notranslate"><span class="pre">cpu_launcher_args</span></code> to note. Italic values are default if applicable.</p>
<ol class="simple">
<li><p>Memory Allocator: [ PTMalloc <code class="docutils literal notranslate"><span class="pre">--use_default_allocator</span></code> | <em>TCMalloc <code class="docutils literal notranslate"><span class="pre">--enable_tcmalloc</span></code></em> | JeMalloc <code class="docutils literal notranslate"><span class="pre">--enable_jemalloc</span></code>]</p>
<ul class="simple">
<li><p>PyTorch by default uses PTMalloc. TCMalloc/JeMalloc generally gives better performance.</p></li>
</ul>
</li>
<li><p>OpenMP library: [GNU OpenMP <code class="docutils literal notranslate"><span class="pre">--disable_iomp</span></code> | <em>Intel OpenMP</em>]</p>
<ul class="simple">
<li><p>PyTorch by default uses GNU OpenMP. Launcher by default uses Intel OpenMP. Intel OpenMP library generally gives better performance.</p></li>
</ul>
</li>
<li><p>Node id: [<code class="docutils literal notranslate"><span class="pre">--node_id</span></code>]</p>
<ul class="simple">
<li><p>Launcher by default uses all NUMA nodes. Limit memory access to local memories on the Nth Numa node to avoid Non-Uniform Memory Access (NUMA).</p></li>
</ul>
</li>
</ol>
<p>Refer to <a class="reference internal" href="launch_script.html"><span class="doc">Launch Script Usage Guide</span></a> for a full list of tunable configuration of launcher. And refer to <a class="reference internal" href="tuning_guide.html"><span class="doc">Performance Tuning Guide</span></a> for more details.</p>
<section id="launcher-core-pinning-to-boost-performance-of-torchserve-multi-worker-inference">
<h3>Launcher Core Pinning to Boost Performance of TorchServe Multi Worker Inference<a class="headerlink" href="#launcher-core-pinning-to-boost-performance-of-torchserve-multi-worker-inference" title="Link to this heading"></a></h3>
<p>When running <a class="reference external" href="https://pytorch.org/serve/management_api.html#scale-workers">multi-worker inference</a> with Torchserve (Required torchserve&gt;=0.6.1), launcher pin cores to workers to boost performance. Internally, launcher equally divides the number of cores by the number of workers such that each worker is pinned to assigned cores. Doing so avoids core overlap among workers which can signficantly boost performance for TorchServe multi-worker inference. For example, assume running 4 workers on a machine with Intel(R) Xeon(R) Platinum 8180 CPU, 2 sockets, 28 cores per socket, 2 threads per core. Launcher will bind worker 0 to cores 0-13, worker 1 to cores 14-27, worker 2 to cores 28-41, and worker 3 to cores 42-55.</p>
<p>CPU usage is shown below. 4 main worker threads were launched, each launching 14 threads affinitized to the assigned physical cores.
<img alt="26" src="https://user-images.githubusercontent.com/93151422/170373651-fd8a0363-febf-4528-bbae-e1ddef119358.gif" /></p>
<section id="scaling-workers">
<h4>Scaling workers<a class="headerlink" href="#scaling-workers" title="Link to this heading"></a></h4>
<p>Additionally when dynamically <a class="reference external" href="https://pytorch.org/serve/management_api.html#scale-workers">scaling the number of workers</a>, cores that were pinned to killed workers by the launcher could be left unutilized. To address this problem, launcher internally restarts the workers to re-distribute cores that were pinned to killed workers to the remaining, alive workers. This is taken care internally, so users do not have to worry about this.</p>
<p>Continuing with the above example with 4 workers, assume killing workers 2 and 3. If cores were not re-distributed after the scale down, cores 28-55 would be left unutilized. Instead, launcher re-distributes cores 28-55 to workers 0 and 1 such that now worker 0 binds to cores 0-27 and worker 1 binds to cores 28-55.<sup>2</sup></p>
<p>CPU usage is shown below. 4 main worker threads were initially launched. Then after scaling down the number of workers from 4 to 2, 2 main worker threads were launched, each launching 28 threads affinitized to the assigned physical cores.
<img alt="worker_scaling" src="https://user-images.githubusercontent.com/93151422/170374697-7497c2d5-4c17-421b-9993-1434d1f722f6.gif" /></p>
<p><sup>2. Serving is interrupted for few seconds while re-distributing cores to scaled workers.</sup></p>
<p>Again, all it needs to use TorchServe with launcher core pinning for multiple workers as well as scaling workers is to set its configuration in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code>.</p>
<p>Add the following lines in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code> to use launcher with its default configuration.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
</pre></div>
</div>
</section>
</section>
</section>
<section id="creating-and-exporting-int8-model-for-intel-extension-for-pytorch">
<h2>Creating and Exporting INT8 model for Intel® Extension for PyTorch*<a class="headerlink" href="#creating-and-exporting-int8-model-for-intel-extension-for-pytorch" title="Link to this heading"></a></h2>
<p>Intel® Extension for PyTorch* supports both eager and torchscript mode. In this section, we show how to deploy INT8 model for Intel® Extension for PyTorch*. Refer to <a class="reference internal" href="../features/int8_overview.html"><span class="doc">here</span></a> for more details on Intel® Extension for PyTorch* optimizations for quantization.</p>
<section id="creating-a-serialized-file">
<h3>1. Creating a serialized file<a class="headerlink" href="#creating-a-serialized-file" title="Link to this heading"></a></h3>
<p>First create <code class="docutils literal notranslate"><span class="pre">.pt</span></code> serialized file using Intel® Extension for PyTorch* INT8 inference. Here we show two examples with BERT and ResNet50.</p>
<section id="bert">
<h4>BERT<a class="headerlink" href="#bert" title="Link to this heading"></a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">transformers</span><span class="w"> </span><span class="kn">import</span> <span class="n">BertModel</span>

<span class="c1"># load the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># define dummy input tensor to use for the model&#39;s forward call to record operations in the model for tracing</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">vocab_size</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">seq_length</span> <span class="o">=</span> <span class="mi">384</span>
<span class="n">dummy_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="p">[</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">])</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch.quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">prepare</span><span class="p">,</span> <span class="n">convert</span>

<span class="c1"># ipex supports two quantization schemes: static and dynamic</span>
<span class="c1"># default dynamic qconfig</span>
<span class="n">qconfig</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">default_dynamic_qconfig</span>

<span class="c1"># prepare and calibrate</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">,</span> <span class="n">example_inputs</span><span class="o">=</span><span class="n">dummy_tensor</span><span class="p">)</span>

<span class="c1"># convert and deploy</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dummy_tensor</span><span class="p">,</span> <span class="n">check_trace</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">strict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;bert_int8_jit.pt&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="resnet50">
<h4>ResNet50<a class="headerlink" href="#resnet50" title="Link to this heading"></a></h4>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">ipex</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torchvision.models</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">models</span>

<span class="c1"># load the model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="o">.</span><span class="n">resnet50</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>

<span class="c1"># define dummy input tensor to use for the model&#39;s forward call to record operations in the model for tracing</span>
<span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span>
<span class="n">dummy_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">)</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch.quantization</span><span class="w"> </span><span class="kn">import</span> <span class="n">prepare</span><span class="p">,</span> <span class="n">convert</span>

<span class="c1"># ipex supports two quantization schemes: static and dynamic</span>
<span class="c1"># default static qconfig</span>
<span class="n">qconfig</span> <span class="o">=</span> <span class="n">ipex</span><span class="o">.</span><span class="n">quantization</span><span class="o">.</span><span class="n">default_static_qconfig</span>

<span class="c1"># prepare and calibrate</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">prepare</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">qconfig</span><span class="p">,</span> <span class="n">example_inputs</span><span class="o">=</span><span class="n">dummy_tensor</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

<span class="n">n_iter</span> <span class="o">=</span> <span class="mi">100</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_iter</span><span class="p">):</span>
    <span class="n">model</span><span class="p">(</span><span class="n">dummy_tensor</span><span class="p">)</span>

<span class="c1"># convert and deploy</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">convert</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">dummy_tensor</span><span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">freeze</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>

<span class="n">torch</span><span class="o">.</span><span class="n">jit</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s1">&#39;rn50_int8_jit.pt&#39;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="creating-a-model-archive">
<h3>2. Creating a Model Archive<a class="headerlink" href="#creating-a-model-archive" title="Link to this heading"></a></h3>
<p>Once the serialized file ( <code class="docutils literal notranslate"><span class="pre">.pt</span></code>) is created, it can be used with <code class="docutils literal notranslate"><span class="pre">torch-model-archiver</span></code> as ususal.</p>
<p>Use the following command to package <code class="docutils literal notranslate"><span class="pre">rn50_int8_jit.pt</span></code> into <code class="docutils literal notranslate"><span class="pre">rn50_ipex_int8.mar</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">-</span><span class="n">model</span><span class="o">-</span><span class="n">archiver</span> <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="n">name</span> <span class="n">rn50_ipex_int8</span> <span class="o">--</span><span class="n">version</span> <span class="mf">1.0</span> <span class="o">--</span><span class="n">serialized</span><span class="o">-</span><span class="n">file</span> <span class="n">rn50_int8_jit</span><span class="o">.</span><span class="n">pt</span> <span class="o">--</span><span class="n">handler</span> <span class="n">image_classifier</span>
</pre></div>
</div>
<p>Similarly, use the following command in the <a class="reference external" href="https://github.com/pytorch/serve/tree/master/examples/Huggingface_Transformers">Huggingface_Transformers directory</a> to package <code class="docutils literal notranslate"><span class="pre">bert_int8_jit.pt</span></code> into <code class="docutils literal notranslate"><span class="pre">bert_ipex_int8.mar</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">-</span><span class="n">model</span><span class="o">-</span><span class="n">archiver</span> <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="n">name</span> <span class="n">bert_ipex_int8</span> <span class="o">--</span><span class="n">version</span> <span class="mf">1.0</span> <span class="o">--</span><span class="n">serialized</span><span class="o">-</span><span class="n">file</span> <span class="n">bert_int8_jit</span><span class="o">.</span><span class="n">pt</span> <span class="o">--</span><span class="n">handler</span> <span class="o">./</span><span class="n">Transformer_handler_generalized</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">extra</span><span class="o">-</span><span class="n">files</span> <span class="s2">&quot;./setup_config.json,./Seq_classification_artifacts/index_to_name.json&quot;</span>
</pre></div>
</div>
</section>
<section id="start-torchserve-to-serve-the-model">
<h3>3. Start TorchServe to serve the model<a class="headerlink" href="#start-torchserve-to-serve-the-model" title="Link to this heading"></a></h3>
<p>Make sure to set <code class="docutils literal notranslate"><span class="pre">ipex_enable=true</span></code> in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code>. Use the following command to start TorchServe with Intel® Extension for PyTorch*.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torchserve</span> <span class="o">--</span><span class="n">start</span> <span class="o">--</span><span class="n">ncs</span> <span class="o">--</span><span class="n">model</span><span class="o">-</span><span class="n">store</span> <span class="n">model_store</span> <span class="o">--</span><span class="n">ts</span><span class="o">-</span><span class="n">config</span> <span class="n">config</span><span class="o">.</span><span class="n">properties</span>
</pre></div>
</div>
</section>
<section id="registering-and-deploying-model">
<h3>4. Registering and Deploying model<a class="headerlink" href="#registering-and-deploying-model" title="Link to this heading"></a></h3>
<p>Registering and deploying the model follows the same steps shown <a class="reference external" href="https://pytorch.org/serve/use_cases.html">here</a>.</p>
</section>
</section>
<section id="benchmarking-with-launcher">
<h2>Benchmarking with Launcher<a class="headerlink" href="#benchmarking-with-launcher" title="Link to this heading"></a></h2>
<p>Launcher can be used with TorchServe official <a class="reference external" href="https://github.com/pytorch/serve/tree/master/benchmarks">benchmark</a> to launch server and benchmark requests with optimal configuration on Intel hardware.</p>
<p>In this section we provide examples of benchmarking with launcher with its default configuration.</p>
<p>Add the following lines to <code class="docutils literal notranslate"><span class="pre">config.properties</span></code> in the benchmark directory to use launcher with its default setting.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipex_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
</pre></div>
</div>
<p>The rest of the steps for benchmarking follows the same steps shown <a class="reference external" href="https://github.com/pytorch/serve/tree/master/benchmarks">here</a>.</p>
<p><code class="docutils literal notranslate"><span class="pre">model_log.log</span></code> contains information and command that were used for this execution launch.</p>
<p>CPU usage on a machine with Intel(R) Xeon(R) Platinum 8180 CPU, 2 sockets, 28 cores per socket, 2 threads per core is shown as below:
<img alt="launcher_default_2sockets" src="https://user-images.githubusercontent.com/93151422/144373537-07787510-039d-44c4-8cfd-6afeeb64ac78.gif" /></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ cat logs/model_log.log
2021-12-01 21:22:40,096 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /home/&lt;user&gt;/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance
2021-12-01 21:22:40,096 - __main__ - INFO - OMP_NUM_THREADS=56
2021-12-01 21:22:40,096 - __main__ - INFO - Using Intel OpenMP
2021-12-01 21:22:40,096 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
2021-12-01 21:22:40,096 - __main__ - INFO - KMP_BLOCKTIME=1
2021-12-01 21:22:40,096 - __main__ - INFO - LD_PRELOAD=&lt;VIRTUAL_ENV&gt;/lib/libiomp5.so
2021-12-01 21:22:40,096 - __main__ - WARNING - Numa Aware: cores:[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55] in different NUMA node
</pre></div>
</div>
<p>CPU usage on a machine with Intel(R) Xeon(R) Platinum 8375C CPU, 1 socket, 2 cores per socket, 2 threads per socket is shown as below:
<img alt="launcher_default_1socket" src="https://user-images.githubusercontent.com/93151422/144372993-92b2ca96-f309-41e2-a5c8-bf2143815c93.gif" /></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ cat logs/model_log.log
2021-12-02 06:15:03,981 - __main__ - WARNING - Both TCMalloc and JeMalloc are not found in $CONDA_PREFIX/lib or $VIRTUAL_ENV/lib or /.local/lib/ or /usr/local/lib/ or /usr/local/lib64/ or /usr/lib or /usr/lib64 or /home/&lt;user&gt;/.local/lib/ so the LD_PRELOAD environment variable will not be set. This may drop the performance
2021-12-02 06:15:03,981 - __main__ - INFO - OMP_NUM_THREADS=2
2021-12-02 06:15:03,982 - __main__ - INFO - Using Intel OpenMP
2021-12-02 06:15:03,982 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
2021-12-02 06:15:03,982 - __main__ - INFO - KMP_BLOCKTIME=1
2021-12-02 06:15:03,982 - __main__ - INFO - LD_PRELOAD=&lt;VIRTUAL_ENV&gt;/lib/libiomp5.so
</pre></div>
</div>
<section id="benchmarking-with-launcher-core-pinning">
<h3>Benchmarking with Launcher Core Pinning<a class="headerlink" href="#benchmarking-with-launcher-core-pinning" title="Link to this heading"></a></h3>
<p>As described previously in <a class="reference external" href="#torchserve-with-launcher">TorchServe with Launcher</a>, launcher core pinning boosts performance of multi-worker inference. We’ll demonstrate launcher core pinning with TorchServe benchmark, but keep in mind that launcher core pinning is a generic feature applicable to any TorchServe multi-worker inference use casese.</p>
<p>For example, assume running 4 workers</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">benchmark</span><span class="o">-</span><span class="n">ab</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">workers</span> <span class="mi">4</span>
</pre></div>
</div>
<p>on a machine with Intel(R) Xeon(R) Platinum 8180 CPU, 2 sockets, 28 cores per socket, 2 threads per core. Launcher will bind worker 0 to cores 0-13, worker 1 to cores 14-27, worker 2 to cores 28-41, and worker 3 to cores 42-55.</p>
<p>All it needs to use TorchServe with launcher’s core pinning is to enable launcher in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code>.</p>
<p>Add the following lines to <code class="docutils literal notranslate"><span class="pre">config.properties</span></code> in the benchmark directory to use launcher’s core pinning:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
</pre></div>
</div>
<p>CPU usage is shown as below:
<img alt="launcher_core_pinning" src="https://user-images.githubusercontent.com/93151422/159063975-e7e8d4b0-e083-4733-bdb6-4d92bdc10556.gif" /></p>
<p>4 main worker threads were launched, then each launched a num_physical_cores/num_workers number (14) of threads affinitized to the assigned physical cores.</p>
<pre><code>
$ cat logs/model_log.log
2022-03-24 10:41:32,223 - __main__ - INFO - Use TCMalloc memory allocator
2022-03-24 10:41:32,223 - __main__ - INFO - OMP_NUM_THREADS=14
2022-03-24 10:41:32,223 - __main__ - INFO - Using Intel OpenMP
2022-03-24 10:41:32,223 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
2022-03-24 10:41:32,223 - __main__ - INFO - KMP_BLOCKTIME=1
2022-03-24 10:41:32,223 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so:<VIRTUAL_ENV>/lib/libtcmalloc.so
2022-03-24 10:41:32,223 - __main__ - INFO - <b>numactl -C 0-13 -m 0</b> <VIRTUAL_ENV>/bin/python -u <VIRTUAL_ENV>/lib/python/site-packages/ts/model_service_worker.py --sock-type unix --sock-name /tmp/.ts.sock.9000

2022-03-24 10:49:03,760 - __main__ - INFO - Use TCMalloc memory allocator
2022-03-24 10:49:03,761 - __main__ - INFO - OMP_NUM_THREADS=14
2022-03-24 10:49:03,762 - __main__ - INFO - Using Intel OpenMP
2022-03-24 10:49:03,762 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
2022-03-24 10:49:03,762 - __main__ - INFO - KMP_BLOCKTIME=1
2022-03-24 10:49:03,762 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so:<VIRTUAL_ENV>/lib/libtcmalloc.so
2022-03-24 10:49:03,763 - __main__ - INFO - <b>numactl -C 14-27 -m 0</b> <VIRTUAL_ENV>/bin/python -u <VIRTUAL_ENV>/lib/python/site-packages/ts/model_service_worker.py --sock-type unix --sock-name /tmp/.ts.sock.9001

2022-03-24 10:49:26,274 - __main__ - INFO - Use TCMalloc memory allocator
2022-03-24 10:49:26,274 - __main__ - INFO - OMP_NUM_THREADS=14
2022-03-24 10:49:26,274 - __main__ - INFO - Using Intel OpenMP
2022-03-24 10:49:26,274 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
2022-03-24 10:49:26,274 - __main__ - INFO - KMP_BLOCKTIME=1
2022-03-24 10:49:26,274 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so:<VIRTUAL_ENV>/lib/libtcmalloc.so
2022-03-24 10:49:26,274 - __main__ - INFO - <b>numactl -C 28-41 -m 1</b> <VIRTUAL_ENV>/bin/python -u <VIRTUAL_ENV>/lib/python/site-packages/ts/model_service_worker.py --sock-type unix --sock-name /tmp/.ts.sock.9002

2022-03-24 10:49:42,975 - __main__ - INFO - Use TCMalloc memory allocator
2022-03-24 10:49:42,975 - __main__ - INFO - OMP_NUM_THREADS=14
2022-03-24 10:49:42,975 - __main__ - INFO - Using Intel OpenMP
2022-03-24 10:49:42,975 - __main__ - INFO - KMP_AFFINITY=granularity=fine,compact,1,0
2022-03-24 10:49:42,975 - __main__ - INFO - KMP_BLOCKTIME=1
2022-03-24 10:49:42,975 - __main__ - INFO - LD_PRELOAD=<VIRTUAL_ENV>/lib/libiomp5.so:<VIRTUAL_ENV>/lib/libtcmalloc.so
2022-03-24 10:49:42,975 - __main__ - INFO - <b>numactl -C 42-55 -m 1</b> <VIRTUAL_ENV>/bin/python -u <VIRTUAL_ENV>/lib/python/site-packages/ts/model_service_worker.py --sock-type unix --sock-name /tmp/.ts.sock.9003
</code></pre></section>
</section>
<section id="performance-boost-with-intel-extension-for-pytorch-and-launcher">
<h2>Performance Boost with Intel® Extension for PyTorch* and Launcher<a class="headerlink" href="#performance-boost-with-intel-extension-for-pytorch-and-launcher" title="Link to this heading"></a></h2>
<p><img alt="pdt_perf" src="https://user-images.githubusercontent.com/93151422/159067306-dfd604e3-8c66-4365-91ae-c99f68d972d5.png" /></p>
<p>Above shows performance improvement of Torchserve with Intel® Extension for PyTorch* and launcher on ResNet50 and BERT-base-uncased. Torchserve official <a class="reference external" href="https://github.com/pytorch/serve/tree/master/benchmarks#benchmarking-with-apache-bench">apache-bench benchmark</a> on Amazon EC2 m6i.24xlarge was used to collect the results<sup>2</sup>. Add the following lines in <code class="docutils literal notranslate"><span class="pre">config.properties</span></code> to reproduce the results. Notice that launcher is configured such that a single instance uses all physical cores on a single socket to avoid cross socket communication and core overlap.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">ipex_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_enable</span><span class="o">=</span><span class="n">true</span>
<span class="n">cpu_launcher_args</span><span class="o">=--</span><span class="n">node_id</span> <span class="mi">0</span> <span class="o">--</span><span class="n">enable_jemalloc</span>
</pre></div>
</div>
<p>Use the following command to reproduce the results.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">benchmark</span><span class="o">-</span><span class="n">ab</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">url</span> <span class="p">{</span><span class="n">modelUrl</span><span class="p">}</span> <span class="o">--</span><span class="nb">input</span> <span class="p">{</span><span class="n">inputPath</span><span class="p">}</span> <span class="o">--</span><span class="n">concurrency</span> <span class="mi">1</span>
</pre></div>
</div>
<p>For example, run the following command to reproduce latency performance of ResNet50 with data type of Intel® Extension for PyTorch* int8 and batch size of 1. Refer to <a class="reference external" href="#creating-and-exporting-int8-model-for-intel-extension-for-pytorch">Creating and Exporting INT8 model for Intel® Extension for PyTorch*</a> for steps to creating <code class="docutils literal notranslate"><span class="pre">rn50_ipex_int8.mar</span></code> file for ResNet50 with Intel® Extension for PyTorch* int8 data type.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">benchmark</span><span class="o">-</span><span class="n">ab</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">url</span> <span class="s1">&#39;file:///model_store/rn50_ipex_int8.mar&#39;</span> <span class="o">--</span><span class="n">concurrency</span> <span class="mi">1</span>
</pre></div>
</div>
<p>For example, run the following command to reproduce latency performance of BERT with data type of Intel® Extension for PyTorch* int8 and batch size of 1. Refer to <a class="reference external" href="#creating-and-exporting-int8-model-for-intel-extension-for-pytorch">Creating and Exporting INT8 model for Intel® Extension for PyTorch*</a> for steps to creating <code class="docutils literal notranslate"><span class="pre">bert_ipex_int8.mar</span></code> file for BERT with Intel® Extension for PyTorch* int8 data type.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">python</span> <span class="n">benchmark</span><span class="o">-</span><span class="n">ab</span><span class="o">.</span><span class="n">py</span> <span class="o">--</span><span class="n">url</span> <span class="s1">&#39;file:///model_store/bert_ipex_int8.mar&#39;</span> <span class="o">--</span><span class="nb">input</span> <span class="s1">&#39;../examples/Huggingface_Transformers/Seq_classification_artifacts/sample_text_captum_input.txt&#39;</span> <span class="o">--</span><span class="n">concurrency</span> <span class="mi">1</span>
</pre></div>
</div>
<p><sup>3. Amazon EC2 m6i.24xlarge was used for benchmarking purpose only. For multi-core instances, Intel® Extension for PyTorch* optimizations automatically scale and leverage full instance resources.</sup></p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="launch_script.html" class="btn btn-neutral float-left" title="Launch Script Usage Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../contribution.html" class="btn btn-neutral float-right" title="Contribution" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7cbf915f2fb0> 
<p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a><a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a><a href='/#' data-wap_ref='dns' id='wap_dns'><small>| Your Privacy Choices</small><span style='height:10px;width:28px;display:inline-block;position:relative;'><svg style='position:absolute;width:28px;bottom:-2px;' version='1.1' id='Layer_1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' x='0px' y='0px' viewBox='0 0 30 14' xml:space='preserve'><title>California Consumer Privacy Act (CCPA) Opt-Out Icon</title><style type='text/css'> .st0 { fill-rule: evenodd; clip-rule: evenodd; fill: #FFFFFF; } .st1 { fill-rule: evenodd; clip-rule: evenodd; fill: #0066FF; } .st2 { fill: #FFFFFF; } .st3 { fill: #0066FF; } </style><g><g id='final---dec.11-2020_1_'><g id='_x30_208-our-toggle_2_' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2_2_' transform='translate(1275.000000, 200.000000)'><path class='st0' d='M7.4,12.8h6.8l3.1-11.6H7.4C4.2,1.2,1.6,3.8,1.6,7S4.2,12.8,7.4,12.8z'></path></g></g></g><g id='final---dec.11-2020'><g id='_x30_208-our-toggle' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2' transform='translate(1275.000000, 200.000000)'><path class='st1' d='M22.6,0H7.4c-3.9,0-7,3.1-7,7s3.1,7,7,7h15.2c3.9,0,7-3.1,7-7S26.4,0,22.6,0z M1.6,7c0-3.2,2.6-5.8,5.8-5.8 h9.9l-3.1,11.6H7.4C4.2,12.8,1.6,10.2,1.6,7z'></path><path id='x' class='st2' d='M24.6,4c0.2,0.2,0.2,0.6,0,0.8l0,0L22.5,7l2.2,2.2c0.2,0.2,0.2,0.6,0,0.8c-0.2,0.2-0.6,0.2-0.8,0 l0,0l-2.2-2.2L19.5,10c-0.2,0.2-0.6,0.2-0.8,0c-0.2-0.2-0.2-0.6,0-0.8l0,0L20.8,7l-2.2-2.2c-0.2-0.2-0.2-0.6,0-0.8 c0.2-0.2,0.6-0.2,0.8,0l0,0l2.2,2.2L23.8,4C24,3.8,24.4,3.8,24.6,4z'></path><path id='y' class='st3' d='M12.7,4.1c0.2,0.2,0.3,0.6,0.1,0.8l0,0L8.6,9.8C8.5,9.9,8.4,10,8.3,10c-0.2,0.1-0.5,0.1-0.7-0.1l0,0 L5.4,7.7c-0.2-0.2-0.2-0.6,0-0.8c0.2-0.2,0.6-0.2,0.8,0l0,0L8,8.6l3.8-4.5C12,3.9,12.4,3.9,12.7,4.1z'></path></g></g></g></g></svg></span></a><a href=https://www.intel.com/content/www/us/en/privacy/privacy-residents-certain-states.html data-wap_ref='nac' id='wap_nac'><small>| Notice at Collection</small></a></div><p></p><div>&copy; Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others. No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (OBSD), <a href='http://opensource.org/licenses/0BSD'>http://opensource.org/licenses/0BSD</a>.</div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>