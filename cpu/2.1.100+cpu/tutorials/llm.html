<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Large Language Models (LLM) Optimizations Overview &mdash; Intel&amp;#174 Extension for PyTorch* 2.1.100+cpu documentation</title>
      <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-pytorch"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Transformers Optimization Frontend API" href="llm/llm_optimize_transformers.html" />
    <link rel="prev" title="Fast BERT (Experimental)" href="features/fast_bert.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../index.html" class="icon icon-home">
            Intel&#174 Extension for PyTorch*
          </a>
              <div class="version">
                <a href="../../../">2.1.100+cpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ABOUT</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="features.html">Features</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Large Language Models (LLM)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="llm/llm_optimize_transformers.html">Transformers Optimization Frontend API</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimized-models">Optimized Models</a></li>
<li class="toctree-l2"><a class="reference internal" href="#demos">Demos</a></li>
<li class="toctree-l2"><a class="reference internal" href="#optimization-methodologies">Optimization Methodologies</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#linear-operator-optimization">Linear Operator Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#low-precision-data-types">Low Precision Data Types</a></li>
<li class="toctree-l3"><a class="reference internal" href="#indirect-access-kv-cache">Indirect Access KV Cache</a></li>
<li class="toctree-l3"><a class="reference internal" href="#graph-optimization">Graph Optimization</a></li>
<li class="toctree-l3"><a class="reference internal" href="#distributed-inference">Distributed Inference</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="known_issues.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference internal" href="blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="getting_started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="cheat_sheet.html">Cheat Sheet</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="api_doc.html">API Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">PERFORMANCE TUNING</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning/tuning_guide.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning/launch_script.html">Launch Script Usage Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="performance_tuning/torchserve.html">TorchServe with Intel® Extension for PyTorch*</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CONTRIBUTING GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="contribution.html">Contribution</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Intel&#174 Extension for PyTorch*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Large Language Models (LLM) Optimizations Overview</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../_sources/tutorials/llm.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="large-language-models-llm-optimizations-overview">
<h1>Large Language Models (LLM) Optimizations Overview<a class="headerlink" href="#large-language-models-llm-optimizations-overview" title="Permalink to this heading"></a></h1>
<p>In the current technological landscape, Generative AI (GenAI) workloads and models have gained widespread attention and popularity. Large Language Models (LLMs) have emerged as the dominant models driving these GenAI applications. Most of LLMs are GPT-like architectures that consist of multiple Decoder layers.
The MultiHeadAttention and FeedForward layer are two key components of every Decoder layer. The generation task is memory bound because iterative decode and kv_cache require special management to reduce memory overheads. Intel® Extension for PyTorch* provides a lot of specific optimizations for these LLMs.
On the operator level, the extension provides highly efficient GEMM kernel to speed up Linear layer and customized operators to reduce the memory footprint. To better trade-off the performance and accuracy, different low-precision solutions e.g., smoothQuant and weight-only-quantization are also enabled. Besides, tensor parallel can also adopt to get lower latency for LLMs.</p>
<p>These LLM-specific optimizations can be automatically applied with a single frontend API function in Python interface, <cite>ipex.optimize_transformers()</cite>. Check <a class="reference external" href="./llm/llm_optimize_transformers.html">optimize_transformers</a> for more details.</p>
<div class="toctree-wrapper compound">
</div>
<section id="optimized-models">
<h2>Optimized Models<a class="headerlink" href="#optimized-models" title="Permalink to this heading"></a></h2>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Model Family</p></th>
<th class="head"><p>LLAMA</p></th>
<th class="head"><p>GPT-J</p></th>
<th class="head"><p>GPT-NEOX</p></th>
<th class="head"><p>FALCON*</p></th>
<th class="head"><p>OPT</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Verified &lt; MODEL ID &gt; (Huggingface hub)</p></td>
<td><p>“meta-llama/Llama-2-7b-hf”, “meta-llama/Llama-2-13b-hf”, “meta-llama/Llama-2-70b-hf”</p></td>
<td><p>“EleutherAI/gpt-j-6b”</p></td>
<td><p>“EleutherAI/gpt-neox-20b”</p></td>
<td><p>“tiiuae/falcon-40b”</p></td>
<td><p>“facebook/opt-30b”, “facebook/opt-1.3b”</p></td>
</tr>
<tr class="row-odd"><td><p>FP32/BF16</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p>Weight only quantzation INT8</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-odd"><td><p>Weight only quantization INT4</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
</tr>
<tr class="row-even"><td><p>Static quantization INT8</p></td>
<td><p>✅</p></td>
<td><p>✅</p></td>
<td><p>❎**</p></td>
<td><p>❎**</p></td>
<td><p>❎**</p></td>
</tr>
</tbody>
</table>
<p>* For Falcon models from remote hub, we need to modify the config.json to use the modeling_falcon.py in transformers. Therefore, in the following scripts, we need to pass an extra configuration file like “–config-file=model_config/tiiuae_falcon-40b_config.json”. This is optional for FP32/BF16 but needed for quantizations.</p>
<p>** For GPT-NEOX/FALCON/OPT models, the accuracy recipes of static quantization INT8 are not ready, thus, they will be skipped in our coverage.</p>
<p><em>Note</em>: The above verified models (including other models in the same model family, like “codellama/CodeLlama-7b-hf” from LLAMA family) are well optimized with all approaches like indirect access KV cache, fused ROPE, and prepacked TPP Linear (fp32/bf16). For other LLM families, we are working in progress to cover those optimizations, which will expand the model list above.</p>
<p>Check <a class="reference external" href="https://github.com/intel/intel-extension-for-pytorch/tree/v2.1.100%2Bcpu/examples/cpu/inference/python/llm">LLM best known practice</a> for instructions to install/setup environment and example scripts..</p>
</section>
<section id="demos">
<h2>Demos<a class="headerlink" href="#demos" title="Permalink to this heading"></a></h2>
<p>Intel® Extension for PyTorch* LLM optimizations can be integrated into a typical LLM Q&amp;A web service.</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><a class="reference internal image-reference" href="../_images/GenAI-bf16.gif"><img alt="UI with BF16" src="../_images/GenAI-bf16.gif" style="width: 500px;" /></a>
</td>
<td><a class="reference internal image-reference" href="../_images/GenAI-int8.gif"><img alt="UI with INT8" src="../_images/GenAI-int8.gif" style="width: 500px;" /></a>
</td>
</tr>
</tbody>
</table>
<p>Following figures show demos with Llama 2 model and GPT-J model with single inference and distributed inference with deepspeed with lower precision data types.</p>
<table class="docutils align-default">
<tbody>
<tr class="row-odd"><td><figure class="align-default" id="id1">
<a class="reference internal image-reference" href="../_images/bf16_llama.gif"><img alt="Llama 2 with BF16" src="../_images/bf16_llama.gif" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-text">a</span><a class="headerlink" href="#id1" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</td>
<td><figure class="align-default" id="id2">
<a class="reference internal image-reference" href="../_images/smoothquant_int8_llama.gif"><img alt="Llama 2 with INT8 Quantization with SmoothQuant" src="../_images/smoothquant_int8_llama.gif" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-text">b</span><a class="headerlink" href="#id2" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</td>
<td><figure class="align-default" id="id3">
<a class="reference internal image-reference" href="../_images/woq_int8_llama.gif"><img alt="Weight Only Quantization with INT8 for Llama 2" src="../_images/woq_int8_llama.gif" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-text">c</span><a class="headerlink" href="#id3" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</td>
</tr>
<tr class="row-even"><td><figure class="align-default" id="id4">
<a class="reference internal image-reference" href="../_images/woq_int4_gptj.gif"><img alt="Weight Only Quantization with INT4 for GPT-J" src="../_images/woq_int4_gptj.gif" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-text">d</span><a class="headerlink" href="#id4" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</td>
<td><figure class="align-default" id="id5">
<a class="reference internal image-reference" href="../_images/autotp_bf16_llama.gif"><img alt="Distributed Inference with DeepSpeed with BF16 on Llama 2 with AutoTP feature" src="../_images/autotp_bf16_llama.gif" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-text">e</span><a class="headerlink" href="#id5" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</td>
<td><figure class="align-default" id="id6">
<a class="reference internal image-reference" href="../_images/autotp_woq_int8_llama.gif"><img alt="Distributed Inference with DeepSpeed with Weight Only Quantization INT8 on Llama 2 with AutoTP feature" src="../_images/autotp_woq_int8_llama.gif" style="width: 300px;" /></a>
<figcaption>
<p><span class="caption-text">f</span><a class="headerlink" href="#id6" title="Permalink to this image"></a></p>
</figcaption>
</figure>
</td>
</tr>
</tbody>
</table>
<p>Figure Legends:</p>
<ol class="loweralpha simple">
<li><p>Llama 2 model with BF16</p></li>
<li><p>Llama 2 model with INT8 Quantization with SmoothQuant technique</p></li>
<li><p>Llama 2 model with INT8 Weight Only Quantization</p></li>
<li><p>GPT-J model with INT4 Weight Only Quantization</p></li>
<li><p>Llama 2 model Distributed Inference with DeepSpeed with AutoTP feature on BF16</p></li>
<li><p>Llama 2 model Distributed Inference with DeepSpeed with AutoTP feature with Weight Only Quantization INT8</p></li>
</ol>
</section>
<section id="optimization-methodologies">
<h2>Optimization Methodologies<a class="headerlink" href="#optimization-methodologies" title="Permalink to this heading"></a></h2>
<p>The section below provides a brief introduction to LLM optimization methodologies:</p>
<section id="linear-operator-optimization">
<h3>Linear Operator Optimization<a class="headerlink" href="#linear-operator-optimization" title="Permalink to this heading"></a></h3>
<p>Linear operator is the most obvious hotspot in LLMs inference. There are three backend to speedup linear GEMM kernels in Intel® Extension for PyTorch*. They are oneDNN, Tensor Processing Primitives (TPP), which are used by <a class="reference external" href="./features/fast_bert.html">Fast BERT feature</a>, and customized linear kernels for weight only quantization. All of them use specific block format to utilize hardware resources in a highly efficient way.</p>
</section>
<section id="low-precision-data-types">
<h3>Low Precision Data Types<a class="headerlink" href="#low-precision-data-types" title="Permalink to this heading"></a></h3>
<p>While Generative AI (GenAI) workloads and models are getting more and more popular, LLMs used in these workloads are getting more and more parameters. The increasing size of LLMs enhances workload accuracies; however, it also leads to significantly heavier computations and places higher requirements to the underlying hardware. Given that, quantization becomes a more important methodology for inference workloads.</p>
<p>Quantization with shorter data types benefits from its nature to improve memory IO throughputs and amount of computations on CPU. Moreover, shorter data types make it possible to keep more data in CPU cache, thus reducing memory access occurrences. Comparing to cache access, memory access is much more time costing. Specifically from computation perspective, AVX-512 Vector Neural Network Instructions (VNNI) instruction set shipped with the 2nd Generation Intel® Xeon® Scalable Processors and newer, as well as Intel® Advanced Matrix Extensions (Intel® AMX) instruction set shipped with the 4th Generation Intel® Xeon® Scalable Processors, provide instruction level accelerations to INT8 computations.</p>
<p>Except for the mixed-precision and INT8 native quantization solution, e.g., post-training static quantization and dynamic quantization in Pytorch, <a class="reference external" href="https://arxiv.org/abs/2211.10438">SmoothQuant</a> and weight only quantization (both INT8 weight and INT4 weight are supported) are also enabled in Intel® Extension for PyTorch* to get beeter accuracy and performance compared with native solution.</p>
<p>Intel® Extension for PyTorch* speeds up INT8 computations by leveraging oneDNN and oneDNN graph as the backend. Intel® Extension for PyTorch* static quantization provides a default recipe to automatically decide which operators to quantize. Its backend oneDNN graph brings matrix-multiplication-based fusions for common seen operator patterns and other common fusions like quantization + data type casting. These fusions help achieve best computation cache locality and efficiency, and thus reduce INT8 quantization overhead significantly.</p>
<p>Intel® Extension for PyTorch* also delivers INT4 optimizations via 4-bit weight-only quantization (WOQ). As the name indicates, WOQ quantizes only weights to 4-bit integers to further improve the computation efficiency via saved memory bandwidth utilization. This technique reduces text generation latency especially from the second token. AMX INT8 instructions and fusions are also applied for these performant computations.</p>
</section>
<section id="indirect-access-kv-cache">
<h3>Indirect Access KV Cache<a class="headerlink" href="#indirect-access-kv-cache" title="Permalink to this heading"></a></h3>
<p>kv_cache is used to reduce computation for decoder layer but it also brings memory overheads. For example, when we use beam search, the kv_cache should be reordered according to latest beam idx and the current key/value should also be concat with kv_cache in the attention layer to get entire context to do scale dot product. When the sequence is very long, memory overheads caused by the reorder_cache and concat will be performance bottleneck. Indirect Access KV_cache (IAKV) is provided to reduce these overheads. Firstly, IAKV pre-allocates buffers (key and value use different buffer) to store all key/value hidden states and beam index information, the data format is shown in the following left figure (beam_width=4 in this case) and token state of key (value) in every timestamp will be store in this pre-allocated buffer. Secondly, we can use beam index history which is shown in the following right figure to decide which beam should be used by a timestamp and this information will generate a offset to access the kv_cache buffer which means that the reorder_cache and concat overheads will be eliminated by this way.</p>
<a class="reference internal image-reference" href="../_images/llm_iakv_1.png"><img alt="The key/value cache data format" src="../_images/llm_iakv_1.png" style="width: 400px;" /></a>
<a class="reference internal image-reference" href="../_images/llm_iakv_2.png"><img alt="The beam idx trace for every step" src="../_images/llm_iakv_2.png" style="width: 400px;" /></a>
</section>
<section id="graph-optimization">
<h3>Graph Optimization<a class="headerlink" href="#graph-optimization" title="Permalink to this heading"></a></h3>
<p>Operators fusion is generally used to enable sub-graph fusion to reduce the memory footprint. Except for linear post ops fusion, e.g, linear + activation function, a lot of customized operators are also provided in Intel® Extension for PyTorch* for further performance improvement. For example, Rotary Position Embedding (ROPE) and Root Mean Square Layer Normalization (RMSNorm).</p>
</section>
<section id="distributed-inference">
<h3>Distributed Inference<a class="headerlink" href="#distributed-inference" title="Permalink to this heading"></a></h3>
<p>All above optimizations already help you to get very good performance with single instance. To furthly reduce the inference latency and improve throughput, tensor parallel is also enabled in our soluction. You can firstly use DeepSpeed to auto shard the model and then apply above optimizations with the frontend API function provided by Intel® Extension for PyTorch.</p>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="features/fast_bert.html" class="btn btn-neutral float-left" title="Fast BERT (Experimental)" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="llm/llm_optimize_transformers.html" class="btn btn-neutral float-right" title="Transformers Optimization Frontend API" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Intel(R).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f5a48c2fa30> 
  <p>*Other names and brands may be claimed as the property of others. <a href="http://www.intel.com/content/www/us/en/legal/trademarks.html">Trademarks</a></p>
  <p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a> <a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a></div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>
