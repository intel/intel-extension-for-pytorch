{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acf43ad0-b446-4d1a-8c50-3d1f78c4fc7a",
   "metadata": {},
   "source": [
    "#  LLaMA2* inference on Intel® Xeon® 4th generation Scalable Processor with Intel® Extension for PyTorch*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba0472f-827a-4947-b1d0-6599f8bd3bc9",
   "metadata": {},
   "source": [
    "\n",
    "### <a href=\"https://ai.meta.com/llama/\">LLaMA2</a> should need no introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03451f48-c7b0-45e7-9aa5-4f3994266763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import time\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer, AutoConfig\n",
    "import torch\n",
    "# For this demo, we are using a custom version of Intel® Extension for PyTorch*\n",
    "# These features would be in Intel® Extension for PyTorch* version 2.1\n",
    "# For now, https://github.com/intel/intel-extension-for-pytorch/tree/llm_feature_branch/examples/cpu/inference/python/llm/README.md has instructions for installing it\n",
    "import intel_extension_for_pytorch as ipex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3842d5ca-8e07-48ad-a361-25c1acc8ed4a",
   "metadata": {},
   "source": [
    "### Select Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ff5000-ac01-45a9-b6ef-4f04c9211095",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_beams = 1\n",
    "# Reference: https://huggingface.co/docs/transformers/v4.33.2/en/main_classes/text_generation#transformers.GenerationConfig\n",
    "config = AutoConfig.from_pretrained(\"meta-llama/Llama-2-7b-hf\", torchscript=True)\n",
    "config.text_max_length = 64\n",
    "generate_kwargs = dict(do_sample=False, temperature=0.9, num_beams=num_beams,\n",
    "                       max_new_tokens=32, min_new_tokens=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69e24eb-e576-4a30-bf12-c177c533f0cc",
   "metadata": {},
   "source": [
    "### Select Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419a58a3-1c0a-4dff-9603-a07adfbe35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "model =  LlamaForCausalLM.from_pretrained(\n",
    "        \"meta-llama/Llama-2-7b-hf\", config=config, low_cpu_mem_usage=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7cd16c-8d94-49bd-8c9d-a7d31baa8e83",
   "metadata": {},
   "source": [
    "### Select Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67958eaa-5f3d-436c-8bf4-c09dc629b0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d23169-5d57-4aca-a4bf-8aaa4c870adc",
   "metadata": {},
   "source": [
    "#### Some specific optimizations are unable to be applied directly on HF generation via the Torchscript , such as indirect kv cache, and first token optimization. They need modifications on the generation loop (like beam search function), while these modifications are related to <i>Intel® Extension for PyTorch*</i> optimization only, which makes them unable to do any upstream to HF.\n",
    "\n",
    "Please note that this is more of a \"preview\" feature for you because the IPEX release alongside PyTorch* 2.1 would have formal support for them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdab22a-f7a1-4d62-bdfc-6e0fcaa8cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ipex._optimize_transformers(\n",
    "    model.eval(), dtype=torch.int8, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043d14fe-5c06-495c-aece-769d3c1b2d04",
   "metadata": {},
   "source": [
    "### Load prequantized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba7711-9489-4d01-8275-28d769381099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reference: https://github.com/intel/intel-extension-for-pytorch/tree/llm_feature_branch/examples/cpu/inference/python/llm#single-instance-performance\n",
    "\n",
    "torch._C._jit_set_texpr_fuser_enabled(False)\n",
    "self_jit = torch.jit.load(\"saved_results/int8_quantized_llama2_7b.pt\")\n",
    "self_jit = torch.jit.freeze(self_jit.eval())\n",
    "setattr(model, \"trace_graph\", self_jit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3abd22-5aba-4217-86f6-4174f05800b9",
   "metadata": {},
   "source": [
    "### Select prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c086c7-a6a8-4a19-8d49-300e08fae7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Once upon a time, there existed a little girl who liked to have adventures. She wanted to go to places and meet new people, and have fun\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d195c7b-cad0-4a94-8c2c-611e5a6619d6",
   "metadata": {},
   "source": [
    "### Benchmark\n",
    "\n",
    "<i>Intel® Extension for PyTorch*</i> uses oneDNN Graph® fusions with int8 static quantization to speed up inference.\n",
    "We also leverage BFloat16 based Automatic Mixed Precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fa4936-c12b-4621-978d-b90562830aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 0.0\n",
    "num_iter = 5\n",
    "num_warmup = 3\n",
    "prompt = [prompt] * 1\n",
    "total_list = []\n",
    "with torch.inference_mode(), torch.no_grad(), torch.autocast(\n",
    "    device_type=\"cpu\",\n",
    "    enabled=True,\n",
    "    dtype=torch.bfloat16,\n",
    "):\n",
    "    for i in range(num_iter):\n",
    "        tic = time.time()\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(torch.device(\"cpu\"))\n",
    "        output = model.generate(input_ids, **generate_kwargs)\n",
    "        gen_ids = output\n",
    "        gen_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        toc = time.time()\n",
    "        input_tokens_lengths = [x.shape[0] for x in input_ids]\n",
    "        output_tokens_lengths = [x.shape[0] for x in gen_ids]\n",
    "        total_new_tokens = [\n",
    "            o - i for i, o in zip(input_tokens_lengths, output_tokens_lengths)\n",
    "        ]\n",
    "        print(gen_text, total_new_tokens, flush=True)\n",
    "        print(\"Iteration: %d, Time: %.6f sec\" % (i, toc - tic), flush=True)\n",
    "        if i >= num_warmup:\n",
    "            total_time += toc - tic\n",
    "print(\"\\n\", \"-\" * 10, \"Summary:\", \"-\" * 10)\n",
    "latency = total_time / (num_iter - num_warmup)\n",
    "print(\"Inference latency: %.4f sec.\" % latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222d93cc-8f5b-4eb0-82f7-96415680bdc9",
   "metadata": {},
   "source": [
    "### Let's profile it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12efea7c-2c84-40ea-b9a5-2ee49dc67e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.profiler.profile(\n",
    "    activities=[torch.profiler.ProfilerActivity.CPU],\n",
    "    schedule=torch.profiler.schedule(\n",
    "        wait=1,\n",
    "        warmup=3,\n",
    "        active=1),\n",
    ") as prof:\n",
    "    for i in range(5):\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(torch.device(\"cpu\"))\n",
    "        output = model.generate(input_ids, **generate_kwargs)\n",
    "        gen_ids = output\n",
    "        gen_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        prof.step()\n",
    "print(prof.key_averages().table(sort_by=\"self_cpu_time_total\", row_limit=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2241798a-913f-448f-9556-785ae5928eab",
   "metadata": {},
   "source": [
    "### Benchmark token latency\n",
    "\n",
    "#### First restart kernel, then run code cells 1 through 7 (until the prompt selection step), followed by the following one -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f34cf8f-c3e4-4330-bfc5-6ef44bd9f9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.token_latency = True\n",
    "\n",
    "total_time = 0.0\n",
    "num_iter = 5\n",
    "num_warmup = 3\n",
    "prompt = [prompt] * 1\n",
    "total_list = []\n",
    "\n",
    "with torch.inference_mode(), torch.no_grad(), torch.autocast(\n",
    "    device_type=\"cpu\",\n",
    "    enabled=True,\n",
    "    dtype=torch.bfloat16,\n",
    "):\n",
    "    for i in range(num_iter):\n",
    "        tic = time.time()\n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(torch.device(\"cpu\"))\n",
    "        output = model.generate(input_ids, **generate_kwargs)\n",
    "        gen_ids = output[0]\n",
    "        gen_text = tokenizer.batch_decode(gen_ids, skip_special_tokens=True)\n",
    "        toc = time.time()\n",
    "        input_tokens_lengths = [x.shape[0] for x in input_ids]\n",
    "        output_tokens_lengths = [x.shape[0] for x in gen_ids]\n",
    "        total_new_tokens = [\n",
    "            o - i for i, o in zip(input_tokens_lengths, output_tokens_lengths)\n",
    "        ]\n",
    "        print(gen_text, total_new_tokens, flush=True)\n",
    "        print(\"Iteration: %d, Time: %.6f sec\" % (i, toc - tic), flush=True)\n",
    "        if i >= num_warmup:\n",
    "            total_time += toc - tic\n",
    "            total_list.append(output[1])\n",
    "\n",
    "print(\"\\n\", \"-\" * 10, \"Summary:\", \"-\" * 10)\n",
    "latency = total_time / (num_iter - num_warmup)\n",
    "print(\"Inference latency: %.4f sec.\" % latency)\n",
    "\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "first_latency = np.mean([x[0] for x in total_list])\n",
    "average_2n = list(chain(*[x[1:] for x in total_list]))\n",
    "average_2n.sort()\n",
    "average_2n_latency = np.mean(average_2n)\n",
    "p90_latency = average_2n[int(len(average_2n) * 0.9)]\n",
    "p99_latency = average_2n[int(len(average_2n) * 0.99)]\n",
    "print(\"First token average latency: %.4f sec.\" % first_latency)\n",
    "print(\"Average 2... latency: %.4f sec.\" % average_2n_latency)\n",
    "print(\"P90 2... latency: %.4f sec.\" % p90_latency)\n",
    "print(\"P99 2... latency: %.4f sec.\" % p99_latency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cde434-ddc6-4d5a-ac08-79d366a66b44",
   "metadata": {},
   "source": [
    "* Other names & brands may be claimed as the property of others"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
