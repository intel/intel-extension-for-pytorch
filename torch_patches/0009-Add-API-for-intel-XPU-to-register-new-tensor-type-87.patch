From c282e4296e1f4a49baf02592686380f3c00b6335 Mon Sep 17 00:00:00 2001
From: majing <jing1.ma@intel.com>
Date: Fri, 18 Nov 2022 15:40:13 +0800
Subject: [PATCH 9/9] Add API for intel XPU to register new tensor type (#87)

Co-authored-by: chengjunlu <chengjun.lu@intel.com>
---
 torch/csrc/tensor/python_tensor.cpp | 152 +++++++++++++++-------------
 torch/csrc/tensor/python_tensor.h   |   8 ++
 torch/csrc/utils/tensor_types.cpp   |   2 +-
 torch/csrc/utils/tensor_types.h     |   1 +
 4 files changed, 90 insertions(+), 73 deletions(-)

diff --git a/torch/csrc/tensor/python_tensor.cpp b/torch/csrc/tensor/python_tensor.cpp
index 648fd04186..4d6778a7b0 100644
--- a/torch/csrc/tensor/python_tensor.cpp
+++ b/torch/csrc/tensor/python_tensor.cpp
@@ -60,8 +60,7 @@ static_assert(
 
 static Backend default_backend = Backend::CPU;
 
-static void py_bind_tensor_types(
-    const std::vector<PyTensorType*>& tensor_types);
+static void py_bind_tensor_type(const PyTensorType& tensor_types);
 
 static TypeError unavailable_type(const PyTensorType& type) {
   return TypeError(
@@ -203,33 +202,47 @@ static void py_initialize_tensor_type(
   }
 }
 
-static const char* get_module(Backend backend) {
-  switch (backend) {
-    case Backend::CPU:
-      return "torch";
-    case Backend::CUDA:
-      return "torch.cuda";
-    case Backend::SparseCPU:
-      return "torch.sparse";
-    case Backend::SparseCUDA:
-      return "torch.cuda.sparse";
-    default:
-      AT_ERROR("invalid backend: ", toString(backend));
-  }
-}
-
 static std::string get_name(Backend backend, ScalarType scalarType) {
   std::ostringstream ss;
-  ss << get_module(backend) << "." << toString(scalarType) << "Tensor";
+  ss << torch::utils::backend_to_string(backend) << "." << toString(scalarType) << "Tensor";
   return ss.str();
 }
 
-static THPObjectPtr get_storage_obj(Backend backend, ScalarType dtype) {
-  auto module_name = get_module(backend);
+static THPObjectPtr get_py_module(c10::Backend backend) {
+  auto torch_module = THPObjectPtr(PyImport_ImportModule("torch"));
+  if (!torch_module) throw python_error();
+
+  auto module_name = torch::utils::backend_to_string(backend);
   auto module_obj = THPObjectPtr(PyImport_ImportModule(module_name));
-  if (!module_obj)
-    throw python_error();
+  if (!module_obj) {
+    std::string name(module_name);
+    auto module_idx = name.find('.');
+    if (module_idx != std::string::npos) {
+      auto module_name = name.substr(module_idx + 1);
+      module_obj = THPObjectPtr(PyObject_GetAttrString(torch_module.get(), module_name.c_str()));
+      if (!module_obj) throw python_error();
+    }
+    else {
+      throw python_error();
+    }
+  }
+  return module_obj;
+}
+
+static std::vector<std::unique_ptr<PyTensorType>> tensor_types;
 
+static PyTensorType* get_tensor_type(c10::Backend backend, c10::ScalarType scalar_type) {
+  auto it = std::find_if(tensor_types.begin(), tensor_types.end(),
+                         [backend, scalar_type](const std::unique_ptr<PyTensorType>& x) {
+                             return x->get_backend() == backend && x->get_scalar_type() == scalar_type;
+                         });
+  if (it != tensor_types.end())
+    return (*it).get();
+  return nullptr;
+}
+
+static THPObjectPtr get_storage_obj(Backend backend, ScalarType dtype) {
+  auto module_obj = get_py_module(backend);
   auto storage_name = std::string(toString(dtype)) + "Storage";
   THPObjectPtr storage(
       PyObject_GetAttrString(module_obj.get(), storage_name.c_str()));
@@ -284,24 +297,6 @@ static THPObjectPtr get_tensor_dict() {
   return res;
 }
 
-// A note about the lifetime of the various PyTensorType: normally
-// PyTypeObject instances are statically allocated, but we want to create them
-// dynamically at init time, because their exact number depends on
-// torch::utils::all_declared_types(). The memory for each PyTensorType is
-// allocated by initialize_aten_types() and never freed: technically it's a
-// leak, but it's not a problem since we want them to be alive for the whole
-// time of the process anyway.
-//
-// An alternative is to use a std::vector<PyTensorType> instead, and let
-// std::vector to manage the lifetime of its items. This is problematic
-// though, because it means that the memory of PyTensorType is deallocated at
-// some point during the exit: if by chance we have another global destructor
-// and/or atexit() function which tries to access the PyTensorTypes, we risk
-// an use-after-free error. This happens for example if we embed CPython and
-// call Py_Finalize inside an atexit() function which was registered before
-// importing torch.
-static std::vector<PyTensorType*> tensor_types;
-
 void set_default_storage_type(Backend backend, ScalarType dtype) {
   THPObjectPtr storage = get_storage_obj(backend, dtype);
 
@@ -344,13 +339,13 @@ void set_default_tensor_type(
   }
 }
 
-static void initialize_aten_types(std::vector<PyTensorType*>& tensor_types) {
+static void initialize_aten_types(std::vector<std::unique_ptr<PyTensorType>>& tensor_types) {
   // includes CUDA types even when PyTorch is not built with CUDA
   auto declared_types = torch::utils::all_declared_types();
   tensor_types.resize(declared_types.size());
 
   for (size_t i = 0, end = declared_types.size(); i != end; i++) {
-    tensor_types[i] = new PyTensorType();
+    tensor_types[i] = std::make_unique<PyTensorType>();
     auto& tensor_type = *tensor_types[i];
     Backend backend = declared_types[i].first;
     ScalarType scalar_type = declared_types[i].second;
@@ -386,46 +381,59 @@ void initialize_python_bindings() {
   // Add the type objects to their corresponding modules. e.g. torch.FloatTensor
   // is added to the `torch` module as `FloatTensor`. Also add all the type
   // objects to the set torch._tensor_classes.
-  py_bind_tensor_types(tensor_types);
+  for (auto& tensor_type : tensor_types) {
+    py_bind_tensor_type(*tensor_type);
+  }
 }
 
-static void py_bind_tensor_types(
-    const std::vector<PyTensorType*>& tensor_types) {
-  auto torch_module = THPObjectPtr(PyImport_ImportModule("torch"));
-  if (!torch_module)
-    throw python_error();
+PyObject* register_python_tensor_type(Backend backend, ScalarType scalar_type) {
+  PyTensorType* registed_tensor_type = get_tensor_type(backend, scalar_type);
+  if (registed_tensor_type)
+    return (PyObject*)registed_tensor_type;
 
-  auto tensor_classes = THPObjectPtr(
-      PyObject_GetAttrString(torch_module.get(), "_tensor_classes"));
-  if (!tensor_classes)
-    throw python_error();
+  std::unique_ptr<PyTensorType> new_tensor_type = std::make_unique<PyTensorType>();
 
-  for (auto& tensor_type : tensor_types) {
-    auto name = std::string(tensor_type->name);
-    auto idx = name.rfind('.');
-    auto type_name = name.substr(idx + 1);
-    auto module_name = name.substr(0, idx);
+  set_type(*new_tensor_type, backend, scalar_type);
+  set_name(*new_tensor_type, get_name(backend, scalar_type));
 
-    auto module_obj = THPObjectPtr(PyImport_ImportModule(module_name.c_str()));
-    if (!module_obj)
-      throw python_error();
+  auto tensor_dict = get_tensor_dict();
 
-    PyObject* type_obj = (PyObject*)tensor_type;
-    Py_INCREF(type_obj);
-    if (PyModule_AddObject(module_obj.get(), type_name.c_str(), type_obj) < 0) {
-      throw python_error();
-    }
-    if (PySet_Add(tensor_classes.get(), type_obj) < 0) {
-      throw python_error();
-    }
+  py_initialize_tensor_type(new_tensor_type->py_type, new_tensor_type->name, tensor_dict.get());
+
+  py_bind_tensor_type(*new_tensor_type);
+
+  PyTensorType* ret = new_tensor_type.get();
+  tensor_types.push_back(std::move(new_tensor_type));
+  return (PyObject*)ret;
+}
+
+static void py_bind_tensor_type(const PyTensorType& tensor_type) {
+  auto torch_module = THPObjectPtr(PyImport_ImportModule("torch"));
+  if (!torch_module) throw python_error();
+
+  auto tensor_classes = THPObjectPtr(PyObject_GetAttrString(torch_module.get(), "_tensor_classes"));
+  if (!tensor_classes) throw python_error();
+
+  auto name = std::string(tensor_type.name);
+  auto idx = name.rfind('.');
+  auto type_name = name.substr(idx + 1);
+  auto module_obj = get_py_module(tensor_type.get_backend());
+
+  PyObject* type_obj = (PyObject*)&tensor_type;
+  Py_INCREF(type_obj);
+  if (PyModule_AddObject(module_obj.get(), type_name.c_str(), type_obj) < 0) {
+    throw python_error();
+  }
+  if (PySet_Add(tensor_classes.get(), type_obj) < 0) {
+    throw python_error();
   }
 }
 
 static bool PyTensorType_Check(PyObject* obj) {
-  auto it = std::find_if(
-      tensor_types.begin(), tensor_types.end(), [obj](PyTensorType* x) {
-        return (PyObject*)x == obj;
-      });
+  auto it = std::find_if(tensor_types.begin(), tensor_types.end(),
+    [obj](const std::unique_ptr<PyTensorType>& x) {
+      return (PyObject*)(x.get()) == obj;
+    });
   return it != tensor_types.end();
 }
 
diff --git a/torch/csrc/tensor/python_tensor.h b/torch/csrc/tensor/python_tensor.h
index ab74e05341..5827f3b4ce 100644
--- a/torch/csrc/tensor/python_tensor.h
+++ b/torch/csrc/tensor/python_tensor.h
@@ -4,6 +4,11 @@
 #include <c10/core/DispatchKey.h>
 #include <c10/core/ScalarType.h>
 #include <torch/csrc/python_headers.h>
+#include <c10/core/Backend.h>
+
+namespace c10 {
+struct Device;
+}
 
 namespace at {
 class Tensor;
@@ -22,6 +27,9 @@ void py_set_default_tensor_type(PyObject* type_obj);
 // Same as py_set_default_tensor_type, but only changes the dtype (ScalarType).
 void py_set_default_dtype(PyObject* dtype_obj);
 
+// Register the tensor type for specific Backend and dtype (ScalarType).
+PyObject* register_python_tensor_type(c10::Backend backend, c10::ScalarType scalar_type);
+
 // Gets the DispatchKey for the default tensor type.
 //
 // TODO: This is nuts!  There is no reason to let the default tensor type id
diff --git a/torch/csrc/utils/tensor_types.cpp b/torch/csrc/utils/tensor_types.cpp
index 8a20c93d87..0d3c6c1f31 100644
--- a/torch/csrc/utils/tensor_types.cpp
+++ b/torch/csrc/utils/tensor_types.cpp
@@ -19,7 +19,7 @@ using namespace at;
 namespace torch {
 namespace utils {
 
-static const char* backend_to_string(const at::Backend& backend) {
+const char* backend_to_string(const at::Backend& backend) {
   switch (backend) {
     case at::Backend::CPU:
       return "torch";
diff --git a/torch/csrc/utils/tensor_types.h b/torch/csrc/utils/tensor_types.h
index b45c284c9c..729b3e326f 100644
--- a/torch/csrc/utils/tensor_types.h
+++ b/torch/csrc/utils/tensor_types.h
@@ -8,6 +8,7 @@
 namespace torch {
 namespace utils {
 
+const char* backend_to_string(const at::Backend& backend);
 std::string options_to_string(const at::TensorOptions options);
 std::string type_to_string(const at::DeprecatedTypeProperties& type);
 at::TensorOptions options_from_string(const std::string& str);
-- 
2.25.1

