From 602734510615ec93a5e05364aa5995eb4e624ef9 Mon Sep 17 00:00:00 2001
From: "Wang, Chuanqi" <chuanqi.wang@intel.com>
Date: Fri, 10 Nov 2023 16:40:18 +0800
Subject: [PATCH 16/20] Fix dynamo benchmark random seed for xpu device (#181)

* Fix dynamo benchmark random seed for xpu device

* add comments.

---------

Co-authored-by: Yu, Guangye <106960996+guangyey@users.noreply.github.com>
---
 benchmarks/dynamo/common.py | 10 ++++++----
 1 file changed, 6 insertions(+), 4 deletions(-)

diff --git a/benchmarks/dynamo/common.py b/benchmarks/dynamo/common.py
index e733f41052..9539e45c2d 100644
--- a/benchmarks/dynamo/common.py
+++ b/benchmarks/dynamo/common.py
@@ -3096,6 +3096,12 @@ def run(runner, args, original_dir=None):
                 "DLRM+DDP is unsupported as it requires sharding the embedding layer separately from DDP"
             )
             return sys.exit(-1)
+
+    # As an out-of-tree backend, we have to install XPU runtime callbacks at runtime.
+    if "xpu" in args.devices:
+        import intel_extension_for_pytorch as ipex
+        pass
+
     if args.accuracy:
         # Use small batch size. We use >1 batch size to ensure we test
         # batch_norm type of operators that work on batch dims.
@@ -3163,10 +3169,6 @@ def run(runner, args, original_dir=None):
             log.warning("torch.cuda.is_available() == False, using CPU")
             args.devices = ["cpu"]
 
-    if "xpu" in args.devices:
-        import intel_extension_for_pytorch as ipex
-        pass
-
     if args.devices != ["cpu"] and (torch.cuda.is_available() or torch.xpu.is_available()):
         global synchronize
         synchronize = torch.cuda.synchronize if (torch.cuda.is_available()) else torch.xpu.synchronize
-- 
2.34.1

