diff --git a/aten/src/ATen/SparseTensorImpl.cpp b/aten/src/ATen/SparseTensorImpl.cpp
index a7354047ca..3bf11a646f 100644
--- a/aten/src/ATen/SparseTensorImpl.cpp
+++ b/aten/src/ATen/SparseTensorImpl.cpp
@@ -9,6 +9,8 @@ namespace {
   DeviceType sparseTensorSetToDeviceType(DispatchKeySet key_set) {
     if (key_set.has(DispatchKey::SparseCPUTensorId)) {
       return kCPU;
+    } else if (key_set.has(DispatchKey::SparseDPCPPTensorId)) {
+      return kDPCPP;
     } else if (key_set.has(DispatchKey::SparseCUDATensorId)) {
       return kCUDA;
     } else {
diff --git a/aten/src/ATen/core/VariableFallbackKernel.cpp b/aten/src/ATen/core/VariableFallbackKernel.cpp
index 11d03495c6..3cb40c8cb7 100644
--- a/aten/src/ATen/core/VariableFallbackKernel.cpp
+++ b/aten/src/ATen/core/VariableFallbackKernel.cpp
@@ -32,9 +32,9 @@ void variable_fallback_kernel(const OperatorHandle& op, Stack* stack) {
     Dispatcher::singleton().callBoxed(op, stack);
 }
 
-static auto registry = Dispatcher::singleton().registerBackendFallbackKernel(
-    DispatchKey::VariableTensorId,
-    KernelFunction::makeFromBoxedFunction<&variable_fallback_kernel>()
-);
+// static auto registry = Dispatcher::singleton().registerBackendFallbackKernel(
+//     DispatchKey::VariableTensorId,
+//     KernelFunction::makeFromBoxedFunction<&variable_fallback_kernel>()
+// );
 
 }
diff --git a/aten/src/ATen/native/sparse/SparseTensor.cpp b/aten/src/ATen/native/sparse/SparseTensor.cpp
index 8ce6045ddc..ba7f79e54e 100644
--- a/aten/src/ATen/native/sparse/SparseTensor.cpp
+++ b/aten/src/ATen/native/sparse/SparseTensor.cpp
@@ -76,6 +76,8 @@ SparseTensor new_sparse(const TensorOptions& options) {
   DispatchKey dispatch_key;
   if (options.device().is_cuda()) {
     dispatch_key = DispatchKey::SparseCUDATensorId;
+  } else if (options.device().is_dpcpp()) {
+    dispatch_key = DispatchKey::SparseDPCPPTensorId;
   } else {
     dispatch_key = DispatchKey::SparseCPUTensorId;
   }
diff --git a/c10/core/Backend.h b/c10/core/Backend.h
index 5f3d8c7733..a47240b139 100644
--- a/c10/core/Backend.h
+++ b/c10/core/Backend.h
@@ -25,18 +25,22 @@ namespace c10 {
  * or "SparseCUDA"; backend in torch.backends is something like "MKL" or
  * "CUDNN".
  */
-enum class Backend { CPU, CUDA, HIP, SparseCPU, SparseCUDA, SparseHIP, MSNPU, XLA, QuantizedCPU, Undefined, MkldnnCPU, NumOptions };
+enum class Backend { CPU, CUDA, HIP, SparseCPU, SparseCUDA, SparseHIP, MSNPU, XLA, QuantizedCPU, Undefined, MkldnnCPU, DPCPP, SparseDPCPP, NumOptions };
 
 static inline Backend toSparse(Backend b) {
   switch (b) {
     case Backend::CPU:
       return Backend::SparseCPU;
+    case Backend::DPCPP:
+      return Backend::SparseDPCPP;
     case Backend::CUDA:
       return Backend::SparseCUDA;
     case Backend::HIP:
       return Backend::SparseHIP;
     case Backend::SparseCPU:
       return Backend::SparseCPU;
+    case Backend::SparseDPCPP:
+      return Backend::SparseDPCPP;
     case Backend::SparseCUDA:
       return Backend::SparseCUDA;
     case Backend::SparseHIP:
@@ -58,6 +62,10 @@ static inline Backend toDense(Backend b) {
       return Backend::MSNPU;
     case Backend::XLA:
       return Backend::XLA;
+    case Backend::DPCPP:
+      return Backend::DPCPP;
+    case Backend::SparseDPCPP:
+      return Backend::DPCPP;
     case Backend::SparseCPU:
       return Backend::CPU;
     case Backend::SparseCUDA:
@@ -82,6 +90,10 @@ static inline Backend dispatchKeyToBackend(DispatchKey t) {
     return Backend::MSNPU;
   } else if (t == DispatchKey::XLATensorId || t == DispatchKey::XLAPreAutograd) {
     return Backend::XLA;
+  } else if (t == DispatchKey::DPCPPTensorId) {
+    return Backend::DPCPP;
+  } else if (t == DispatchKey::SparseDPCPPTensorId) {
+    return Backend::SparseDPCPP;
   } else if (t == DispatchKey::SparseCPUTensorId) {
     return Backend::SparseCPU;
   } else if (t == DispatchKey::SparseCUDATensorId) {
@@ -111,6 +123,10 @@ static inline DispatchKey backendToDispatchKey(Backend b) {
       return DispatchKey::MSNPUTensorId;
     case Backend::XLA:
       return DispatchKey::XLATensorId;
+    case Backend::DPCPP:
+      return DispatchKey::DPCPPTensorId;
+    case Backend::SparseDPCPP:
+      return DispatchKey::SparseDPCPPTensorId;
     case Backend::SparseCPU:
       return DispatchKey::SparseCPUTensorId;
     case Backend::SparseCUDA:
@@ -146,6 +162,9 @@ static inline DeviceType backendToDeviceType(Backend b) {
       return DeviceType::CUDA;
     case Backend::SparseHIP:
       return DeviceType::HIP;
+    case Backend::DPCPP:
+    case Backend::SparseDPCPP:
+      return DeviceType::DPCPP;
     case Backend::MkldnnCPU:
     case Backend::QuantizedCPU:
       return DeviceType::CPU;
@@ -158,12 +177,14 @@ static inline DeviceType backendToDeviceType(Backend b) {
 
 static inline Backend backendToCPU(Backend b) {
   switch (b) {
+    case Backend::DPCPP:
     case Backend::CPU:
       return Backend::CPU;
     case Backend::CUDA:
       return Backend::CPU;
     case Backend::HIP:
       return Backend::CPU;
+    case Backend::SparseDPCPP:
     case Backend::SparseCPU:
       return Backend::SparseCPU;
     case Backend::SparseCUDA:
@@ -186,12 +207,14 @@ static inline Backend backendToCPU(Backend b) {
 
 static inline Backend backendToCUDA(Backend b) {
   switch (b) {
+    case Backend::DPCPP:
     case Backend::CPU:
     case Backend::CUDA:
     case Backend::HIP:
     case Backend::MSNPU:
     case Backend::XLA:
       return Backend::CUDA;
+    case Backend::SparseDPCPP:
     case Backend::SparseCPU:
     case Backend::SparseCUDA:
     case Backend::SparseHIP:
@@ -205,12 +228,14 @@ static inline Backend backendToCUDA(Backend b) {
 
 static inline Backend backendToHIP(Backend b) {
   switch (b) {
+    case Backend::DPCPP:
     case Backend::CPU:
     case Backend::CUDA:
     case Backend::HIP:
     case Backend::MSNPU:
     case Backend::XLA:
       return Backend::HIP;
+    case Backend::SparseDPCPP:
     case Backend::SparseCPU:
     case Backend::SparseCUDA:
     case Backend::SparseHIP:
@@ -225,6 +250,8 @@ static inline Backend backendToHIP(Backend b) {
 // TODO: This probably shouldn't actually be static inline
 static inline const char* toString(Backend b) {
   switch (b) {
+    case Backend::DPCPP:
+      return "DPCPP";
     case Backend::CPU:
       return "CPU";
     case Backend::CUDA:
@@ -235,6 +262,8 @@ static inline const char* toString(Backend b) {
       return "MSNPU";
     case Backend::XLA:
       return "XLA";
+    case Backend::SparseDPCPP:
+      return "SparseDPCPP";
     case Backend::SparseCPU:
       return "SparseCPU";
     case Backend::SparseCUDA:
@@ -252,6 +281,7 @@ static inline const char* toString(Backend b) {
 
 static inline bool isSparse(Backend b) {
   switch (b) {
+    case Backend::SparseDPCPP:
     case Backend::SparseCPU:
     case Backend::SparseCUDA:
     case Backend::SparseHIP:
diff --git a/c10/core/Device.cpp b/c10/core/Device.cpp
index 82a02fdf04..82680381ed 100644
--- a/c10/core/Device.cpp
+++ b/c10/core/Device.cpp
@@ -13,7 +13,7 @@
 namespace c10 {
 namespace {
 DeviceType parse_type(const std::string& device_string) {
-  static const std::array<std::pair<std::string, DeviceType>, 9> types = {{
+  static const std::array<std::pair<std::string, DeviceType>, 10> types = {{
       {"cpu", DeviceType::CPU},
       {"cuda", DeviceType::CUDA},
       {"mkldnn", DeviceType::MKLDNN},
@@ -23,6 +23,7 @@ DeviceType parse_type(const std::string& device_string) {
       {"hip", DeviceType::HIP},
       {"msnpu", DeviceType::MSNPU},
       {"xla", DeviceType::XLA},
+      {"dpcpp", DeviceType::DPCPP},
   }};
   auto device = std::find_if(
       types.begin(),
@@ -34,7 +35,7 @@ DeviceType parse_type(const std::string& device_string) {
     return device->second;
   }
   AT_ERROR(
-      "Expected one of cpu, cuda, mkldnn, opengl, opencl, ideep, hip, msnpu device type at start of device string: ", device_string);
+      "Expected one of cpu, cuda, mkldnn, opengl, opencl, ideep, dpcpp, hip, msnpu device type at start of device string: ", device_string);
 }
 } // namespace
 
diff --git a/c10/core/Device.h b/c10/core/Device.h
index f1249e865f..31f15e82d5 100644
--- a/c10/core/Device.h
+++ b/c10/core/Device.h
@@ -86,6 +86,11 @@ struct C10_API Device final {
     return type_ == DeviceType::CPU;
   }
 
+  /// Return true if the device is of DPCPP type.
+  bool is_dpcpp() const noexcept {
+    return type_ == DeviceType::DPCPP;
+  }
+
   /// Same string as returned from operator<<.
   std::string str() const;
 
diff --git a/c10/core/DeviceType.cpp b/c10/core/DeviceType.cpp
index 017267cd97..486b02e1ec 100644
--- a/c10/core/DeviceType.cpp
+++ b/c10/core/DeviceType.cpp
@@ -27,6 +27,8 @@ std::string DeviceTypeName(DeviceType d, bool lower_case) {
       return lower_case ? "msnpu" : "MSNPU";
     case DeviceType::XLA:
       return lower_case ? "xla" : "XLA";
+    case DeviceType::DPCPP:
+      return lower_case ? "dpcpp" : "DPCPP";
     default:
       AT_ERROR(
           "Unknown device: ",
@@ -59,6 +61,7 @@ bool isValidDeviceType(DeviceType d) {
     case DeviceType::FPGA:
     case DeviceType::MSNPU:
     case DeviceType::XLA:
+    case DeviceType::DPCPP:
       return true;
     default:
       return false;
diff --git a/c10/core/DeviceType.h b/c10/core/DeviceType.h
index 9f759666d7..23f68080d4 100644
--- a/c10/core/DeviceType.h
+++ b/c10/core/DeviceType.h
@@ -23,11 +23,12 @@ enum class DeviceType : int16_t {
   FPGA = 7, // FPGA
   MSNPU = 8, // MSNPU
   XLA = 9, // XLA / TPU
+  DPCPP = 10, // DPCPP
   // NB: If you add more devices:
   //  - Change the implementations of DeviceTypeName and isValidDeviceType
   //    in DeviceType.cpp
   //  - Change the number below
-  COMPILE_TIME_MAX_DEVICE_TYPES = 10,
+  COMPILE_TIME_MAX_DEVICE_TYPES = 11,
   ONLY_FOR_TEST = 20901, // This device type is only for test.
 };
 
@@ -36,6 +37,7 @@ constexpr DeviceType kCUDA = DeviceType::CUDA;
 constexpr DeviceType kHIP = DeviceType::HIP;
 constexpr DeviceType kMSNPU = DeviceType::MSNPU;
 constexpr DeviceType kXLA = DeviceType::XLA;
+constexpr DeviceType kDPCPP = DeviceType::DPCPP;
 
 // define explicit int constant
 constexpr int COMPILE_TIME_MAX_DEVICE_TYPES =
diff --git a/c10/core/DispatchKey.cpp b/c10/core/DispatchKey.cpp
index cf20e515c2..b093a648b0 100644
--- a/c10/core/DispatchKey.cpp
+++ b/c10/core/DispatchKey.cpp
@@ -36,6 +36,10 @@ const char* toString(DispatchKey t) {
       return "QuantizedCPUTensorId";
     case DispatchKey::VariableTensorId:
       return "VariableTensorId";
+    case DispatchKey::DPCPPTensorId:
+      return "DPCPPTensorId";
+    case DispatchKey::SparseDPCPPTensorId:
+      return "SparseDPCPPTensorId";
     case DispatchKey::BackendSelect:
       return "BackendSelect";
     case DispatchKey::TESTING_ONLY_GenericModeTensorId:
diff --git a/c10/core/DispatchKey.h b/c10/core/DispatchKey.h
index da7c3c564e..1330c2be6c 100644
--- a/c10/core/DispatchKey.h
+++ b/c10/core/DispatchKey.h
@@ -90,6 +90,9 @@ enum class DispatchKey : uint8_t {
   SparseCUDATensorId, // registered at build/aten/src/ATen/SparseCUDAType.cpp
   SparseHIPTensorId,  // TODO: I think this is not actually used, due to Note [Masquerading as CUDA]
 
+  DPCPPTensorId, // DPCPP only
+  SparseDPCPPTensorId, // DPCPP only
+
   // Here are reserved backends for user-defined backends, see Note [Private use TensorId]
   // To see some example about how to use this, check out MSNPU
   PrivateUse1_TensorId,
diff --git a/c10/core/Layout.h b/c10/core/Layout.h
index c5ecc8908a..4aab4408f5 100644
--- a/c10/core/Layout.h
+++ b/c10/core/Layout.h
@@ -15,6 +15,7 @@ constexpr auto kMkldnn = Layout::Mkldnn;
 inline Layout layout_from_backend(Backend backend) {
   switch (backend) {
     case Backend::SparseCPU:
+    case Backend::SparseDPCPP:
     case Backend::SparseCUDA:
     case Backend::SparseHIP:
       return Layout::Sparse;
diff --git a/c10/core/TensorImpl.h b/c10/core/TensorImpl.h
index de11b22141..57513aeb89 100644
--- a/c10/core/TensorImpl.h
+++ b/c10/core/TensorImpl.h
@@ -425,7 +425,8 @@ struct C10_API TensorImpl : public c10::intrusive_ptr_target {
     // NB: This method is not virtual and avoid dispatches for performance reasons.
     return key_set_.has(DispatchKey::SparseCPUTensorId) ||
            key_set_.has(DispatchKey::SparseCUDATensorId) ||
-           key_set_.has(DispatchKey::SparseHIPTensorId);
+           key_set_.has(DispatchKey::SparseHIPTensorId) ||
+           key_set_.has(DispatchKey::SparseDPCPPTensorId);
   }
 
   bool is_quantized() const {
@@ -433,6 +434,11 @@ struct C10_API TensorImpl : public c10::intrusive_ptr_target {
     return key_set_.has(DispatchKey::QuantizedCPUTensorId);
   }
 
+  bool is_dpcpp() const {
+    return key_set_.has(DispatchKey::DPCPPTensorId) ||
+           key_set_.has(DispatchKey::SparseDPCPPTensorId);
+  }
+
   bool is_cuda() const {
     // NB: This method is not virtual and avoid dispatches for performance reasons.
     return key_set_.has(DispatchKey::CUDATensorId) ||
@@ -866,12 +872,14 @@ struct C10_API TensorImpl : public c10::intrusive_ptr_target {
     auto is_dense = [](DispatchKeySet ts) {
       return ts.has(DispatchKey::CPUTensorId) ||
              ts.has(DispatchKey::CUDATensorId) ||
-             ts.has(DispatchKey::HIPTensorId);
+             ts.has(DispatchKey::HIPTensorId) ||
+             ts.has(DispatchKey::DPCPPTensorId);
     };
     auto is_sparse = [](DispatchKeySet ts) {
       return ts.has(DispatchKey::SparseCPUTensorId) ||
              ts.has(DispatchKey::SparseCUDATensorId) ||
-             ts.has(DispatchKey::SparseHIPTensorId);
+             ts.has(DispatchKey::SparseHIPTensorId) ||
+             ts.has(DispatchKey::SparseDPCPPTensorId);
     };
     return (key_set_ == from) || (is_dense(key_set_) && is_dense(from)) || (is_sparse(key_set_) && is_sparse(from));
   }
diff --git a/c10/core/TensorOptions.h b/c10/core/TensorOptions.h
index 9a4c9b3eb9..6d0240564a 100644
--- a/c10/core/TensorOptions.h
+++ b/c10/core/TensorOptions.h
@@ -398,6 +398,8 @@ struct C10_API TensorOptions {
             return DispatchKey::MSNPUTensorId;
           case DeviceType::XLA:
             return DispatchKey::XLATensorId;
+          case DeviceType::DPCPP:
+            return DispatchKey::DPCPPTensorId;
           default:
             AT_ERROR("Unsupported device type for dense layout: ", device().type());
         }
@@ -409,6 +411,8 @@ struct C10_API TensorOptions {
             return DispatchKey::SparseCUDATensorId;
           case DeviceType::HIP:
             return DispatchKey::SparseHIPTensorId;
+          case DeviceType::DPCPP:
+            return DispatchKey::SparseDPCPPTensorId;
           default:
             AT_ERROR("Unsupported device type for sparse layout: ", device().type());
         }
@@ -634,6 +638,10 @@ inline DeviceType computeDeviceType(DispatchKey tid) {
     return DeviceType::MSNPU;
   } else if (tid == DispatchKey::XLATensorId) {
     return DeviceType::XLA;
+  } else if (tid == DispatchKey::DPCPPTensorId) {
+    return DeviceType::DPCPP;
+  } else if (tid == DispatchKey::SparseDPCPPTensorId) {
+    return DeviceType::DPCPP;
   } else if (tid == DispatchKey::SparseCPUTensorId) {
     return DeviceType::CPU;
   } else if (tid == DispatchKey::SparseCUDATensorId) {
diff --git a/torch/csrc/jit/passes/quantization.cpp b/torch/csrc/jit/passes/quantization.cpp
index a6a9fca462..7c740af04b 100644
--- a/torch/csrc/jit/passes/quantization.cpp
+++ b/torch/csrc/jit/passes/quantization.cpp
@@ -1758,22 +1758,83 @@ std::tuple<at::Tensor, at::Tensor> FoldConvBatchNorm2dHelper::
   return std::make_tuple(new_w, new_b);
 }

+bool extractOptionalBNParams(
+    const script::Module& bn,
+    ConvBNParameters& r) {
+  auto bn_forward = bn.get_method("forward");
+  auto graph = bn_forward.graph();
+  const PatternInfo& pattern_bn = PatternInfo::parse_from_str(R"(
+      graph(%a, %weight, %bias, %running_mean, %running_var,
+          %training, %momentum, %eps, %cudnn_enabled):
+        %bn_out = aten::batch_norm(%a, %weight, %bias, %running_mean,
+            %running_var, %training, %momentum, %eps, %cudnn_enabled)
+        return (%bn_out) )");
+  const Graph& pattern_bn_graph = *pattern_bn.pattern_graph;
+  const auto& bn_vmap = pattern_bn.vmap;
+
+  const auto& matches = findPatternMatches(pattern_bn_graph, *graph);
+
+  if (matches.size() > 1) {
+    return false;
+  }
+
+  if (bn.hasattr("eps")) {
+    r.bn_eps = bn.attr("eps").toDouble();
+  } else {
+    auto optional_eps = toIValue(matches[0].values_map.at(bn_vmap.at("eps")));
+    if(!optional_eps) {
+      return false;
+    }
+    r.bn_eps = optional_eps.value().toDouble();
+  }
+  r.bn_w = at::ones_like(bn.attr("running_mean").toTensor());
+  if (bn.hasattr("weight")) {
+    if (bn.attr("weight").isTensor()) {
+      r.bn_w = bn.attr("weight").toTensor();
+    }
+  } else {
+    auto optional_bn_weight =
+      toIValue(matches[0].values_map.at(bn_vmap.at("weight")));
+    if (!optional_bn_weight) {
+      return false;
+    }
+    if (optional_bn_weight.value().isTensor()) {
+      r.bn_w = optional_bn_weight.value().toTensor();
+    }
+  }
+  r.bn_b = at::zeros_like(bn.attr("running_mean").toTensor());
+  if (bn.hasattr("bias")) {
+    if (bn.attr("bias").isTensor()) {
+      r.bn_b = bn.attr("bias").toTensor();
+    }
+  } else {
+    auto optional_bn_bias =
+      toIValue(matches[0].values_map.at(bn_vmap.at("bias")));
+    if (!optional_bn_bias) {
+      return false;
+    }
+
+    if (optional_bn_bias.value().isTensor()) {
+      r.bn_b = optional_bn_bias.value().toTensor();
+    }
+  }
+  return true;
+}
+
 bool FoldConvBatchNorm2dHelper::tryExtractingConvBNParameters(
     Module& conv,
     Module& bn,
     ConvBNParameters& r) {
   if (!hastensor(conv, "weight") || !conv.hasattr("bias") ||
-      !hastensor(bn, "weight") || !hastensor(bn, "bias") ||
-      !hastensor(bn, "running_mean") || !hastensor(bn, "running_var") ||
-      !bn.hasattr("eps")) {
+      !hastensor(bn, "running_mean") || !hastensor(bn, "running_var")) {
     return false;
   }

   r.bn_rm = bn.attr("running_mean").toTensor();
   r.bn_rv = bn.attr("running_var").toTensor();
-  r.bn_eps = bn.attr("eps").toDouble();
-  r.bn_w = bn.attr("weight").toTensor();
-  r.bn_b = bn.attr("bias").toTensor();
+  if (!extractOptionalBNParams(bn, r)) {
+    return false;
+  }

   r.conv_w = conv.attr("weight").toTensor();
   r.conv_b = at::zeros_like(r.bn_rm);
@@ -1788,9 +1849,9 @@ bool FoldConvBatchNorm2dHelper::tryExtractingConvBNParameters(
 void FoldConvBatchNorm2dHelper::analyze(Module& module) {
   const PatternInfo pattern = PatternInfo::parse_from_str(R"IR(
 graph(%self, %x):
-    %conv_submodule = match::module[name="Conv2d"](%self)
+    %conv_submodule = match::module[name=".Conv2d"](%self)
     %conv_out = prim::CallMethod[name="forward"](%conv_submodule, %x)
-    %bn_submodule = match::module[name="BatchNorm2d"](%self)
+    %bn_submodule = match::module[name=".BatchNorm2d"](%self)
     %bn_out = prim::CallMethod[name="forward"](%bn_submodule, %conv_out)
     return (%bn_out))IR");

@@ -1921,6 +1982,58 @@ void FoldConvBatchNorm2dHelper::transform() {
   }
 }

+
+void replaceConv2dBiasWithGetAttr(Module& module) {
+  auto graph = module.get_method("forward").graph();
+  // Only looks fors _convolution pattern.
+  // Thus assumes that tracing will have always gotten rid of aten::conv2d.
+  // If it did not, BN folding will fail.
+  const PatternInfo& pattern_convolution = PatternInfo::parse_from_str(R"(
+      graph(%a, %w, %b, %stride:int[], %padding:int[], %dilation:int[],
+          %transposed:bool, %output_padding:int[], %groups:int, %benchmark:bool,
+          %deterministic:bool, %cudnn_enabled:bool):
+        %conv_out = aten::_convolution(%a, %w, %b, %stride, %padding, %dilation,
+            %transposed, %output_padding, %groups, %benchmark, %deterministic, %cudnn_enabled)
+        return (%conv_out) )");
+  const Graph& pattern_convolution_graph = *pattern_convolution.pattern_graph;
+  const auto& convolution_vmap = pattern_convolution.vmap;
+
+  const auto& matches = findPatternMatches(pattern_convolution_graph, *graph);
+  for (const auto& match : matches) {
+    // We come here only if the bias was not present in the module.
+    // In that case, the corresponding graph will not have getAttr("bias")
+    // Insert that in the graph.
+    // And change _convolution to take the new value.
+    auto conv_node = match.values_map.at(convolution_vmap.at("conv_out"))->node();
+    WithInsertPoint ins(conv_node);
+    Value* bias_attr_val =
+      graph->insertGetAttr(graph->inputs()[0], "bias")
+          ->setType(TensorType::get());
+    constexpr size_t conv_bias_index = 2;
+    conv_node->replaceInput(conv_bias_index, bias_attr_val);
+  }
+}
+
+void addBiasForConv2dIfNone(Module& module) {
+  auto t = module.type()->expect<ClassType>();
+  auto real_typename = t->name()->qualifiedName();
+  const std::string pattern_name("Conv2d");
+  if (real_typename.size() >= pattern_name.size() &&
+      (0 == real_typename.compare(real_typename.size() - pattern_name.size(),
+                                  pattern_name.size(), pattern_name))) {
+    if (!t->hasAttribute("bias")) {
+      auto optional_tensor_type = OptionalType::create(TensorType::get());
+      t->addAttribute("bias", optional_tensor_type, true);
+      auto optional_tensor = c10::optional<at::Tensor>();
+      module.setattr("bias", optional_tensor);
+      replaceConv2dBiasWithGetAttr(module);
+    }
+  }
+  for (Module m : module.children()) {
+    addBiasForConv2dIfNone(m);
+  }
+}
+
 } // namespace

 TORCH_API Module InsertObservers(
@@ -2083,6 +2196,7 @@ void QuantFusion(std::shared_ptr<Graph>& graph) {
 Module FoldConvBatchNorm2d(const Module& module) {
   FoldConvBatchNorm2dHelper h;
   Module m = module.clone();
+  addBiasForConv2dIfNone(m);
   h.analyze(m);
   h.transform();
   return m;
diff --git a/torch/csrc/utils/tensor_layouts.cpp b/torch/csrc/utils/tensor_layouts.cpp
index 6fcd84f3db..e42a7baf27 100644
--- a/torch/csrc/utils/tensor_layouts.cpp
+++ b/torch/csrc/utils/tensor_layouts.cpp
@@ -23,6 +23,7 @@ void initializeLayouts() {
   registerLayoutObject((THPLayout*)strided_layout, at::Backend::CUDA);
   registerLayoutObject((THPLayout*)strided_layout, at::Backend::MSNPU);
   registerLayoutObject((THPLayout*)strided_layout, at::Backend::XLA);
+  registerLayoutObject((THPLayout*)strided_layout, at::Backend::DPCPP);
   registerLayoutObject((THPLayout*)strided_layout, at::Backend::QuantizedCPU);
 
   PyObject *sparse_coo_layout = THPLayout_New(at::Layout::Sparse, "torch.sparse_coo");
diff --git a/torch/csrc/utils/tensor_new.cpp b/torch/csrc/utils/tensor_new.cpp
index 85add73c57..ea90778e4b 100644
--- a/torch/csrc/utils/tensor_new.cpp
+++ b/torch/csrc/utils/tensor_new.cpp
@@ -59,6 +59,8 @@ Backend backendToBackendOfDeviceType(Backend b, DeviceType d) {
     case DeviceType::XLA:
       TORCH_CHECK(!isSparse(b), "Sparse not implemented for XLA");
       return Backend::XLA;
+    case DeviceType::DPCPP:
+      return Backend::DPCPP;
     default:
       AT_ERROR("Unknown device type");
   }
@@ -333,6 +335,7 @@ Tensor legacy_new_from_sequence(
 void check_base_legacy_new(c10::DispatchKey dispatch_key, at::Layout expected_layout) {
   if (expected_layout == c10::kStrided) {
     TORCH_CHECK(dispatch_key == c10::DispatchKey::CPUTensorId
+                || dispatch_key == c10::DispatchKey::DPCPPTensorId
                 || dispatch_key == c10::DispatchKey::CUDATensorId
                 || dispatch_key == c10::DispatchKey::HIPTensorId
                 || dispatch_key == c10::XLATensorId(),
@@ -344,6 +347,7 @@ void check_base_legacy_new(c10::DispatchKey dispatch_key, at::Layout expected_la
   } else if(expected_layout == c10::kSparse) {
     // NOTE: no sparse XLA
     TORCH_CHECK(dispatch_key == c10::DispatchKey::SparseCPUTensorId
+                || dispatch_key == c10::DispatchKey::SparseDPCPPTensorId
                 || dispatch_key == c10::DispatchKey::SparseCUDATensorId
                 || dispatch_key == c10::DispatchKey::SparseHIPTensorId,
                 "new(): expected DispatchKey: ", c10::DispatchKey::SparseCPUTensorId,
