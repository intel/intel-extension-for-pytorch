diff --git a/bench/run_and_time.sh b/bench/run_and_time.sh
index e241d80..013f0e0 100755
--- a/bench/run_and_time.sh
+++ b/bench/run_and_time.sh
@@ -6,14 +6,29 @@
 #
 #WARNING: must have compiled PyTorch and caffe2
 
+# configure following 3 variables according to your environment
+ncores=28
+nsockets=0
+DATASET_PATH=/mnt/dlrm/dataset/terabyte_bindata
+
 #check if extra argument is passed to the test
 if [[ $# == 1 ]]; then
-    dlrm_extra_option=$1
+    dlrm_extra_option=""
 else
     dlrm_extra_option=""
 fi
 #echo $dlrm_extra_option
 
-python dlrm_s_pytorch.py --arch-sparse-feature-size=128 --arch-mlp-bot="13-512-256-128" --arch-mlp-top="1024-1024-512-256-1" --max-ind-range=40000000 --data-generation=dataset --data-set=terabyte --raw-data-file=./input/day --processed-data-file=./input/terabyte_processed.npz --loss-function=bce --round-targets=True --learning-rate=1.0 --mini-batch-size=2048 --print-freq=2048 --print-time --test-freq=102400 --test-mini-batch-size=16384 --test-num-workers=16 --memory-map --mlperf-logging --mlperf-auc-threshold=0.8025 --mlperf-bin-loader --mlperf-bin-shuffle $dlrm_extra_option 2>&1 | tee run_terabyte_mlperf_pt.log
+export LD_PRELOAD="$HOME/.local/lib/libtcmalloc.so:${CONDA_PREFIX}/lib/libiomp5.so"
+export KMP_BLOCKTIME=1
+export KMP_AFFINITY="granularity=fine,compact,1,0"
+
+numa_cmd="numactl --physcpubind=0-$((ncores-1)) -m $nsockets" #run on one socket, without HT
+
+if [ "x$1" == "xbf16" ] ; then
+    $numa_cmd python -u dlrm_s_pytorch.py --arch-sparse-feature-size=128 --arch-mlp-bot="13-512-256-128" --arch-mlp-top="1024-1024-512-256-1" --max-ind-range=40000000 --data-generation=dataset --data-set=terabyte --raw-data-file=${DATASET_PATH}/day --processed-data-file=${DATASET_PATH}/terabyte_processed.npz --loss-function=bce --round-targets=True --learning-rate=1.0 --mini-batch-size=2048 --print-freq=2048 --print-time --test-freq=102400 --test-mini-batch-size=16384 --num-batches=40960 --test-num-workers=16 --memory-map --mlperf-logging --mlperf-auc-threshold=0.8025 --mlperf-bin-loader --use-ipex --bf16 $dlrm_extra_option 2>&1 | tee run_terabyte_mlperf_pt_bf16.log
+else
+    $numa_cmd python -u dlrm_s_pytorch.py --arch-sparse-feature-size=128 --arch-mlp-bot="13-512-256-128" --arch-mlp-top="1024-1024-512-256-1" --max-ind-range=40000000 --data-generation=dataset --data-set=terabyte --raw-data-file=${DATASET_PATH}/day --processed-data-file=${DATASET_PATH}/terabyte_processed.npz --loss-function=bce --round-targets=True --learning-rate=1.0 --mini-batch-size=2048 --print-freq=2048 --print-time --test-freq=102400 --test-mini-batch-size=16384 --num-batches=40960 --test-num-workers=16 --memory-map --mlperf-logging --mlperf-auc-threshold=0.8025 --mlperf-bin-loader $dlrm_extra_option 2>&1 | tee run_terabyte_mlperf_pt_fp32.log
+fi
 
 echo "done"
diff --git a/dlrm_s_pytorch.py b/dlrm_s_pytorch.py
index 9bf762b..fd51ade 100644
--- a/dlrm_s_pytorch.py
+++ b/dlrm_s_pytorch.py
@@ -86,6 +86,7 @@ from tricks.qr_embedding_bag import QREmbeddingBag
 from tricks.md_embedding_bag import PrEmbeddingBag, md_solver
 
 import sklearn.metrics
+import intel_pytorch_extension as ipex
 
 # from torchviz import make_dot
 # import torch.nn.functional as Functional
@@ -94,6 +95,19 @@ import sklearn.metrics
 exc = getattr(builtins, "IOError", "FileNotFoundError")
 
 
+class Cast(nn.Module):
+    __constants__ = ['to_dtype']
+
+    def __init__(self, to_dtype):
+        super(Cast, self).__init__()
+        self.to_dtype = to_dtype
+
+    def forward(self, input):
+        return input.to(self.to_dtype)
+
+    def extra_repr(self):
+        return 'to(%s)' % self.to_dtype
+
 ### define dlrm in PyTorch ###
 class DLRM_Net(nn.Module):
     def create_mlp(self, ln, sigmoid_layer):
@@ -104,7 +118,10 @@ class DLRM_Net(nn.Module):
             m = ln[i + 1]
 
             # construct fully connected operator
-            LL = nn.Linear(int(n), int(m), bias=True)
+            if self.use_ipex and self.bf16 and i != sigmoid_layer:
+                LL = ipex.LinearFuseRelu(int(n), int(m), bias=True)
+            else:
+                LL = nn.Linear(int(n), int(m), bias=True)
 
             # initialize the weights
             # with torch.no_grad():
@@ -123,13 +140,21 @@ class DLRM_Net(nn.Module):
             # approach 3
             # LL.weight = Parameter(torch.tensor(W),requires_grad=True)
             # LL.bias = Parameter(torch.tensor(bt),requires_grad=True)
+            if self.bf16 and ipex.is_available():
+                LL.to(torch.bfloat16)
+
             layers.append(LL)
 
             # construct sigmoid or relu operator
             if i == sigmoid_layer:
+                if self.bf16:
+                    layers.append(Cast(torch.float32))
                 layers.append(nn.Sigmoid())
             else:
-                layers.append(nn.ReLU())
+                if self.use_ipex and self.bf16:
+                    continue
+                else:
+                    layers.append(nn.ReLU())
 
         # approach 1: use ModuleList
         # return layers
@@ -168,6 +193,8 @@ class DLRM_Net(nn.Module):
                 # EE.weight.data.copy_(torch.tensor(W))
                 # approach 3
                 # EE.weight = Parameter(torch.tensor(W),requires_grad=True)
+                if self.bf16 and ipex.is_available():
+                    EE.to(torch.bfloat16)
 
             emb_l.append(EE)
 
@@ -192,6 +219,8 @@ class DLRM_Net(nn.Module):
         qr_threshold=200,
         md_flag=False,
         md_threshold=200,
+        bf16=False,
+        use_ipex=False,
     ):
         super(DLRM_Net, self).__init__()
 
@@ -212,6 +241,9 @@ class DLRM_Net(nn.Module):
             self.arch_interaction_itself = arch_interaction_itself
             self.sync_dense_params = sync_dense_params
             self.loss_threshold = loss_threshold
+            self.bf16 = bf16
+            self.use_ipex = use_ipex
+            print("########################################### use_ipex: {}, bf16: {}".format(self.use_ipex, self.bf16))
             # create variables for QR embedding if applicable
             self.qr_flag = qr_flag
             if self.qr_flag:
@@ -262,26 +294,30 @@ class DLRM_Net(nn.Module):
 
     def interact_features(self, x, ly):
         if self.arch_interaction_op == "dot":
-            # concatenate dense and sparse features
-            (batch_size, d) = x.shape
-            T = torch.cat([x] + ly, dim=1).view((batch_size, -1, d))
-            # perform a dot product
-            Z = torch.bmm(T, torch.transpose(T, 1, 2))
-            # append dense feature with the interactions (into a row vector)
-            # approach 1: all
-            # Zflat = Z.view((batch_size, -1))
-            # approach 2: unique
-            _, ni, nj = Z.shape
-            # approach 1: tril_indices
-            # offset = 0 if self.arch_interaction_itself else -1
-            # li, lj = torch.tril_indices(ni, nj, offset=offset)
-            # approach 2: custom
-            offset = 1 if self.arch_interaction_itself else 0
-            li = torch.tensor([i for i in range(ni) for j in range(i + offset)])
-            lj = torch.tensor([j for i in range(nj) for j in range(i + offset)])
-            Zflat = Z[:, li, lj]
-            # concatenate dense features and interactions
-            R = torch.cat([x] + [Zflat], dim=1)
+            if self.bf16:
+                T = [x] + ly
+                R = ipex.interaction(*T)
+            else:
+                # concatenate dense and sparse features
+                (batch_size, d) = x.shape
+                T = torch.cat([x] + ly, dim=1).view((batch_size, -1, d))
+                # perform a dot product
+                Z = torch.bmm(T, torch.transpose(T, 1, 2))
+                # append dense feature with the interactions (into a row vector)
+                # approach 1: all
+                # Zflat = Z.view((batch_size, -1))
+                # approach 2: unique
+                _, ni, nj = Z.shape
+                # approach 1: tril_indices
+                # offset = 0 if self.arch_interaction_itself else -1
+                # li, lj = torch.tril_indices(ni, nj, offset=offset)
+                # approach 2: custom
+                offset = 1 if self.arch_interaction_itself else 0
+                li = torch.tensor([i for i in range(ni) for j in range(i + offset)])
+                lj = torch.tensor([j for i in range(nj) for j in range(i + offset)])
+                Zflat = Z[:, li, lj]
+                # concatenate dense features and interactions
+                R = torch.cat([x] + [Zflat], dim=1)
         elif self.arch_interaction_op == "cat":
             # concatenation features (into a row vector)
             R = torch.cat([x] + ly, dim=1)
@@ -302,6 +338,8 @@ class DLRM_Net(nn.Module):
 
     def sequential_forward(self, dense_x, lS_o, lS_i):
         # process dense features (using bottom mlp), resulting in a row vector
+        if self.bf16:
+            dense_x = dense_x.bfloat16()
         x = self.apply_mlp(dense_x, self.bot_l)
         # debug prints
         # print("intermediate")
@@ -519,8 +557,14 @@ if __name__ == "__main__":
     parser.add_argument("--mlperf-auc-threshold", type=float, default=0.0)
     parser.add_argument("--mlperf-bin-loader", action='store_true', default=False)
     parser.add_argument("--mlperf-bin-shuffle", action='store_true', default=False)
+    # bf16 option
+    parser.add_argument("--bf16", action='store_true', default=False)
+    # ipex option
+    parser.add_argument("--use-ipex", action="store_true", default=False)
     args = parser.parse_args()
-
+    if args.use_ipex:
+        import _torch_ipex
+        _torch_ipex.enable_auto_dnnl()
     if args.mlperf_logging:
         print('command line args: ', json.dumps(vars(args)))
 
@@ -538,12 +582,16 @@ if __name__ == "__main__":
         args.test_num_workers = args.num_workers
 
     use_gpu = args.use_gpu and torch.cuda.is_available()
+    use_ipex = args.use_ipex
     if use_gpu:
         torch.cuda.manual_seed_all(args.numpy_rand_seed)
         torch.backends.cudnn.deterministic = True
         device = torch.device("cuda", 0)
         ngpus = torch.cuda.device_count()  # 1
         print("Using {} GPU(s)...".format(ngpus))
+    elif use_ipex:
+        device = torch.device('dpcpp')
+        print("Using IPEX...")
     else:
         device = torch.device("cpu")
         print("Using CPU...")
@@ -722,6 +770,8 @@ if __name__ == "__main__":
         qr_threshold=args.qr_threshold,
         md_flag=args.md_flag,
         md_threshold=args.md_threshold,
+        bf16=args.bf16,
+        use_ipex=args.use_ipex,
     )
     # test prints
     if args.debug_mode:
@@ -730,7 +780,9 @@ if __name__ == "__main__":
             print(param.detach().cpu().numpy())
         # print(dlrm)
 
-    if use_gpu:
+    if use_ipex:
+        dlrm = dlrm.to(device)
+    elif use_gpu:
         # Custom Model-Data Parallel
         # the mlps are replicated and use data parallelism, while
         # the embeddings are distributed and use model parallelism
@@ -751,7 +803,10 @@ if __name__ == "__main__":
 
     if not args.inference_only:
         # specify the optimizer algorithm
-        optimizer = torch.optim.SGD(dlrm.parameters(), lr=args.learning_rate)
+        if args.bf16 and ipex.is_available():
+            optimizer = ipex.SplitSGD(dlrm.parameters(), lr=args.learning_rate)
+        else:
+            optimizer = torch.optim.SGD(dlrm.parameters(), lr=args.learning_rate)
 
     ### main loop ###
     def time_wrap(use_gpu):
@@ -759,7 +814,7 @@ if __name__ == "__main__":
             torch.cuda.synchronize()
         return time.time()
 
-    def dlrm_wrap(X, lS_o, lS_i, use_gpu, device):
+    def dlrm_wrap(X, lS_o, lS_i, use_gpu, use_ipex, device):
         if use_gpu:  # .cuda()
             # lS_i can be either a list of tensors or a stacked tensor.
             # Handle each case below:
@@ -772,13 +827,25 @@ if __name__ == "__main__":
                 lS_o,
                 lS_i
             )
+        elif use_ipex:
+            lS_i = [S_i.to(device) for S_i in lS_i] if isinstance(lS_i, list) \
+                else lS_i.to(device)
+            lS_o = [S_o.to(device) for S_o in lS_o] if isinstance(lS_o, list) \
+                else lS_o.to(device)
+            return dlrm(
+                X.to(device),
+                lS_o,
+                lS_i
+            )
         else:
             return dlrm(X, lS_o, lS_i)
 
-    def loss_fn_wrap(Z, T, use_gpu, device):
+    def loss_fn_wrap(Z, T, use_gpu, use_ipex, device):
         if args.loss_function == "mse" or args.loss_function == "bce":
             if use_gpu:
                 return loss_fn(Z, T.to(device))
+            elif use_ipex:
+                return loss_fn(Z, T.to(device))
             else:
                 return loss_fn(Z, T)
         elif args.loss_function == "wbce":
@@ -904,10 +971,10 @@ if __name__ == "__main__":
                 '''
 
                 # forward pass
-                Z = dlrm_wrap(X, lS_o, lS_i, use_gpu, device)
+                Z = dlrm_wrap(X, lS_o, lS_i, use_gpu, use_ipex, device)
 
                 # loss
-                E = loss_fn_wrap(Z, T, use_gpu, device)
+                E = loss_fn_wrap(Z, T, use_gpu, use_ipex, device)
                 '''
                 # debug prints
                 print("output and loss")
@@ -1000,7 +1067,7 @@ if __name__ == "__main__":
 
                         # forward pass
                         Z_test = dlrm_wrap(
-                            X_test, lS_o_test, lS_i_test, use_gpu, device
+                            X_test, lS_o_test, lS_i_test, use_gpu, use_ipex, device
                         )
                         if args.mlperf_logging:
                             S_test = Z_test.detach().cpu().numpy()  # numpy array
@@ -1009,7 +1076,7 @@ if __name__ == "__main__":
                             targets.append(T_test)
                         else:
                             # loss
-                            E_test = loss_fn_wrap(Z_test, T_test, use_gpu, device)
+                            E_test = loss_fn_wrap(Z_test, T_test, use_gpu, use_ipex, device)
 
                             # compute loss and accuracy
                             L_test = E_test.detach().cpu().numpy()  # numpy array
