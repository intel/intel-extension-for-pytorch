diff --git a/imagenet/main.py b/imagenet/main.py
index 169d127..70504de 100644
--- a/imagenet/main.py
+++ b/imagenet/main.py
@@ -73,13 +73,34 @@ parser.add_argument('--multiprocessing-distributed', action='store_true',
                          'N processes per node, which has N GPUs. This is the '
                          'fastest way to use PyTorch for either single node or '
                          'multi node data parallel training')
-
+parser.add_argument('--ipex', action='store_true', default=False,
+                    help='enable Intel_PyTorch_Extension')
+parser.add_argument('--dnnl', action='store_true', default=False,
+                    help='enable Intel_PyTorch_Extension auto dnnl path')
+parser.add_argument('--jit', action='store_true', default=False,
+                    help='enable Intel_PyTorch_Extension JIT path')
+parser.add_argument('--mix-precision', action='store_true', default=False,
+                    help='enable ipex mix precision')
+parser.add_argument('--checkpoint-dir', default='', type=str, metavar='PATH',
+                    help='path to user checkpoint (default: none), just for user defined model')
 best_acc1 = 0
 
 
 def main():
     args = parser.parse_args()
 
+    if args.ipex:
+        import intel_pytorch_extension as ipex
+        if args.dnnl:
+            ipex.core.enable_auto_dnnl()
+        else:
+            ipex.core.disable_auto_dnnl()
+        if args.mix_precision:
+            ipex.enable_auto_optimization(mixed_dtype=torch.bfloat16, train=not args.evaluate)
+        # jit path only enabled for inference
+        if args.jit and args.evaluate:
+            ipex.core.enable_jit_opt()
+
     if args.seed is not None:
         random.seed(args.seed)
         torch.manual_seed(args.seed)
@@ -131,6 +152,8 @@ def main_worker(gpu, ngpus_per_node, args):
     # create model
     if args.pretrained:
         print("=> using pre-trained model '{}'".format(args.arch))
+        if args.arch == "resnext101_32x4d":
+            checkpoint = torch.load(args.checkpoint_dir)
         model = models.__dict__[args.arch](pretrained=True)
     else:
         print("=> creating model '{}'".format(args.arch))
@@ -138,6 +161,8 @@ def main_worker(gpu, ngpus_per_node, args):
 
     if not torch.cuda.is_available():
         print('using CPU, this will be slow')
+        if args.ipex:
+            model = model.to(device = 'dpcpp:0')
     elif args.distributed:
         # For multiprocessing distributed, DistributedDataParallel constructor
         # should always set the single device scope, otherwise,
@@ -233,7 +258,11 @@ def main_worker(gpu, ngpus_per_node, args):
         num_workers=args.workers, pin_memory=True)
 
     if args.evaluate:
-        validate(val_loader, model, criterion, args)
+        if args.ipex and args.jit:
+            scripted_model = torch.jit.script(model.eval())
+            validate(val_loader, scripted_model, criterion, args)
+        else:
+            validate(val_loader, model, criterion, args)
         return
 
     for epoch in range(args.start_epoch, args.epochs):
@@ -286,6 +315,10 @@ def train(train_loader, model, criterion, optimizer, epoch, args):
         if torch.cuda.is_available():
             target = target.cuda(args.gpu, non_blocking=True)
 
+        if args.ipex:
+            images = images.to(device = 'dpcpp:0')
+            target = target.to(device = 'dpcpp:0')
+
         # compute output
         output = model(images)
         loss = criterion(output, target)
@@ -330,6 +363,10 @@ def validate(val_loader, model, criterion, args):
             if torch.cuda.is_available():
                 target = target.cuda(args.gpu, non_blocking=True)
 
+            if args.ipex:
+                images = images.to(device = 'dpcpp:0')
+                target = target.to(device = 'dpcpp:0')
+
             # compute output
             output = model(images)
             loss = criterion(output, target)
