diff --git a/imagenet/main.py b/imagenet/main.py
index 169d127..e2aed97 100644
--- a/imagenet/main.py
+++ b/imagenet/main.py
@@ -73,13 +73,34 @@ parser.add_argument('--multiprocessing-distributed', action='store_true',
                          'N processes per node, which has N GPUs. This is the '
                          'fastest way to use PyTorch for either single node or '
                          'multi node data parallel training')
-
+parser.add_argument('--ipex', action='store_true', default=False,
+                    help='enable Intel_PyTorch_Extension')
+parser.add_argument('--dnnl', action='store_true', default=False,
+                    help='enable Intel_PyTorch_Extension auto dnnl path')
+parser.add_argument('--jit', action='store_true', default=False,
+                    help='enable Intel_PyTorch_Extension JIT path')
+parser.add_argument('--mix-precision', action='store_true', default=False,
+                    help='enable ipex mix precision')
+parser.add_argument('--checkpoint-dir', default='', type=str, metavar='PATH',
+                    help='path to user checkpoint (default: none), just for user defined model')
 best_acc1 = 0
 
 
 def main():
     args = parser.parse_args()
 
+    if args.ipex:
+        import intel_pytorch_extension as ipex
+        if args.dnnl:
+            ipex.core.enable_auto_dnnl()
+        else:
+            ipex.core.disable_auto_dnnl()
+        if args.mix_precision:
+            ipex.enable_auto_mixed_precision(mixed_dtype=torch.bfloat16, train=not args.evaluate)
+        # jit path only enabled for inference
+        if args.jit and args.evaluate:
+            ipex.core.enable_jit_opt()
+
     if args.seed is not None:
         random.seed(args.seed)
         torch.manual_seed(args.seed)
@@ -131,6 +152,8 @@ def main_worker(gpu, ngpus_per_node, args):
     # create model
     if args.pretrained:
         print("=> using pre-trained model '{}'".format(args.arch))
+        if args.arch == "resnext101_32x4d":
+            checkpoint = torch.load(args.checkpoint_dir)
         model = models.__dict__[args.arch](pretrained=True)
     else:
         print("=> creating model '{}'".format(args.arch))
@@ -138,6 +161,8 @@ def main_worker(gpu, ngpus_per_node, args):
 
     if not torch.cuda.is_available():
         print('using CPU, this will be slow')
+        if args.ipex:
+            model = model.to(ipex.DEVICE)
     elif args.distributed:
         # For multiprocessing distributed, DistributedDataParallel constructor
         # should always set the single device scope, otherwise,
@@ -233,7 +258,11 @@ def main_worker(gpu, ngpus_per_node, args):
         num_workers=args.workers, pin_memory=True)
 
     if args.evaluate:
-        validate(val_loader, model, criterion, args)
+        if args.ipex and args.jit:
+            scripted_model = torch.jit.script(model.eval())
+            validate(val_loader, scripted_model, criterion, args)
+        else:
+            validate(val_loader, model, criterion, args)
         return
 
     for epoch in range(args.start_epoch, args.epochs):
@@ -286,6 +315,10 @@ def train(train_loader, model, criterion, optimizer, epoch, args):
         if torch.cuda.is_available():
             target = target.cuda(args.gpu, non_blocking=True)
 
+        if args.ipex:
+            images = images.to(ipex.DEVICE)
+            target = target.to(ipex.DEVICE)
+
         # compute output
         output = model(images)
         loss = criterion(output, target)
@@ -330,6 +363,10 @@ def validate(val_loader, model, criterion, args):
             if torch.cuda.is_available():
                 target = target.cuda(args.gpu, non_blocking=True)
 
+            if args.ipex:
+                images = images.to(ipex.DEVICE)
+                target = target.to(ipex.DEVICE)
+
             # compute output
             output = model(images)
             loss = criterion(output, target)
diff --git a/imagenet/run_inference_cpu_latency.sh b/imagenet/run_inference_cpu_latency.sh
new file mode 100755
index 0000000..11220a3
--- /dev/null
+++ b/imagenet/run_inference_cpu_latency.sh
@@ -0,0 +1,80 @@
+#!/bin/sh
+
+######################################################################
+### How to run?
+### 1) install pytorch internal
+### 2) install torchvision: for benchmarking ResNext101_32x4d, follow this steps:
+###    1) git clone -b v0.5.0 https://github.com/pytorch/vision.git
+###    2) replace original resnet.py with this fold's resnet.py
+###    3) python setup.py install
+### 3) conda install jemalloc
+### 4) export LD_PRELOAD= "/YOUR_CONDA_PATH/envs/YOUR_CONDA_ENV/lib/libjemalloc.so
+###    /opt/intel/compilers_and_libraries/linux/lib/intel64/libiomp5.so"
+### 5) Test cpu lantancy(14 instance, 4 core/ins). Just run:
+###    bash run_inference_cpu_multi_instance_lantency_bf16.sh resnet50/resnext101_32x4d
+###
+##################################################################3#####
+
+export DNNL_PRIMITIVE_CACHE_CAPACITY=1024
+
+ARGS=""
+if [[ "$1" == "resnet50" ]]
+then
+    ARGS="$ARGS resnet50"
+    echo "### running resnet50 model"
+else
+    ARGS="$ARGS resnext101_32x4d"
+    echo "### running resnext101_32x4d model"
+fi
+
+
+CORES=`lscpu | grep Core | awk '{print $4}'`
+#SOCKETS=`lscpu | grep Socket | awk '{print $2}'`
+SOCKETS=1
+TOTAL_CORES=`expr $CORES`
+#`expr $CORES \* $SOCKETS`
+
+# change this number to adjust number of instances
+CORES_PER_INSTANCE=1
+
+KMP_SETTING="KMP_AFFINITY=granularity=fine,compact,1,0"
+
+BATCH_SIZE=1
+
+export OMP_NUM_THREADS=$CORES_PER_INSTANCE
+export $KMP_SETTING
+
+echo -e "### using OMP_NUM_THREADS=$CORES_PER_INSTANCE"
+echo -e "### using $KMP_SETTING\n\n"
+sleep 3
+
+INSTANCES=`expr $TOTAL_CORES / $CORES_PER_INSTANCE`
+LAST_INSTANCE=`expr $INSTANCES - 1`
+INSTANCES_PER_SOCKET=`expr $INSTANCES / $SOCKETS`
+for i in $(seq 1 $LAST_INSTANCE); do
+    numa_node_i=`echo |awk -v i=$i -v ins=$INSTANCES_PER_SOCKET '{printf("%d",i/ins)}'`
+    start_core_i=`expr $i \* $CORES_PER_INSTANCE`
+    end_core_i=`expr $start_core_i + $CORES_PER_INSTANCE - 1`
+    LOG_i=inference_cpu_bs${BATCH_SIZE}_ins${i}.txt
+
+    echo "### running on instance $i, numa node $numa_node_i, core list {$start_core_i, $end_core_i}..."
+    numactl --physcpubind=$start_core_i-$end_core_i --membind=$numa_node_i python -u main.py -e -a $ARGS \
+        -j 0 $DATASET_PATH -b $BATCH_SIZE 2>&1 | tee $LOG_i &
+done
+
+
+numa_node_0=0
+start_core_0=0
+end_core_0=`expr $CORES_PER_INSTANCE - 1`
+LOG_0=inference_cpu_bs${BATCH_SIZE}_ins0.txt
+
+echo "### running on instance 0, numa node $numa_node_0, core list {$start_core_0, $end_core_0}...\n\n"
+numactl --physcpubind=$start_core_0-$end_core_0 --membind=$numa_node_0 python -u main.py -e -a $ARGS \
+        -j 0 $DATASET_PATH -b $BATCH_SIZE 2>&1 | tee $LOG_0
+
+sleep 10
+echo -e "\n\n Sum sentences/s together:"
+for i in $(seq 0 $LAST_INSTANCE); do
+    log=inference_cpu_bs${BATCH_SIZE}_ins${i}.txt
+    tail -n 2 $log
+done
diff --git a/imagenet/run_inference_cpu_latency_ipex.sh b/imagenet/run_inference_cpu_latency_ipex.sh
new file mode 100755
index 0000000..4fe8571
--- /dev/null
+++ b/imagenet/run_inference_cpu_latency_ipex.sh
@@ -0,0 +1,100 @@
+#!/bin/sh
+
+######################################################################
+### How to run?
+### 1) install pytorch internal
+### 2) install torchvision: for benchmarking ResNext101_32x4d, follow this steps:
+###    1) git clone -b v0.5.0 https://github.com/pytorch/vision.git
+###    2) replace original resnet.py with this fold's resnet.py
+###    3) python setup.py install
+### 3) conda install jemalloc
+### 4) export LD_PRELOAD= "/YOUR_CONDA_PATH/envs/YOUR_CONDA_ENV/lib/libjemalloc.so
+###    /opt/intel/compilers_and_libraries/linux/lib/intel64/libiomp5.so"
+### 5) Test cpu lantancy(14 instance, 4 core/ins). Just run:
+###    bash run_inference_cpu_multi_instance_lantency_ipex.sh resnet50/resnext101_32x4d dataset_path dnnl bf16 jit
+###
+######################################################################
+
+export DNNL_PRIMITIVE_CACHE_CAPACITY=1024
+
+ARGS=""
+if [[ "$1" == "resnet50" ]]
+then
+    ARGS="$ARGS resnet50"
+    echo "### running resnet50 model"
+else
+    ARGS="$ARGS resnext101_32x4d"
+    echo "### running resnext101_32x4d model"
+fi
+
+ARGS="$ARGS $2"
+echo "### dataset path: $2"
+
+if [[ "$3" == "dnnl" ]]
+then
+    ARGS="$ARGS --dnnl"
+    echo "### running auto_dnnl mode"
+fi
+
+if [[ "$4" == "bf16" ]]
+then
+    ARGS="$ARGS --mix-precision"
+    echo "### running bf16 datatype"
+fi
+
+if [[ "$5" == "jit" ]]
+then
+    ARGS="$ARGS --jit"
+    echo "### running jit mode"
+fi
+
+CORES=`lscpu | grep Core | awk '{print $4}'`
+#SOCKETS=`lscpu | grep Socket | awk '{print $2}'`
+SOCKETS=1
+TOTAL_CORES=`expr $CORES`
+#`expr $CORES \* $SOCKETS`
+
+# change this number to adjust number of instances
+CORES_PER_INSTANCE=1
+
+KMP_SETTING="KMP_AFFINITY=granularity=fine,compact,1,0"
+
+BATCH_SIZE=1
+
+export OMP_NUM_THREADS=$CORES_PER_INSTANCE
+export $KMP_SETTING
+
+echo -e "### using OMP_NUM_THREADS=$CORES_PER_INSTANCE"
+echo -e "### using $KMP_SETTING\n\n"
+sleep 3
+
+INSTANCES=`expr $TOTAL_CORES / $CORES_PER_INSTANCE`
+LAST_INSTANCE=`expr $INSTANCES - 1`
+INSTANCES_PER_SOCKET=`expr $INSTANCES / $SOCKETS`
+for i in $(seq 1 $LAST_INSTANCE); do
+    numa_node_i=`echo |awk -v i=$i -v ins=$INSTANCES_PER_SOCKET '{printf("%d",i/ins)}'`
+    start_core_i=`expr $i \* $CORES_PER_INSTANCE`
+    end_core_i=`expr $start_core_i + $CORES_PER_INSTANCE - 1`
+    LOG_i=inference_cpu_bs${BATCH_SIZE}_ins${i}.txt
+
+    echo "### running on instance $i, numa node $numa_node_i, core list {$start_core_i, $end_core_i}..."
+    numactl --physcpubind=$start_core_i-$end_core_i --membind=$numa_node_i python -u main.py -e -a $ARGS \
+        --ipex -j 0 -b $BATCH_SIZE 2>&1 | tee $LOG_i &
+done
+
+
+numa_node_0=0
+start_core_0=0
+end_core_0=`expr $CORES_PER_INSTANCE - 1`
+LOG_0=inference_cpu_bs${BATCH_SIZE}_ins0.txt
+
+echo "### running on instance 0, numa node $numa_node_0, core list {$start_core_0, $end_core_0}...\n\n"
+numactl --physcpubind=$start_core_0-$end_core_0 --membind=$numa_node_0 python -u main.py -e -a $ARGS \
+        --ipex -j 0 -b $BATCH_SIZE 2>&1 | tee $LOG_0
+
+sleep 10
+echo -e "\n\n Sum sentences/s together:"
+for i in $(seq 0 $LAST_INSTANCE); do
+    log=inference_cpu_bs${BATCH_SIZE}_ins${i}.txt
+    tail -n 2 $log
+done
diff --git a/imagenet/run_training_cpu.sh b/imagenet/run_training_cpu.sh
new file mode 100755
index 0000000..e7714cc
--- /dev/null
+++ b/imagenet/run_training_cpu.sh
@@ -0,0 +1,43 @@
+#!/bin/sh
+
+###############################################################################
+### How to run?
+### 1) install pytorch internal
+### 2) install torchvision: for benchmarking ResNext101_32x4d, follow this steps:
+###    1) git clone -b v0.5.0 https://github.com/pytorch/vision.git
+###    2) replace original resnet.py with this fold's resnet.py
+###    3) python setup.py install
+### 3) conda install jemalloc
+### 4) export LD_PRELOAD= "/YOUR_CONDA_PATH/envs/YOUR_CONDA_ENV/lib/libjemalloc.so
+###    /opt/intel/compilers_and_libraries/linux/lib/intel64/libiomp5.so"
+### 5) bash run_training_cpu_ipex.sh  resnet50/resnext101_32x4d dataset_path dnnl bf16 jit
+###
+###############################################################################
+
+export DNNL_PRIMITIVE_CACHE_CAPACITY=1024
+
+ARGS=""
+if [[ "$1" == "resnet50" ]]
+then
+    ARGS="$ARGS resnet50"
+    echo "### running resnet50 model"
+else
+    ARGS="$ARGS resnext101_32x4d"
+    echo "### running resnext101_32x4d model"
+fi
+
+CORES=`lscpu | grep Core | awk '{print $4}'`
+SOCKETS=`lscpu | grep Socket | awk '{print $2}'`
+END_CORE=`expr $CORES - 1`
+
+KMP_SETTING="KMP_AFFINITY=granularity=fine,compact,1,0"
+
+BATCH_SIZE=128
+
+export OMP_NUM_THREADS=$CORES
+export $KMP_SETTING
+
+echo -e "### using OMP_NUM_THREADS=$CORES"
+echo -e "### using $KMP_SETTING\n\n"
+
+numactl --physcpubind=0-$END_CORE --membind=0 python -u main.py -a $ARGS $DATASET_PATH -j 0 -b $BATCH_SIZE --epochs 1 --seed 2020
diff --git a/imagenet/run_training_cpu_ipex.sh b/imagenet/run_training_cpu_ipex.sh
new file mode 100755
index 0000000..68b5e4a
--- /dev/null
+++ b/imagenet/run_training_cpu_ipex.sh
@@ -0,0 +1,58 @@
+#!/bin/sh
+
+###############################################################################
+### How to run?
+### 1) install pytorch internal
+### 2) install torchvision: for benchmarking ResNext101_32x4d, follow this steps:
+###    1) git clone -b v0.5.0 https://github.com/pytorch/vision.git
+###    2) replace original resnet.py with this fold's resnet.py
+###    3) python setup.py install
+### 3) conda install jemalloc
+### 4) export LD_PRELOAD= "/YOUR_CONDA_PATH/envs/YOUR_CONDA_ENV/lib/libjemalloc.so
+###    /opt/intel/compilers_and_libraries/linux/lib/intel64/libiomp5.so"
+### 5) bash run_training_cpu_ipex.sh  resnet50/resnext101_32x4d dataset_path dnnl bf16 jit
+###
+###############################################################################
+
+export DNNL_PRIMITIVE_CACHE_CAPACITY=1024
+
+ARGS=""
+if [[ "$1" == "resnet50" ]]
+then
+    ARGS="$ARGS resnet50"
+    echo "### running resnet50 model"
+else
+    ARGS="$ARGS resnext101_32x4d"
+    echo "### running resnext101_32x4d model"
+fi
+
+ARGS="$ARGS $2"
+echo "### dataset path: $2"
+
+if [[ "$3" == "dnnl" ]]
+then
+    ARGS="$ARGS --dnnl"
+    echo "### running auto_dnnl mode"
+fi
+
+if [[ "$4" == "bf16" ]]
+then
+    ARGS="$ARGS --mix-precision"
+    echo "### running bf16 datatype"
+fi
+
+CORES=`lscpu | grep Core | awk '{print $4}'`
+SOCKETS=`lscpu | grep Socket | awk '{print $2}'`
+END_CORE=`expr $CORES - 1`
+
+KMP_SETTING="KMP_AFFINITY=granularity=fine,compact,1,0"
+
+BATCH_SIZE=128
+
+export OMP_NUM_THREADS=$CORES
+export $KMP_SETTING
+
+echo -e "### using OMP_NUM_THREADS=$CORES"
+echo -e "### using $KMP_SETTING\n\n"
+
+numactl --physcpubind=0-$END_CORE --membind=0 python -u main.py -a $ARGS --ipex -j 0 -b $BATCH_SIZE --epochs 1 --seed 2020
