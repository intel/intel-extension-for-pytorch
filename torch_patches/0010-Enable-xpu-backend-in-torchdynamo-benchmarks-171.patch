From 58392a58c207ab03600c7abd6e7b3baa21262c3a Mon Sep 17 00:00:00 2001
From: "Wang, Chuanqi" <chuanqi.wang@intel.com>
Date: Thu, 12 Oct 2023 14:29:35 +0800
Subject: [PATCH 10/16] Enable xpu backend in torchdynamo benchmarks (#171)

---
 benchmarks/dynamo/common.py      | 29 ++++++++++++++++++++++-------
 benchmarks/dynamo/huggingface.py |  4 ++--
 benchmarks/dynamo/timm_models.py |  4 ++--
 benchmarks/dynamo/torchbench.py  |  4 ++--
 4 files changed, 28 insertions(+), 13 deletions(-)

diff --git a/benchmarks/dynamo/common.py b/benchmarks/dynamo/common.py
index 521618a9b35..e733f410525 100644
--- a/benchmarks/dynamo/common.py
+++ b/benchmarks/dynamo/common.py
@@ -1689,7 +1689,7 @@ class BenchmarkRunner:
     def __init__(self):
         self.model_iter_fn = None
         self.grad_scaler = DummyGradScaler()
-        self.autocast = contextlib.nullcontext
+        self.autocast = contextlib.nullcontext()
         self.optimizer = None
         self._args = None
 
@@ -1719,9 +1719,17 @@ class BenchmarkRunner:
             #  factor between eager and dynamo run, making accuracy check
             #  harder.
             # self.grad_scaler = torch.cuda.amp.GradScaler(init_scale=2.0)
-            self.autocast = torch.cuda.amp.autocast
+            self.autocast = torch.cuda.amp.autocast()
+        elif self.args.amp and self.args.devices == ["xpu"]:
+            amp_dtype = os.getenv("INDUCTOR_AMP_DT")
+            if amp_dtype in ["fp16", "float16", "FP16", "FLOAT16"]:
+                amp_dtype = torch.float16
+            else:
+                amp_dtype = torch.bfloat16
+            print("Test amp with dt:", amp_dtype)
+            self.autocast = torch.xpu.amp.autocast(dtype=amp_dtype)
         elif (self.args.bfloat16 or self.args.amp) and self.args.devices == ["cpu"]:
-            self.autocast = torch.cpu.amp.autocast
+            self.autocast = torch.cpu.amp.autocast()
 
     def init_optimizer(self, name, device, params):
         if device == "cuda" and self.args.training and name not in CI_SKIP_OPTIMIZER:
@@ -2083,7 +2091,7 @@ class BenchmarkRunner:
                         correct_rerun_result,
                         fp64_ref=None,
                         cos_similarity=False,
-                        tol=0,
+                        tol=0.00001,
                         equal_nan=self.equal_nan,
                     )
                 ):
@@ -2505,7 +2513,7 @@ def parse_args(args=None):
         help="ID of the benchmark suite partition to be run. Used to divide CI tasks",
     )
     parser.add_argument(
-        "--devices", "--device", "-d", action="append", help="cpu or cuda"
+        "--devices", "--device", "-d", action="append", help="cpu, xpu or cuda"
     )
     parser.add_argument("--device-index", help="CUDA device index")
     parser.add_argument(
@@ -3155,13 +3163,20 @@ def run(runner, args, original_dir=None):
             log.warning("torch.cuda.is_available() == False, using CPU")
             args.devices = ["cpu"]
 
-    if args.devices != ["cpu"] and torch.cuda.is_available():
+    if "xpu" in args.devices:
+        import intel_extension_for_pytorch as ipex
+        pass
+
+    if args.devices != ["cpu"] and (torch.cuda.is_available() or torch.xpu.is_available()):
         global synchronize
-        synchronize = torch.cuda.synchronize
+        synchronize = torch.cuda.synchronize if (torch.cuda.is_available()) else torch.xpu.synchronize
 
     if (
         args.devices == ["cuda"]
         and torch.cuda.get_device_properties(0).total_memory < 25 * 2**30
+    ) or (
+        args.devices == ["xpu"]
+        and torch.xpu.get_device_properties(0).total_memory < 25 * 2**30
     ):
         # OOM errors on an RTX 3090 with 24gb RAM
         runner.skip_models.update(
diff --git a/benchmarks/dynamo/huggingface.py b/benchmarks/dynamo/huggingface.py
index bd22cbff659..a95c34e023a 100755
--- a/benchmarks/dynamo/huggingface.py
+++ b/benchmarks/dynamo/huggingface.py
@@ -546,13 +546,13 @@ class HuggingfaceRunner(BenchmarkRunner):
         return pred[0]
 
     def forward_pass(self, mod, inputs, collect_outputs=True):
-        with self.autocast():
+        with self.autocast:
             return mod(**inputs)
 
     def forward_and_backward_pass(self, mod, inputs, collect_outputs=True):
         cloned_inputs = clone_inputs(inputs)
         self.optimizer_zero_grad(mod)
-        with self.autocast():
+        with self.autocast:
             pred = mod(**cloned_inputs)
             loss = self.compute_loss(pred)
         self.grad_scaler.scale(loss).backward()
diff --git a/benchmarks/dynamo/timm_models.py b/benchmarks/dynamo/timm_models.py
index 59fb18c70cc..76abb30e6a2 100755
--- a/benchmarks/dynamo/timm_models.py
+++ b/benchmarks/dynamo/timm_models.py
@@ -318,13 +318,13 @@ class TimmRunner(BenchmarkRunner):
         return reduce_to_scalar_loss(pred) / 1000.0
 
     def forward_pass(self, mod, inputs, collect_outputs=True):
-        with self.autocast():
+        with self.autocast:
             return mod(*inputs)
 
     def forward_and_backward_pass(self, mod, inputs, collect_outputs=True):
         cloned_inputs = clone_inputs(inputs)
         self.optimizer_zero_grad(mod)
-        with self.autocast():
+        with self.autocast:
             pred = mod(*cloned_inputs)
             if isinstance(pred, tuple):
                 pred = pred[0]
diff --git a/benchmarks/dynamo/torchbench.py b/benchmarks/dynamo/torchbench.py
index 8ffecb56aff..4c32e0667c7 100755
--- a/benchmarks/dynamo/torchbench.py
+++ b/benchmarks/dynamo/torchbench.py
@@ -454,13 +454,13 @@ class TorchBenchmarkRunner(BenchmarkRunner):
         return reduce_to_scalar_loss(pred)
 
     def forward_pass(self, mod, inputs, collect_outputs=True):
-        with self.autocast():
+        with self.autocast:
             return mod(*inputs)
 
     def forward_and_backward_pass(self, mod, inputs, collect_outputs=True):
         cloned_inputs = clone_inputs(inputs)
         self.optimizer_zero_grad(mod)
-        with self.autocast():
+        with self.autocast:
             pred = mod(*cloned_inputs)
             loss = self.compute_loss(pred)
         self.grad_scaler.scale(loss).backward()
-- 
2.34.1

