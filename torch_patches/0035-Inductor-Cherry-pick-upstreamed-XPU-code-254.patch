From 1f7dc28c2fe6ce2e1bf51dab69ec49f9071af60a Mon Sep 17 00:00:00 2001
From: Stonepia <tong.su@intel.com>
Date: Wed, 3 Jul 2024 09:41:56 +0800
Subject: [PATCH 35/38] [Inductor] Cherry pick upstreamed XPU code (#254)

* [dynamo] Delay cuda device registration (#122795)

the module-level `torch.cuda.device_count` calls are delayed until reading the registered devices.

Fixes #122085

Pull Request resolved: https://github.com/pytorch/pytorch/pull/122795
Approved by: https://github.com/ezyang

* [Inductor Intel GPU backend Upstream] Register general runtime device for Intel GPU (#121883)

Following the RFC https://github.com/pytorch/pytorch/issues/114856, Intel GPU Inductor backend uses device specific runtime API. To generalize this and reuse the existing generalize device interface, this PR registers the general device interface for Intel GPU.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/121883
Approved by: https://github.com/EikanWang, https://github.com/guangyey, https://github.com/jansel

* [Inductor Intel GPU backend Upstream] Add Inductor Intel GPU backend. (#121895)

As the design in RFC https://github.com/pytorch/pytorch/issues/114856, this PR implemented Intel GPU Inductor backend by:
- Reuse WrapperCodegen and TritonScheduling for python wrapper and kernel code generation. And implenented device-specific code generation in XPUDeviceOpOverrides
- Reuse fx_pass, lowering, codecache, triton kernel auto-tuning, and compilation.

For the test case, this PR provided test/inductor/test_xpu_basic.py for basic inductor backend functionality testing.
We'll reuse all the existing Inductor test case in the next PR.

Pull Request resolved: https://github.com/pytorch/pytorch/pull/121895
Approved by: https://github.com/EikanWang, https://github.com/jansel, https://github.com/desertfire

---------

Co-authored-by: Wenqi Li <wenqil@nvidia.com>
Co-authored-by: xinan.lin <xinan.lin@intel.com>
(cherry picked from commit 5e524886bd2f37340bfde9d2e991abf9810c1e42)
---
 torch/_dynamo/device_interface.py | 84 +++++++++++++++++++++++++++++--
 torch/_inductor/graph.py          |  5 ++
 2 files changed, 86 insertions(+), 3 deletions(-)

diff --git a/torch/_dynamo/device_interface.py b/torch/_dynamo/device_interface.py
index 4857b963881..9070222a0ee 100644
--- a/torch/_dynamo/device_interface.py
+++ b/torch/_dynamo/device_interface.py
@@ -171,7 +171,73 @@ class CudaInterface(DeviceInterface):
         return major * 10 + min
 
 
+get_xpu_stream: Optional[Callable[[int], int]]
+if torch.xpu._is_compiled():
+    from torch._C import _xpu_getCurrentRawStream as get_xpu_stream
+else:
+    get_xpu_stream = None
+
+
+class XpuInterface(DeviceInterface):
+    device = torch.xpu.device
+    Event = torch.xpu.Event
+    Stream = torch.xpu.Stream
+
+    class Worker:
+        @staticmethod
+        def set_device(device: int):
+            caching_worker_current_devices["xpu"] = device
+
+        @staticmethod
+        def current_device() -> int:
+            if "xpu" in caching_worker_current_devices:
+                return caching_worker_current_devices["xpu"]
+            return torch.xpu.current_device()
+
+        @staticmethod
+        def get_device_properties(device: _device_t = None):
+            if device is not None:
+                if isinstance(device, str):
+                    device = torch.device(device)
+                    assert device.type == "xpu"
+                if isinstance(device, torch.device):
+                    device = device.index
+            if device is None:
+                device = XpuInterface.Worker.current_device()
+
+            if "xpu" not in caching_worker_device_properties:
+                device_prop = [
+                    torch.xpu.get_device_properties(i)
+                    for i in range(torch.xpu.device_count())
+                ]
+                caching_worker_device_properties["xpu"] = device_prop
+
+            return caching_worker_device_properties["xpu"][device]
+
+    current_device = staticmethod(torch.xpu.current_device)
+    set_device = staticmethod(torch.xpu.set_device)
+    device_count = staticmethod(torch.xpu.device_count)
+    stream = staticmethod(torch.xpu.stream)  # type: ignore[assignment]
+    current_stream = staticmethod(torch.xpu.current_stream)
+    set_stream = staticmethod(torch.xpu.set_stream)  # type: ignore[assignment]
+    _set_stream_by_id = staticmethod(torch.xpu._set_stream_by_id)  # type: ignore[assignment]
+    synchronize = staticmethod(torch.xpu.synchronize)
+    get_device_properties = staticmethod(torch.xpu.get_device_properties)  # type: ignore[assignment]
+    get_raw_stream = staticmethod(get_xpu_stream)  # type: ignore[arg-type]
+
+    # Can be mock patched by @patch decorator.
+    @staticmethod
+    def is_available() -> bool:
+        return torch.xpu.is_available()
+
+    @staticmethod
+    def get_compute_capability(device: _device_t = None):
+        cc = torch.xpu.get_device_capability(device)
+        return cc
+
+
 device_interfaces: Dict[str, Type[DeviceInterface]] = {}
+_device_initialized = False
 
 
 def register_interface_for_device(
@@ -185,15 +251,27 @@ def register_interface_for_device(
 def get_interface_for_device(device: Union[str, torch.device]) -> Type[DeviceInterface]:
     if isinstance(device, torch.device):
         device = str(device)
+    if not _device_initialized:
+        init_device_reg()
     if device in device_interfaces:
         return device_interfaces[device]
     raise NotImplementedError(f"No interface for device {device}")
 
 
 def get_registered_device_interfaces() -> Iterable[Tuple[str, Type[DeviceInterface]]]:
+    if not _device_initialized:
+        init_device_reg()
     return device_interfaces.items()
 
 
-register_interface_for_device("cuda", CudaInterface)
-for i in range(torch.cuda.device_count()):
-    register_interface_for_device(f"cuda:{i}", CudaInterface)
+def init_device_reg():
+    global _device_initialized
+    register_interface_for_device("cuda", CudaInterface)
+    for i in range(torch.cuda.device_count()):
+        register_interface_for_device(f"cuda:{i}", CudaInterface)
+
+    register_interface_for_device("xpu", XpuInterface)
+    for i in range(torch.xpu.device_count()):
+        register_interface_for_device(f"xpu:{i}", XpuInterface)
+
+    _device_initialized = True
diff --git a/torch/_inductor/graph.py b/torch/_inductor/graph.py
index 62af6fb05e7..e0c191cdfe6 100644
--- a/torch/_inductor/graph.py
+++ b/torch/_inductor/graph.py
@@ -200,6 +200,11 @@ class GraphLowering(torch.fx.Interpreter):
 
             register_backend_for_device("xpu", TritonScheduling, WrapperCodeGen)
 
+        if get_scheduling_for_device("xpu") is None:
+            from .codegen.triton import TritonScheduling
+
+            register_backend_for_device("xpu", TritonScheduling, WrapperCodeGen)
+
     def __init__(
         self,
         gm: torch.fx.GraphModule,
-- 
2.34.1

