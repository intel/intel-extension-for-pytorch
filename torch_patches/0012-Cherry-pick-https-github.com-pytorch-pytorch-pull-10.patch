From 56978c02bd83816bc92f4dbc34c93e2f3169e5b7 Mon Sep 17 00:00:00 2001
From: zejun <zejun.chen@intel.com>
Date: Wed, 18 Oct 2023 15:19:50 +0800
Subject: [PATCH 12/19] Cherry-pick:
 https://github.com/pytorch/pytorch/pull/108312 (#175)

Signed-off-by: Chen, Zejun <zejun.chen@intel.com>
---
 test/dynamo/test_ctx_manager.py        |  99 ++++++++++++++++++--
 torch/_dynamo/device_interface.py      |  40 +++++++--
 torch/_dynamo/variables/__init__.py    |   4 +-
 torch/_dynamo/variables/builder.py     |  58 +++++++++---
 torch/_dynamo/variables/builtin.py     |   6 ++
 torch/_dynamo/variables/ctx_manager.py | 120 +++++++++++++++++++++----
 torch/_dynamo/variables/torch.py       |  26 +++---
 torch/_streambase.py                   |  45 ++++++++++
 torch/cuda/__init__.py                 |  16 +++-
 torch/cuda/streams.py                  |   6 +-
 10 files changed, 363 insertions(+), 57 deletions(-)
 create mode 100644 torch/_streambase.py

diff --git a/test/dynamo/test_ctx_manager.py b/test/dynamo/test_ctx_manager.py
index b05fea1c558..d06a45a1795 100644
--- a/test/dynamo/test_ctx_manager.py
+++ b/test/dynamo/test_ctx_manager.py
@@ -162,7 +162,7 @@ class CtxManagerTests(torch._dynamo.test_case.TestCase):
             x = torch.cos(x)
             return x
 
-        x = torch.randn((2, 2))
+        x = torch.randn((2, 2), device="cuda")
         ref = fn(x)
         cnts = torch._dynamo.testing.CompileCounter()
         opt_fn = torch._dynamo.optimize(cnts, nopython=True)(fn)
@@ -178,20 +178,105 @@ class CtxManagerTests(torch._dynamo.test_case.TestCase):
             x = torch.add(x, 2)
             with torch.cuda.stream(s):
                 x = torch.relu(x)
+
+            s1 = torch.cuda.current_stream()
+            with torch.cuda.stream(s1):
+                x = torch.relu(x)
+
+            s2 = torch.cuda.Stream()
+            with torch.cuda.stream(s2):
+                x = torch.relu(x)
+
             x = torch.add(x, 1)
             x = torch.cos(x)
             return x
 
-        x = torch.randn((2, 2))
+        x = torch.randn((2, 2), device="cuda")
         s = torch.cuda.Stream()
         ref = fn(x, s)
         cnts = torch._dynamo.testing.CompileCounter()
         opt_fn = torch._dynamo.optimize(cnts, nopython=True)(fn)
-        with self.assertRaisesRegex(
-            torch._dynamo.exc.Unsupported,
-            "CUDAStreamVariable does not currently work soundly.",
-        ):
-            res = opt_fn(x, s)
+        res = opt_fn(x, s)
+        self.assertTrue(same(ref, res))
+        self.assertEqual(cnts.frame_count, 1)
+        self.assertEqual(cnts.op_count, 18)
+
+    @unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
+    def test_cuda_stream_method(self):
+        def fn(x):
+            x = torch.mul(x, 1)
+            x = torch.add(x, 2)
+
+            new_stream = torch.cuda.Stream()
+            with torch.cuda.stream(new_stream):
+                x = torch.sin(x)
+                x = torch.add(x, 3)
+
+            cur_stream = torch.cuda.current_stream()
+            cur_stream.wait_stream(new_stream)
+
+            x = torch.add(x, 4)
+            is_idle = cur_stream.query()
+            cur_stream.synchronize()
+
+            with torch.cuda.stream(new_stream):
+                x = torch.add(x, 5)
+            new_stream.synchronize()
+
+            is_equal = cur_stream == new_stream
+
+            x = torch.relu(x)
+            x = torch.cos(x)
+            return x
+
+        x = torch.randn((2, 2), device="cuda")
+        ref = fn(x)
+        cnts = torch._dynamo.testing.CompileCounter()
+        opt_fn = torch._dynamo.optimize(cnts, nopython=True)(fn)
+        res = opt_fn(x)
+        self.assertTrue(same(ref, res))
+        self.assertEqual(cnts.frame_count, 1)
+        self.assertEqual(cnts.op_count, 20)
+
+    @unittest.skipIf(not torch.cuda.is_available(), "requires cuda")
+    def test_cuda_event_method(self):
+        def fn(x):
+            x = torch.mul(x, 1)
+            x = torch.add(x, 2)
+
+            cur_stream = torch.cuda.current_stream()
+            new_stream = torch.cuda.Stream()
+
+            x = torch.add(x, 3)
+
+            event = cur_stream.record_event()
+            is_idle = event.query()
+
+            new_stream.wait_event(event)
+            with torch.cuda.stream(new_stream):
+                x = torch.add(x, 4)
+
+            new_event = torch.cuda.Event()
+            new_event.record(new_stream)
+
+            x = torch.add(x, 5)
+            new_event.wait(cur_stream)
+
+            # use new event to sync
+            new_event.synchronize()
+
+            x = torch.relu(x)
+            x = torch.cos(x)
+            return x
+
+        x = torch.randn((2, 2), device="cuda")
+        ref = fn(x)
+        cnts = torch._dynamo.testing.CompileCounter()
+        opt_fn = torch._dynamo.optimize(cnts, nopython=True)(fn)
+        res = opt_fn(x)
+        self.assertTrue(same(ref, res))
+        self.assertEqual(cnts.frame_count, 1)
+        self.assertEqual(cnts.op_count, 19)
 
     def test_autograd_profiler_enabled(self):
         def fn(x):
diff --git a/torch/_dynamo/device_interface.py b/torch/_dynamo/device_interface.py
index 8d19cb92d51..184637b56aa 100644
--- a/torch/_dynamo/device_interface.py
+++ b/torch/_dynamo/device_interface.py
@@ -1,6 +1,8 @@
+import inspect
 from typing import Any, Dict, Union
 
 import torch
+from torch._streambase import _EventBase, _StreamBase
 
 if torch.cuda._is_compiled():
     from torch._C import _cuda_getCurrentRawStream as get_cuda_stream
@@ -14,16 +16,26 @@ caching_worker_device_properties: Dict[str, Any] = {}
 caching_worker_current_devices: Dict[str, int] = {}
 
 
-class DeviceInterface:
+class DeviceInterfaceMeta(type):
+    def __new__(metacls, *args, **kwargs):
+        class_member = args[2]
+        if "Event" in class_member:
+            assert inspect.isclass(class_member["Event"]) and issubclass(
+                class_member["Event"], _EventBase
+            ), "DeviceInterface member Event should be inherit from _EventBase"
+        if "Stream" in class_member:
+            assert inspect.isclass(class_member["Stream"]) and issubclass(
+                class_member["Stream"], _StreamBase
+            ), "DeviceInterface member Stream should be inherit from _StreamBase"
+        return super().__new__(metacls, *args, **kwargs)
+
+
+class DeviceInterface(metaclass=DeviceInterfaceMeta):
     """
     This is a simple device runtime interface for Inductor. It enables custom
     backends to be integrated with Inductor in a device-agnostic semantic.
     """
 
-    class Event:
-        def __new__(cls, *args, **kwargs):
-            raise NotImplementedError()
-
     class device:
         def __new__(cls, device: _device_t):
             raise NotImplementedError()
@@ -64,6 +76,10 @@ class DeviceInterface:
     def is_available() -> bool:
         raise NotImplementedError()
 
+    @staticmethod
+    def stream(stream: torch.Stream):
+        raise NotImplementedError()
+
     @staticmethod
     def current_stream():
         raise NotImplementedError()
@@ -72,6 +88,10 @@ class DeviceInterface:
     def set_stream(stream: torch.Stream):
         raise NotImplementedError()
 
+    @staticmethod
+    def _set_stream_by_id(stream_id: int, device_index: int, device_type: int):
+        raise NotImplementedError()
+
     @staticmethod
     def get_raw_stream():
         raise NotImplementedError()
@@ -90,9 +110,13 @@ class DeviceInterface:
 
 
 class CudaInterface(DeviceInterface):
-    Event = torch.cuda.Event
     device = torch.cuda.device
 
+    # register Event and Stream class into the backend interface
+    # make sure Event and Stream are implemented and inherited from the _EventBase and _StreamBase
+    Event = torch.cuda.Event
+    Stream = torch.cuda.Stream
+
     class Worker:
         @staticmethod
         def set_device(device: int):
@@ -127,8 +151,10 @@ class CudaInterface(DeviceInterface):
     current_device = staticmethod(torch.cuda.current_device)
     set_device = staticmethod(torch.cuda.set_device)
     device_count = staticmethod(torch.cuda.device_count)
+    stream = staticmethod(torch.cuda.stream)
     current_stream = staticmethod(torch.cuda.current_stream)
     set_stream = staticmethod(torch.cuda.set_stream)
+    _set_stream_by_id = staticmethod(torch.cuda._set_stream_by_id)
     synchronize = staticmethod(torch.cuda.synchronize)
     get_device_properties = staticmethod(torch.cuda.get_device_properties)
     get_raw_stream = staticmethod(get_cuda_stream)
@@ -159,4 +185,4 @@ def get_registered_device_interfaces():
     return device_interfaces.items()
 
 
-register_interface_for_device("cuda", CudaInterface)
+register_interface_for_device("cuda", CudaInterface)
\ No newline at end of file
diff --git a/torch/_dynamo/variables/__init__.py b/torch/_dynamo/variables/__init__.py
index b4b09e6110c..193ff30f90e 100644
--- a/torch/_dynamo/variables/__init__.py
+++ b/torch/_dynamo/variables/__init__.py
@@ -3,11 +3,11 @@ from .builtin import BuiltinVariable
 from .constant import ConstantVariable, EnumVariable
 from .ctx_manager import (
     ContextWrappingVariable,
-    CUDAStreamContextVariable,
-    CUDAStreamVariable,
     DeterministicAlgorithmsVariable,
     DisabledSavedTensorsHooksVariable,
     GradModeVariable,
+    StreamContextVariable,
+    StreamVariable,
     WithExitFunctionVariable,
 )
 from .dicts import ConstDictVariable, DataClassVariable, DefaultDictVariable
diff --git a/torch/_dynamo/variables/builder.py b/torch/_dynamo/variables/builder.py
index 3dfc8545c20..0fa92d3e63d 100644
--- a/torch/_dynamo/variables/builder.py
+++ b/torch/_dynamo/variables/builder.py
@@ -21,6 +21,7 @@ import torch
 from torch import SymInt
 from torch._guards import GuardSource, TracingContext
 from torch._ops import HigherOrderOperator
+from torch._streambase import _EventBase, _StreamBase
 from torch._subclasses.fake_tensor import FakeTensor, is_fake
 from torch.fx.experimental.symbolic_shapes import (
     DimConstraint,
@@ -30,7 +31,6 @@ from torch.fx.experimental.symbolic_shapes import (
 from torch.fx.immutable_collections import immutable_list
 from torch.utils._python_dispatch import is_traceable_wrapper_subclass
 from torch.utils.weak import TensorWeakRef, WeakIdRef
-
 from .. import config, mutation_guard, replay_record, skipfiles
 from ..allowed_functions import (
     is_allowed,
@@ -38,6 +38,8 @@ from ..allowed_functions import (
     is_numpy,
     is_user_defined_allowed,
 )
+
+from ..device_interface import device_interfaces
 from ..exc import unimplemented
 from ..guards import GuardBuilder, make_dupe_guard
 from ..side_effects import SideEffects
@@ -76,7 +78,7 @@ from ..utils import (
 from .base import MutableLocal, typestr, VariableTracker
 from .builtin import BuiltinVariable
 from .constant import ConstantVariable, EnumVariable
-from .ctx_manager import CUDAStreamVariable, NullContextVariable
+from .ctx_manager import EventVariable, NullContextVariable, StreamVariable
 from .dicts import (
     ConstDictVariable,
     DataClassVariable,
@@ -580,14 +582,21 @@ class VariableBuilder:
                 value,
                 guards=make_guards(GuardBuilder.FUNCTION_MATCH),
             )
-        elif isinstance(value, torch.cuda.streams.Stream):
-            unimplemented("CUDAStreamVariable does not currently work soundly.")
-            # return CUDAStreamVariable(
-            #     None,
-            #     value,
-            #     source=self.source,
-            #     guards=self.make_guards(GuardBuilder.ID_MATCH),
-            # )
+        elif isinstance(value, _StreamBase):
+            return StreamVariable(
+                None,
+                value,
+                value.device.type,
+                source=self.source,
+                guards=make_guards(GuardBuilder.ID_MATCH),
+            )
+        elif isinstance(value, _EventBase):
+            return EventVariable(
+                None,
+                value,
+                source=self.source,
+                guards=make_guards(GuardBuilder.ID_MATCH),
+            )
         elif (
             isinstance(value, torch._C._TensorMeta)
             and value in config.traceable_tensor_subclasses
@@ -1393,9 +1402,34 @@ def wrap_fx_proxy_cls(
     elif isinstance(example_value, (torch.SymInt, torch.SymFloat, torch.SymBool)):
         proxy.node.meta["example_value"] = example_value
         return SymNodeVariable(proxy, example_value, **options)
-    elif proxy.node.target in [torch.cuda.streams.Stream, torch.cuda.current_stream]:
+    elif (
+        inspect.isclass(proxy.node.target)
+        and issubclass(proxy.node.target, _StreamBase)
+    ) or proxy.node.target in [
+        interface_elem.current_stream for interface_elem in device_interfaces.values()
+    ]:
+        proxy.node.meta["example_value"] = example_value
+        return StreamVariable(
+            proxy, example_value, example_value.device.type, **options
+        )
+    elif (
+        inspect.isclass(proxy.node.target) and issubclass(proxy.node.target, _EventBase)
+    ) or proxy.node.target in [
+        interface_elem.Event for interface_elem in device_interfaces.values()
+    ]:
+        proxy.node.meta["example_value"] = example_value
+        return EventVariable(proxy, example_value, **options)
+    elif proxy.node.target == "query" and proxy.node.op == "call_method":
+        proxy.node.meta["example_value"] = example_value
+        return ConstantVariable(example_value, **options)
+    elif (
+        example_value is not None
+        and isinstance(example_value, _EventBase)
+        and proxy.node.target == "record_event"
+        and proxy.node.op == "call_method"
+    ):
         proxy.node.meta["example_value"] = example_value
-        return CUDAStreamVariable(proxy, example_value, **options)
+        return EventVariable(proxy, example_value, **options)
     elif isinstance(example_value, int) and proxy.node.target in [
         torch.sym_int,
         getattr,
diff --git a/torch/_dynamo/variables/builtin.py b/torch/_dynamo/variables/builtin.py
index ba5b4769331..602ffb7cb9c 100644
--- a/torch/_dynamo/variables/builtin.py
+++ b/torch/_dynamo/variables/builtin.py
@@ -37,6 +37,7 @@ from ..utils import (
 )
 from .base import MutableLocal, typestr, VariableTracker
 from .constant import ConstantVariable, EnumVariable
+from .ctx_manager import EventVariable, StreamVariable
 from .dicts import ConstDictVariable
 from .lists import (
     BaseListVariable,
@@ -1394,6 +1395,11 @@ class BuiltinVariable(VariableTracker):
         ):
             return ConstantVariable(op(left.value, right.value))
 
+        if (
+            (isinstance(left, StreamVariable) and isinstance(right, StreamVariable))
+            or (isinstance(left, EventVariable) and isinstance(right, EventVariable))
+        ) and op is operator.eq:
+            return ConstantVariable(op(left.value, right.value))
         if op.__name__ == "is_":
             # If the two objects are of different type, we can safely return False
             if type(left) is not type(right):
diff --git a/torch/_dynamo/variables/ctx_manager.py b/torch/_dynamo/variables/ctx_manager.py
index d68f6410db9..284f7453960 100644
--- a/torch/_dynamo/variables/ctx_manager.py
+++ b/torch/_dynamo/variables/ctx_manager.py
@@ -6,6 +6,7 @@ from torch._guards import Guard
 
 from .. import variables
 from ..bytecode_transformation import create_call_function, create_instruction
+from ..device_interface import get_interface_for_device
 from ..exc import unimplemented
 from ..guards import GuardBuilder
 from ..source import AttrSource, DummyGlobalSource
@@ -356,69 +357,139 @@ class NullContextVariable(ContextWrappingVariable):
         return "nullcontext"
 
 
-class CUDAStreamContextVariable(ContextWrappingVariable):
+class StreamContextVariable(ContextWrappingVariable):
     @staticmethod
     def create(tx, target_value, **kwargs):
         from .builder import wrap_fx_proxy_cls
 
+        current_stream_method = get_interface_for_device(
+            target_value.device
+        ).current_stream
         current_stream = wrap_fx_proxy_cls(
-            CUDAStreamVariable,
+            StreamVariable,
             tx,
             tx.output.create_proxy(
                 "call_function",
-                torch.cuda.current_stream,
+                current_stream_method,
                 (None,),
                 {},
             ),
         )
-        return CUDAStreamContextVariable(
+        return StreamContextVariable(
             target_values=[target_value],
             initial_values=[current_stream],
+            device=target_value.device,
             **kwargs,
         )
 
-    def __init__(self, target_values, initial_values=None, **kwargs):
+    def __init__(self, target_values, device, initial_values=None, **kwargs):
         super().__init__(
             target_values=target_values, initial_values=initial_values, **kwargs
         )
+        self.device = device
+        self.set_stream = get_interface_for_device(self.device).set_stream
+        self.set_stream_id = get_interface_for_device(self.device)._set_stream_by_id
 
     def enter(self, tx):
-        # CUDA stream generated inside of traced function
+        # stream generated inside of traced function
         if self.target_values[0].as_proxy() is not None:
             tx.output.create_proxy(
                 "call_function",
-                torch.cuda.set_stream,
+                self.set_stream,
                 (self.target_values[0].as_proxy(),),
                 {},
             )
-        # CUDA stream passed from outside of traced function
+        # stream passed from outside of traced function
         else:
             stream = self.target_values[0].value
             tx.output.create_proxy(
                 "call_function",
-                torch._C._cuda_setStream,
+                self.set_stream_id,
                 (stream.stream_id, stream.device_index, stream.device_type),
                 {},
             )
-        torch.cuda.set_stream(self.target_values[0].value)
+        self.set_stream(self.target_values[0].value)
 
     def exit(self, tx, *args):
         tx.output.create_proxy(
             "call_function",
-            torch.cuda.set_stream,
+            self.set_stream,
             (self.initial_values[0].as_proxy(),),
             {},
         )
-        torch.cuda.set_stream(self.initial_values[0].value)
+        self.set_stream(self.initial_values[0].value)
 
     def module_name(self):
-        return "torch.cuda"
+        return "torch." + str(self.device)
 
     def fn_name(self):
         return "stream"
 
 
-class CUDAStreamVariable(VariableTracker):
+class StreamVariable(VariableTracker):
+    def __init__(self, proxy, value, device, **kwargs):
+        if proxy is not None and "example_value" in proxy.node.meta:
+            assert proxy.node.meta["example_value"] == value
+        assert (
+            value.device.type == device
+        ), "stream value is not equal to the passed device"
+        super().__init__(**kwargs)
+        self.proxy = proxy
+        self.value = value
+        self.device = device
+
+    def call_method(
+        self,
+        tx,
+        name,
+        args: "List[VariableTracker]",
+        kwargs: "Dict[str, VariableTracker]",
+    ) -> "VariableTracker":
+        assert hasattr(self.value, name), f"no stream method found named {name}"
+        assert name in [
+            "wait_stream",
+            "synchronize",
+            "query",
+            "record_event",
+            "wait_event",
+        ], f" unsupported stream method {name}"
+
+        from ..utils import proxy_args_kwargs
+        from .builder import wrap_fx_proxy_cls
+
+        if name in ("wait_stream", "synchronize", "wait_event"):
+            tx.output.create_proxy(
+                "call_method", name, *proxy_args_kwargs([self] + args, kwargs)
+            )
+            return variables.ConstantVariable(None)
+        elif name == "query":
+            options = VariableTracker.propagate(self, args, kwargs.values())
+            return wrap_fx_proxy_cls(
+                target_cls=variables.ConstantVariable,
+                tx=tx,
+                proxy=tx.output.create_proxy(
+                    "call_method", name, *proxy_args_kwargs([self] + args, kwargs)
+                ),
+                **options,
+            )
+        elif name == "record_event":
+            options = VariableTracker.propagate(self, args, kwargs.values())
+            return wrap_fx_proxy_cls(
+                target_cls=EventVariable,
+                tx=tx,
+                proxy=tx.output.create_proxy(
+                    "call_method", name, *proxy_args_kwargs([self] + args, kwargs)
+                ),
+                **options,
+            )
+        else:
+            unimplemented(self.device + " stream method " + name + " unsupported")
+
+    def as_proxy(self):
+        return self.proxy
+
+
+class EventVariable(VariableTracker):
     def __init__(self, proxy, value, **kwargs):
         if proxy is not None and "example_value" in proxy.node.meta:
             assert proxy.node.meta["example_value"] == value
@@ -433,7 +504,26 @@ class CUDAStreamVariable(VariableTracker):
         args: "List[VariableTracker]",
         kwargs: "Dict[str, VariableTracker]",
     ) -> "VariableTracker":
-        unimplemented("cuda stream")
+        from ..utils import proxy_args_kwargs
+        from .builder import wrap_fx_proxy_cls
+
+        if name in ("wait", "record", "synchronize"):
+            tx.output.create_proxy(
+                "call_method", name, *proxy_args_kwargs([self] + args, kwargs)
+            )
+            return variables.ConstantVariable(None)
+        elif name == "query":
+            options = VariableTracker.propagate(self, args, kwargs.values())
+            return wrap_fx_proxy_cls(
+                target_cls=variables.ConstantVariable,
+                tx=tx,
+                proxy=tx.output.create_proxy(
+                    "call_method", name, *proxy_args_kwargs([self] + args, kwargs)
+                ),
+                **options,
+            )
+        else:
+            unimplemented(f"event method {name} unsupported")
 
     def as_proxy(self):
         return self.proxy
diff --git a/torch/_dynamo/variables/torch.py b/torch/_dynamo/variables/torch.py
index 704153bc50f..3f74ed3335a 100644
--- a/torch/_dynamo/variables/torch.py
+++ b/torch/_dynamo/variables/torch.py
@@ -1,4 +1,5 @@
 import collections
+import inspect
 import logging
 
 import math
@@ -6,6 +7,8 @@ import re
 import types
 from typing import Dict, List
 
+from torch._streambase import _StreamBase
+
 try:
     import numpy as np
 except ModuleNotFoundError:
@@ -19,6 +22,7 @@ from torch._dynamo.variables import UserFunctionVariable
 
 from .. import config, variables
 from ..allowed_functions import torch_get_name
+from ..device_interface import device_interfaces
 from ..exc import unimplemented
 from ..source import GeneratorStateSource
 from ..utils import (
@@ -210,11 +214,11 @@ class TorchVariable(VariableTracker):
     ) -> "VariableTracker":
         from . import (
             ConstantVariable,
-            CUDAStreamContextVariable,
-            CUDAStreamVariable,
             DeterministicAlgorithmsVariable,
             DisabledSavedTensorsHooksVariable,
             GradModeVariable,
+            StreamContextVariable,
+            StreamVariable,
             SymNodeVariable,
             TensorVariable,
             UserDefinedObjectVariable,
@@ -337,19 +341,21 @@ class TorchVariable(VariableTracker):
         elif self.value is torch._C.DisableTorchFunctionSubclass:
             assert not (args or kwargs)
             return TorchFunctionDisableVariable.create(tx, **options)
-        elif self.value is torch.cuda.stream:
-            log.warning(
-                "torch.cuda.stream() not fully supported, streams may be ignored"
-            )
+        elif any(
+            self.value is method
+            for method in [
+                interface_elem.stream for interface_elem in device_interfaces.values()
+            ]
+        ):
             assert len(args) == 1
-            return CUDAStreamContextVariable.create(tx, args[0], **options)
-        elif self.value is torch.cuda.streams.Stream:
+            return StreamContextVariable.create(tx, args[0], **options)
+        elif inspect.isclass(self.value) and issubclass(self.value, _StreamBase):
             return wrap_fx_proxy_cls(
-                CUDAStreamVariable,
+                StreamVariable,
                 tx,
                 tx.output.create_proxy(
                     "call_function",
-                    torch.cuda.streams.Stream,
+                    self.value,
                     (),
                     {},
                 ),
diff --git a/torch/_streambase.py b/torch/_streambase.py
new file mode 100644
index 00000000000..689ea7d9b6a
--- /dev/null
+++ b/torch/_streambase.py
@@ -0,0 +1,45 @@
+from abc import ABC, abstractmethod
+
+
+class _StreamBase(ABC):
+    r"""Base stream class abstraction for multi backends Stream to herit from"""
+
+    @abstractmethod
+    def wait_event(self, event):
+        raise NotImplementedError()
+
+    @abstractmethod
+    def wait_stream(self, stream):
+        raise NotImplementedError()
+
+    @abstractmethod
+    def record_event(self, event=None):
+        raise NotImplementedError()
+
+    @abstractmethod
+    def query(self):
+        raise NotImplementedError()
+
+    @abstractmethod
+    def synchronize(self):
+        raise NotImplementedError()
+
+    @abstractmethod
+    def __eq__(self, stream):
+        raise NotImplementedError()
+
+
+class _EventBase(ABC):
+    r"""Base Event class abstraction for multi backends Event to herit from"""
+
+    @abstractmethod
+    def wait(self, stream=None):
+        raise NotImplementedError()
+
+    @abstractmethod
+    def query(self):
+        raise NotImplementedError()
+
+    @abstractmethod
+    def synchronize(self):
+        raise NotImplementedError()
\ No newline at end of file
diff --git a/torch/cuda/__init__.py b/torch/cuda/__init__.py
index 9849848ffec..40074bef6f8 100644
--- a/torch/cuda/__init__.py
+++ b/torch/cuda/__init__.py
@@ -534,6 +534,20 @@ def stream(stream: Optional["torch.cuda.Stream"]) -> StreamContext:
     return StreamContext(stream)
 
 
+def _set_stream_by_id(stream_id, device_index, device_type):
+    r"""set stream specified by the stream id, device index and
+        device type
+    Args: stream_id (int): stream id in stream pool
+          device_index (int): device index in topo
+          device_type (int): enum device type
+    """
+    torch._C._cuda_setStream(
+        stream_id=stream_id,
+        device_index=device_index,
+        device_type=device_type,
+    )
+
+
 def set_stream(stream: Stream):
     r"""Sets the current stream.This is a wrapper API to set the stream.
         Usage of this function is discouraged in favor of the ``stream``
@@ -545,7 +559,7 @@ def set_stream(stream: Stream):
     """
     if stream is None:
         return
-    torch._C._cuda_setStream(
+    _set_stream_by_id(
         stream_id=stream.stream_id,
         device_index=stream.device_index,
         device_type=stream.device_type,
diff --git a/torch/cuda/streams.py b/torch/cuda/streams.py
index 87269189e0d..d4b71b6ce65 100644
--- a/torch/cuda/streams.py
+++ b/torch/cuda/streams.py
@@ -1,7 +1,7 @@
 import ctypes
 
 import torch
-
+from torch._streambase import _EventBase, _StreamBase
 from ._utils import _dummy_type
 
 
@@ -11,7 +11,7 @@ if not hasattr(torch._C, "_CudaStreamBase"):
     torch._C.__dict__["_CudaEventBase"] = _dummy_type("_CudaEventBase")
 
 
-class Stream(torch._C._CudaStreamBase):
+class Stream(torch._C._CudaStreamBase, _StreamBase):
     r"""Wrapper around a CUDA stream.
 
     A CUDA stream is a linear sequence of execution that belongs to a specific
@@ -136,7 +136,7 @@ class ExternalStream(Stream):
             return super().__new__(cls, stream_ptr=stream_ptr, **kwargs)
 
 
-class Event(torch._C._CudaEventBase):
+class Event(torch._C._CudaEventBase, _EventBase):
     r"""Wrapper around a CUDA event.
 
     CUDA events are synchronization markers that can be used to monitor the
-- 
2.34.1

