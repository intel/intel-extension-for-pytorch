<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Split SGD &mdash; intel_extension_for_pytorch 1.11.200+cpu documentation</title>
      <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
      <link rel="stylesheet" href="../../_static/custom.css" type="text/css" />
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="../../_static/js/theme.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Optimizer Fusion" href="optimizer_fusion.html" />
    <link rel="prev" title="Graph Optimization" href="graph_optimization.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
            <a href="../../index.html" class="icon icon-home"> intel_extension_for_pytorch
          </a>
              <div class="version">
                <a href="../../../versions.html">1.11.200+cpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../features.html">Features</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../features.html#ease-of-use-python-api">Ease-of-use Python API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#channels-last">Channels Last</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#auto-mixed-precision-amp">Auto Mixed Precision (AMP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#graph-optimization">Graph Optimization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#operator-optimization">Operator Optimization</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../features.html#optimizer-optimization">Optimizer Optimization</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Split SGD</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#bfloat16">BFloat16</a></li>
<li class="toctree-l4"><a class="reference internal" href="#stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">Split SGD</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="optimizer_fusion.html">Optimizer Fusion</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#runtime-extension-experimental">Runtime Extension (Experimental)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#int8-quantization-experimental">INT8 Quantization (Experimental)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../installation.html">Installation Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance_tuning.html">Performance Tuning Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../blogs_publications.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">intel_extension_for_pytorch</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
          <li><a href="../features.html">Features</a> &raquo;</li>
      <li>Split SGD</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/features/split_sgd.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="split-sgd">
<h1>Split SGD<a class="headerlink" href="#split-sgd" title="Permalink to this headline"></a></h1>
<p>Not only optimizations for inference workloads are Intel’s focus, training workloads are also within Intel’s optimization scope. As part of it, optimizations for train optimizer functions are an important perspective. The optimizations as implemented as a mechanism called <strong>Split SGD</strong>, taking advantage of BFloat16 data type and operator fusion. Optimizer <strong>adagrad</strong>, <strong>lamb</strong> and <strong>sgd</strong> are supported.</p>
<section id="bfloat16">
<h2>BFloat16<a class="headerlink" href="#bfloat16" title="Permalink to this headline"></a></h2>
<p>The figure below shows definition of Float32 (top) and <a class="reference external" href="https://www.intel.com/content/www/us/en/developer/articles/technical/intel-deep-learning-boost-new-instruction-bfloat16.html">BFloat16</a> (bottom) data types. Comparing to Float32, BFloat16 is only half-long, and thus saves half memory. It is supported natively at instruction set level to boost deep learning workloads from the 3rd Generation of Xeon Scalable Processors. It is highly compatible to Float32, both have the same bit length for “sign” and “exponent” part. Though, BFloat16 only has 7-bit “mantissa” part while Float32 has 23 bits. This makes BFloat16 has the same capacity to represent “digit ranges” with that of Float32, but has shorter “precision” part.</p>
<a class="reference internal image-reference" href="https://user-images.githubusercontent.com/33838455/86600181-00f5c200-bfa0-11ea-93f0-95af3f0bff08.png"><img alt="Data types" class="align-center" src="https://user-images.githubusercontent.com/33838455/86600181-00f5c200-bfa0-11ea-93f0-95af3f0bff08.png" style="width: 1200px;" /></a>
<p>Advantage of BFloat16 is that it saves memory and reduces computation workload, but the less mantissa bits brings negative effects as well. Let’s use an “ADD” operation as an example to explain the disadvantage. To perform addition of 2 floating point numbers, we need to shift the mantissa part of them left or right to align their exponent parts. Since BFloat16 has shorter mantissa part, it is much easier than Float32 to lose its mantissa part after the shifting, and thus cause accuracy issue.</p>
<p>Let’s use the following two decimal numbers <strong>x</strong> and <strong>y</strong> as an example. We first do the calculation in a high precision data type (10 valid numbers after decimal point).</p>
<div class="math notranslate nohighlight">
\[\begin{split}x &amp;= 0.1234500000*10^{10} \\
y &amp;= 0.1234500000*10^{5} \\
x+y &amp;= 0.1234500000*10^{10} + 0.1234500000*10^{5} \\
    &amp;= 0.1234500000*10^{10} + 0.0000012345*10^{10} \\
        &amp; =0.1234512345*10^{10}\end{split}\]</div>
<p>This makes sense because after shifting <strong>y</strong> right by 5 digits, the fraction part is still there.</p>
<p>Then, let’s do the calculation in a low precision data type (5 valid numbers after decimal point)</p>
<div class="math notranslate nohighlight">
\[\begin{split}x &amp;= 0.12345*10^{10} \\
y &amp;= 0.12345*10^{5} \\
x+y &amp;= 0.12345*10^{10} + 0.12345*10^{5} \\
    &amp;= 0.12345*10^{10} + 0.00000*10^{10} \\
    &amp;= 0.12345*10^{10}\end{split}\]</div>
<p>Since the data type has only 5 digits for the fraction part, after shifting y by 5 digits, its fraction part is fully removed. This brings accuracy loss. This is a drawback of lower precision data types form their nature.</p>
</section>
<section id="stochastic-gradient-descent-sgd">
<h2>Stochastic Gradient Descent (SGD)<a class="headerlink" href="#stochastic-gradient-descent-sgd" title="Permalink to this headline"></a></h2>
<p>Basically, training involves 3 steps:</p>
<ol class="arabic simple">
<li><p>Forward propagation: Performance inference once and compare the results with ground truth to get loss number.</p></li>
<li><p>Backward propagation: Utilize chain rule to calculate gradients of parameters based on the loss number.</p></li>
<li><p>Parameter update: Update value of parameters by gradients along with calculated loss values.</p></li>
</ol>
<p>The training is actually a loop of these 3 steps in sequence untill the loss number meets requirements or after a determine timeout duration. The Stochastic Gradient Descent (SGD) is most widely used at the 3rd step to update parameter values. To make it easy to understand, the 3rd step is described as the following formula:</p>
<div class="math notranslate nohighlight">
\[W = W + α * gW\]</div>
<p>Where <span class="math notranslate nohighlight">\(W\)</span> denotes parameters to be updated. <span class="math notranslate nohighlight">\(gW\)</span> denotes gradient got during backward propagation and <span class="math notranslate nohighlight">\(α\)</span> denotes learning rate.</p>
</section>
<section id="id2">
<h2>Split SGD<a class="headerlink" href="#id2" title="Permalink to this headline"></a></h2>
<p>Since the addition applied in SGD is repeated again and again, according to the drawback that we mentioned before of low precision data types, if both the <span class="math notranslate nohighlight">\(W\)</span> and <span class="math notranslate nohighlight">\(gW\)</span> are stored in BFloat16 data type, we will most likely lose valid bits and make the training results not accurate. Using FP32 master parameters is a common practice of avoiding the round-off errors at parameter update step.
To keep FP32 master parameters, we have 3 design choices:
(1) Only save FP32 parameters: For this choice, we need introduce additional FP32-&gt;BF16 cast at each iter to get benefit from BF16 at forward and backward propagation steps.
(2) Save both FP32 and BF16 parameters: BF16 parameter are used at forward and backward propagation steps. And use FP32 master parameters at update steps. For this choice we introduce more memory footprint.
(3) “Split” choice: In order to get performance benefits with BFloat16 at forward and backward propagation steps, while avoiding increase the memory footprint, we propose the mechanism <strong>“Split SGD”</strong>.</p>
<p>The idea is to “split” a 32-bit floating point number into 2 parts:</p>
<ol class="arabic simple">
<li><p>Top half: First 16 bits can be viewed as exactly a BFloat16 number.</p></li>
<li><p>Bottom half: Last 16 bits are still kept to avoid accuracy loss.</p></li>
</ol>
<p>FP32 parameters are split into “Top half” and “Bottom half”. When performing forward and backward propagations, the Top halfs are used to benefit from Intel BFloat16 support. When performing paramter update with SGD, we concatenate the Top half and the Bottom half to recover the parameters back to FP32 and then perform regular SGD operations.</p>
<p>It is a common pratice to use FP32 for master parameters in order to avoid round-off errors with BF16 parameter update. <strong>SplitSGD</strong> is an optimization of storing FP32 master parameters with reduced memory footprint.</p>
<a class="reference internal image-reference" href="../../_images/split_sgd.png"><img alt="Split SGD" class="align-center" src="../../_images/split_sgd.png" style="width: 800px;" /></a>
<div class="line-block">
<div class="line"><br /></div>
</div>
<p>The following pseudo code illustrates the process of Split SGD.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">fp32_w</span> <span class="o">=</span> <span class="n">concat_fp32_from_bf16</span><span class="p">(</span><span class="n">bf16_w</span><span class="p">,</span> <span class="n">trail</span><span class="p">)</span>
<span class="n">fp32_gw</span> <span class="o">=</span> <span class="n">bf16_gw</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">fp32_w</span> <span class="o">+=</span> <span class="n">α</span><span class="o">*</span> <span class="n">fp32_gw</span> <span class="p">(</span><span class="n">sgd</span> <span class="n">step</span> <span class="n">without</span> <span class="n">weight_dacay</span><span class="p">,</span> <span class="n">momentum</span><span class="p">)</span>
<span class="n">bf16_w</span><span class="p">,</span> <span class="n">trail</span> <span class="o">=</span> <span class="n">split_bf16_from_fp32</span><span class="p">(</span><span class="n">fp32_w</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="graph_optimization.html" class="btn btn-neutral float-left" title="Graph Optimization" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="optimizer_fusion.html" class="btn btn-neutral float-right" title="Optimizer Fusion" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright Intel(R).</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>