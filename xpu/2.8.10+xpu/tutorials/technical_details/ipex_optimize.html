

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>ipex.optimize Frontend API &mdash; Intel&amp;#174 Extension for PyTorch* 2.8.10+xpu documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=a95c1af4" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-pytorch"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Releases" href="../releases.html" />
    <link rel="prev" title="Memory Management" href="memory_management.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel&#174 Extension for PyTorch*
          </a>
<div class="version">
            <a href="../../../../">2.8.10+xpu ▼</a>
            <p>Click link above to switch version</p>
          </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ABOUT</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm.html">Large Language Models (LLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../technical_details.html">Technical Details</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../technical_details.html#optimizer-optimization-gpu">Optimizer Optimization [GPU]</a></li>
<li class="toctree-l2"><a class="reference internal" href="../technical_details.html#ahead-of-time-compilation-aot-gpu">Ahead of Time Compilation (AOT) [GPU]</a></li>
<li class="toctree-l2"><a class="reference internal" href="../technical_details.html#memory-management-gpu">Memory Management [GPU]</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../technical_details.html#ipex-optimize-gpu"><code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> [GPU]</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#"><code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> Frontend API</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#automatic-channels-last">Automatic Channels Last</a></li>
<li class="toctree-l4"><a class="reference internal" href="#conv-bn-folding"><code class="docutils literal notranslate"><span class="pre">conv_bn_folding</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#linear-bn-folding"><code class="docutils literal notranslate"><span class="pre">linear_bn_folding</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#replace-dropout-with-identity"><code class="docutils literal notranslate"><span class="pre">replace_dropout_with_identity</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#split-master-weight-for-bf16"><code class="docutils literal notranslate"><span class="pre">split_master_weight_for_bf16</span></code></a></li>
<li class="toctree-l4"><a class="reference internal" href="#fuse-update-step">fuse_update_step</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../known_issues.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/blogs.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu&amp;version=v2.3.110%2bxpu">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CONTRIBUTING GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel&#174 Extension for PyTorch*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../technical_details.html">Technical Details</a></li>
      <li class="breadcrumb-item active"><code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> Frontend API</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/technical_details/ipex_optimize.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="ipex-optimize-frontend-api">
<h1><code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> Frontend API<a class="headerlink" href="#ipex-optimize-frontend-api" title="Link to this heading"></a></h1>
<p>The <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> API is designed to optimize PyTorch* modules (<code class="docutils literal notranslate"><span class="pre">nn.modules</span></code>) and specific optimizers within Python modules. Its optimization options for Intel® GPU device include:</p>
<ul class="simple">
<li><p>Automatic Channels Last</p></li>
<li><p>Fusing Convolutional Layers with Batch Normalization</p></li>
<li><p>Fusing Linear Layers with Batch Normalization</p></li>
<li><p>Replacing Dropout with Identity</p></li>
<li><p>Splitting Master Weights</p></li>
<li><p>Fusing Optimizer Update Step</p></li>
</ul>
<p>The original python modules will be replaced to optimized versions automatically during model execution, if <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> is called in the model running script.</p>
<p>The following sections provide detailed descriptions for each optimization flag supported by <strong>XPU</strong> models on Intel® GPU. For CPU-specific flags, please refer to the <a class="reference external" href="../api_doc.html#ipex.optimize">API Docs page</a>.</p>
<section id="automatic-channels-last">
<h2>Automatic Channels Last<a class="headerlink" href="#automatic-channels-last" title="Link to this heading"></a></h2>
<p>By default, <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> checks if current running GPU platform supports 2D Block Array Load or not. If it does, the <code class="docutils literal notranslate"><span class="pre">Conv*d</span></code> and <code class="docutils literal notranslate"><span class="pre">ConvTranspose*d</span></code> modules inside the model will be optimized for using channels last memory format. Use <code class="docutils literal notranslate"><span class="pre">ipex.enable_auto_channels_last</span></code> or <code class="docutils literal notranslate"><span class="pre">ipex.disable_auto_channels_last</span></code> before calling <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code> to enable or disable this feature manually.</p>
</section>
<section id="conv-bn-folding">
<h2><code class="docutils literal notranslate"><span class="pre">conv_bn_folding</span></code><a class="headerlink" href="#conv-bn-folding" title="Link to this heading"></a></h2>
<p>This flag is applicable for model inference. Intel® Extension for PyTorch* tries to match all connected <code class="docutils literal notranslate"><span class="pre">nn.Conv(1/2/3)d</span></code> and <code class="docutils literal notranslate"><span class="pre">nn.BatchNorm(1/2/3)d</span></code> layers with matching dimensions in the model and fuses them to improve performance. If the fusion fails, the optimization process will be ended and the model will be executed automatically in normal path.</p>
</section>
<section id="linear-bn-folding">
<h2><code class="docutils literal notranslate"><span class="pre">linear_bn_folding</span></code><a class="headerlink" href="#linear-bn-folding" title="Link to this heading"></a></h2>
<p>This flag is applicable for model inference. Intel® Extension for PyTorch* tries to match all connected <code class="docutils literal notranslate"><span class="pre">nn.Linear</span></code> and <code class="docutils literal notranslate"><span class="pre">nn.BatchNorm(1/2/3)d</span></code> layers in the model and fuse them to improve performance. If the fusion fails, the optimization process will be ended and the model will be executed automatically in normal path.</p>
</section>
<section id="replace-dropout-with-identity">
<h2><code class="docutils literal notranslate"><span class="pre">replace_dropout_with_identity</span></code><a class="headerlink" href="#replace-dropout-with-identity" title="Link to this heading"></a></h2>
<p>This flag is applicable for model inference. All instances of <code class="docutils literal notranslate"><span class="pre">torch.nn.Dropout</span></code> will be replaced with <code class="docutils literal notranslate"><span class="pre">torch.nn.Identity</span></code>. The <code class="docutils literal notranslate"><span class="pre">Identity</span></code> modules will be ignored during the static graph generation. This optimization could potentially create additional fusion opportunities for the generated graph.</p>
</section>
<section id="split-master-weight-for-bf16">
<h2><code class="docutils literal notranslate"><span class="pre">split_master_weight_for_bf16</span></code><a class="headerlink" href="#split-master-weight-for-bf16" title="Link to this heading"></a></h2>
<p>This flag is applicable for model training. The optimization will be enabled once the following requirements are met:</p>
<ul class="simple">
<li><p>When calling <code class="docutils literal notranslate"><span class="pre">ipex.optimize</span></code>, the <code class="docutils literal notranslate"><span class="pre">dtype</span></code> flag must be set to <code class="docutils literal notranslate"><span class="pre">torch.bfloat16</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">fuse_update_step</span></code> must be enabled.</p></li>
</ul>
<p>The optimization process is as follows:</p>
<ul class="simple">
<li><p>Wrap all parameters of this model with <code class="docutils literal notranslate"><span class="pre">ParameterWrapper</span></code>.</p></li>
<li><p>Convert the parameters that meet the condition specified by <code class="docutils literal notranslate"><span class="pre">ipex.nn.utils._parameter_wrapper.can_cast_training</span></code>. This includes the original dtype <code class="docutils literal notranslate"><span class="pre">torch.float</span></code>, and module types defined in <code class="docutils literal notranslate"><span class="pre">ipex.nn.utils._parameter_wrapper.IPEX_WEIGHT_CONVERT_MODULE_XPU</span></code>.</p></li>
<li><p>Convert the parameters wrapped by <code class="docutils literal notranslate"><span class="pre">ParameterWrapper</span></code> to the user-specified dtype. If <strong>split master weight</strong> is needed, the optimizer can only be SGD. The original parameters will be divided into top and bottom parts. The top part will be used for forward and backward computation. When updating weights, both the top and bottom parts will be updated simultaneously.</p></li>
</ul>
</section>
<section id="fuse-update-step">
<h2>fuse_update_step<a class="headerlink" href="#fuse-update-step" title="Link to this heading"></a></h2>
<p>This flag is used to specify whether to replace the original optimizer step with a fused step for better performance. The supported optimizers can be referenced from <code class="docutils literal notranslate"><span class="pre">IPEX_FUSED_OPTIMIZER_LIST_XPU</span></code> in <code class="docutils literal notranslate"><span class="pre">ipex.optim._optimizer_utils</span></code>. During the optimization, the original step is saved as <code class="docutils literal notranslate"><span class="pre">optimizer._original_step</span></code>, <code class="docutils literal notranslate"><span class="pre">optimizer.step</span></code> is replaced with a SYCL-written kernel, and the <code class="docutils literal notranslate"><span class="pre">optimizer.fused</span></code> parameter is set to <code class="docutils literal notranslate"><span class="pre">True</span></code>.</p>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="memory_management.html" class="btn btn-neutral float-left" title="Memory Management" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="../releases.html" class="btn btn-neutral float-right" title="Releases" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7393850fc1f0> 
<p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a><a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a><a href='/#' data-wap_ref='dns' id='wap_dns'><small>| Your Privacy Choices</small><span style='height:10px;width:28px;display:inline-block;position:relative;'><svg style='position:absolute;width:28px;bottom:-2px;' version='1.1' id='Layer_1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' x='0px' y='0px' viewBox='0 0 30 14' xml:space='preserve'><title>California Consumer Privacy Act (CCPA) Opt-Out Icon</title><style type='text/css'> .st0 { fill-rule: evenodd; clip-rule: evenodd; fill: #FFFFFF; } .st1 { fill-rule: evenodd; clip-rule: evenodd; fill: #0066FF; } .st2 { fill: #FFFFFF; } .st3 { fill: #0066FF; } </style><g><g id='final---dec.11-2020_1_'><g id='_x30_208-our-toggle_2_' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2_2_' transform='translate(1275.000000, 200.000000)'><path class='st0' d='M7.4,12.8h6.8l3.1-11.6H7.4C4.2,1.2,1.6,3.8,1.6,7S4.2,12.8,7.4,12.8z'></path></g></g></g><g id='final---dec.11-2020'><g id='_x30_208-our-toggle' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2' transform='translate(1275.000000, 200.000000)'><path class='st1' d='M22.6,0H7.4c-3.9,0-7,3.1-7,7s3.1,7,7,7h15.2c3.9,0,7-3.1,7-7S26.4,0,22.6,0z M1.6,7c0-3.2,2.6-5.8,5.8-5.8 h9.9l-3.1,11.6H7.4C4.2,12.8,1.6,10.2,1.6,7z'></path><path id='x' class='st2' d='M24.6,4c0.2,0.2,0.2,0.6,0,0.8l0,0L22.5,7l2.2,2.2c0.2,0.2,0.2,0.6,0,0.8c-0.2,0.2-0.6,0.2-0.8,0 l0,0l-2.2-2.2L19.5,10c-0.2,0.2-0.6,0.2-0.8,0c-0.2-0.2-0.2-0.6,0-0.8l0,0L20.8,7l-2.2-2.2c-0.2-0.2-0.2-0.6,0-0.8 c0.2-0.2,0.6-0.2,0.8,0l0,0l2.2,2.2L23.8,4C24,3.8,24.4,3.8,24.6,4z'></path><path id='y' class='st3' d='M12.7,4.1c0.2,0.2,0.3,0.6,0.1,0.8l0,0L8.6,9.8C8.5,9.9,8.4,10,8.3,10c-0.2,0.1-0.5,0.1-0.7-0.1l0,0 L5.4,7.7c-0.2-0.2-0.2-0.6,0-0.8c0.2-0.2,0.6-0.2,0.8,0l0,0L8,8.6l3.8-4.5C12,3.9,12.4,3.9,12.7,4.1z'></path></g></g></g></g></svg></span></a><a href=https://www.intel.com/content/www/us/en/privacy/privacy-residents-certain-states.html data-wap_ref='nac' id='wap_nac'><small>| Notice at Collection</small></a></div><p></p><div>&copy; Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others. No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (OBSD), <a href='http://opensource.org/licenses/0BSD'>http://opensource.org/licenses/0BSD</a>.</div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>