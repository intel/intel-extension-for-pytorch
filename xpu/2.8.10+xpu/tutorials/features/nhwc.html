

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Channels Last &mdash; Intel&amp;#174 Extension for PyTorch* 2.8.10+xpu documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=a95c1af4" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-pytorch"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Auto Channels Last" href="auto_channels_last.html" />
    <link rel="prev" title="Features" href="../features.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel&#174 Extension for PyTorch*
          </a>
<div class="version">
            <a href="../../../../">2.8.10+xpu ▼</a>
            <p>Click link above to switch version</p>
          </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ABOUT</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../features.html">Features</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../features.html#easy-to-use-python-api">Easy-to-use Python API</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../features.html#channels-last">Channels Last</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Channels Last</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#what-is-channels-last">What is Channels Last</a></li>
<li class="toctree-l4"><a class="reference internal" href="#memory-format-is-all-that-matters">Memory Format Is All That Matters</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#a-nchw-default">a. NCHW (default)</a></li>
<li class="toctree-l5"><a class="reference internal" href="#b-nhwc">b. NHWC</a></li>
<li class="toctree-l5"><a class="reference internal" href="#c-blocked-nchw16c-on-cpu">c. Blocked (nChw16c, on CPU)</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#pytorch-strided-layout">PyTorch Strided Layout</a></li>
<li class="toctree-l4"><a class="reference internal" href="#channels-last-memory-format-apis">Channels Last Memory Format APIs</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#a-tensor-creation">a. tensor creation</a></li>
<li class="toctree-l5"><a class="reference internal" href="#b-tensor-conversion">b. tensor conversion</a></li>
<li class="toctree-l5"><a class="reference internal" href="#c-model-conversion">c. model conversion</a></li>
<li class="toctree-l5"><a class="reference internal" href="#d-operator-coverage-in-pytorch">d. operator coverage in PyTorch</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#writing-channels-last-kernels-on-cpu">Writing Channels Last Kernels on CPU</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#a-register-channels-last-kernel-in-aten-native-manner">a. Register Channels Last Kernel in ATen Native Manner</a></li>
<li class="toctree-l5"><a class="reference internal" href="#b-register-onednn-kernel-on-channels-last">b. Register oneDNN Kernel on Channels Last</a></li>
</ul>
</li>
<li class="toctree-l4"><a class="reference internal" href="#onednn-nhwc-apis">oneDNN NHWC APIs</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#a-create-nhwc-memory">a. Create NHWC Memory</a></li>
<li class="toctree-l5"><a class="reference internal" href="#b-create-convolution-primitive">b. Create Convolution Primitive</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="auto_channels_last.html">Auto Channels Last</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#auto-mixed-precision-amp">Auto Mixed Precision (AMP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#quantization">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#distributed-training">Distributed Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#dlpack-solution">DLPack Solution</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#dpc-extension">DPC++ Extension</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#advanced-configuration">Advanced Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#fully-sharded-data-parallel-fsdp">Fully Sharded Data Parallel (FSDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#torch-compile-for-gpu-beta">torch.compile for GPU (Beta)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#kineto-supported-profiler-tool-prototype">Kineto Supported Profiler Tool (Prototype)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#compute-engine-prototype-feature-for-debug">Compute Engine (Prototype feature for debug)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#ipex-log-prototype-feature-for-debug"><code class="docutils literal notranslate"><span class="pre">IPEX_LOG</span></code> (Prototype feature for debug)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../llm.html">Large Language Models (LLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../technical_details.html">Technical Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../known_issues.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/blogs.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu&amp;version=v2.3.110%2bxpu">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CONTRIBUTING GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel&#174 Extension for PyTorch*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../features.html">Features</a></li>
      <li class="breadcrumb-item active">Channels Last</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/features/nhwc.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="channels-last">
<h1>Channels Last<a class="headerlink" href="#channels-last" title="Link to this heading"></a></h1>
<section id="what-is-channels-last">
<h2>What is Channels Last<a class="headerlink" href="#what-is-channels-last" title="Link to this heading"></a></h2>
<p><strong>Note</strong>: In PyTorch, <strong>memory format</strong> refers to data representation that describes how multidimensional arrays (nD) are stored in linear (1D) memory address space. <strong>Memory format</strong> has the same semantic meaning as <strong>layout</strong> in oneDNN. <strong>Layout</strong> in PyTorch has other semantic of describing <strong>dense</strong> or <strong>sparse</strong> with the attributes: ‘torch.strided’, ‘torch.sparse_coo’.</p>
<p>On CNN models, the canonical order of tensor dimensions is assigned with semantic meaning. For example the input tensor of 2D convolution is of NCHW by default on PyTorch - &lt;batch_size, channels, height, width&gt;. NHWC is an alternative way of describing the tensor dimensions - &lt;batch_size, height, width, channels&gt;.</p>
<p>Look at the following image of illustrating NCHW and NHWC when N=1. Actually when N=1, NHWC has the same format with BMP file image.
<img alt="fig-1-memory-layout" src="../../_images/figure1_memory_layout.png" /></p>
<p>PyTorch refers to NCHW as <code class="docutils literal notranslate"><span class="pre">torch.contiguous_format</span></code> (the default memory format) and to NHWC as <code class="docutils literal notranslate"><span class="pre">torch.channels_last</span></code>, which is a new feature as of the 1.5 release.</p>
<p>TensorFlow uses NHWC as the default memory format because NHWC has a performance advantage over NCHW. On Intel® platforms, we propose to optimize Channels Last memory path for the following reasons:</p>
<ul class="simple">
<li><p><strong>Performance</strong> - NHWC performance is not as good as blocked memory format (nChw16c), but it is close, and much better performance than NCHW.</p></li>
<li><p><strong>User Experience</strong> - Operator coverage of NHWC would be higher than blocked memory format, so user experience is better. To be specific, it is difficult to enable operators that manipulates <code class="docutils literal notranslate"><span class="pre">dim</span></code> on blocked format such as <code class="docutils literal notranslate"><span class="pre">sum(dim=?)</span></code>. You would need to convert tensor from blocked memory format back to NHWC using <code class="docutils literal notranslate"><span class="pre">to_dense()</span></code>, before feeding it into <code class="docutils literal notranslate"><span class="pre">sum()</span></code>. This is naturally supported on Channels Last memory format already.</p></li>
<li><p><strong>Upstream</strong> - Will be easier since CPU doesn’t hold secret ingredient and both inference and training will be covered.</p></li>
</ul>
</section>
<section id="memory-format-is-all-that-matters">
<h2>Memory Format Is All That Matters<a class="headerlink" href="#memory-format-is-all-that-matters" title="Link to this heading"></a></h2>
<p>On CNN models, memory format is almost the foundation of any upper level design. One important fact is that converting memory format could be very expensive. Thus, in case that multiple CNN operators are performed in sequence, e.g. <code class="docutils literal notranslate"><span class="pre">Conv2d</span> <span class="pre">-&gt;</span> <span class="pre">ReLU</span> <span class="pre">-&gt;</span> <span class="pre">Conv2d</span></code>, it’s beneficial to transform them from different memory formats once, do computation and reorder them back.</p>
<p>On PyTorch, you can use 3 types of memory formats on CNN models:</p>
<section id="a-nchw-default">
<h3>a. NCHW (default)<a class="headerlink" href="#a-nchw-default" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span> <span class="c1"># or &#39;xpu&#39;</span>
<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s1">&#39;xpu&#39;</span><span class="p">:</span>
  <span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span>

<span class="c1">## NB: internally blocked format will still be used.</span>
<span class="c1">##   aka. we do &#39;reorder&#39; for &#39;input&#39;, &#39;weight&#39; and &#39;output&#39;,</span>
<span class="c1">##   and believe me this is expensive, roughly 50% perf loss...</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="b-nhwc">
<h3>b. NHWC<a class="headerlink" href="#b-nhwc" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span> <span class="c1"># or &#39;xpu&#39;</span>
<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s1">&#39;xpu&#39;</span><span class="p">:</span>
  <span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span>

<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="c1">## NB: convert to Channels Last memory format.</span>
<span class="c1">##   oneDNN supports NHWC for feature maps (input, output),</span>
<span class="c1">##   but weight still needs to be of blocked format.</span>
<span class="c1">##   Still we can save reorders for feature maps.</span>
<span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="c-blocked-nchw16c-on-cpu">
<h3>c. Blocked (nChw16c, on CPU)<a class="headerlink" href="#c-blocked-nchw16c-on-cpu" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">torch.utils</span><span class="w"> </span><span class="kn">import</span> <span class="n">mkldnn</span> <span class="k">as</span> <span class="n">mkldnn_utils</span>
<span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">20</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="c1">## NB: convert to blocked memory format.</span>
<span class="c1">##   Note that &#39;output&#39; is in blocked memory format,</span>
<span class="c1">##   in case the subsequent operator doesn&#39;t support blocked memory format</span>
<span class="c1">##   you need to manually reorder it back to NCHW by output.to_dense()</span>
<span class="c1">##   mkldnn_utils.to_mkldnn(model) is used to prepack the weight, this will save weight reorder time</span>
<span class="c1">##   for inference. For training, it is not needed.</span>
<span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to_mkldnn</span><span class="p">()</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">mkldnn_utils</span><span class="o">.</span><span class="n">to_mkldnn</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
</pre></div>
</div>
<p>Better to explain the concepts here with a diagram, the <strong>dotted lines</strong> indicate simple memory view, no hard copy.
<img alt="fig-2(1)-pt-conv-layout-path-dispatch" src="../../_images/figure2_dispatch.png" /></p>
<p><strong>Conclusion</strong> is that NHWC path saves the reorders from feature maps compared with NCHW path, but still weight reorder is necessary since oneDNN requires weights to be in blocked memory format. From performance perspective, when <code class="docutils literal notranslate"><span class="pre">batch_size=N</span></code>, weight reorder is minimum compared to feature map reorder. But when <code class="docutils literal notranslate"><span class="pre">batch_size=1</span></code>, weight reorder is usually not negligible. So whether to enable weight prepacking on channels last memory format needs further discussion.</p>
</section>
</section>
<section id="pytorch-strided-layout">
<h2>PyTorch Strided Layout<a class="headerlink" href="#pytorch-strided-layout" title="Link to this heading"></a></h2>
<p>Before moving on, let’s explain how PyTorch organizes tensors in memory - the <strong>layout</strong>. Here we only focus on <strong>dense</strong> tensors, skipping ‘coo’ layout of <strong>sparse</strong> tensor.</p>
<p>The question itself can be reinterpreted as, for a tensor of size &lt;N, C, H, W&gt;, how does PyTorch access the element with index &lt;n, c, h, w&gt; from memory? The answer is <strong>stride</strong>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="o">&gt;</span>
<span class="n">index</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="o">&gt;</span>
<span class="n">strides</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">CHW</span><span class="p">,</span> <span class="n">HW</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="mi">1</span><span class="o">&gt;</span>
<span class="n">offset</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> <span class="o">=</span> <span class="n">stride_n</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">stride_c</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="n">stride_h</span> <span class="o">*</span> <span class="n">h</span> <span class="o">+</span> <span class="n">stride_w</span> <span class="o">*</span> <span class="n">w</span>
                <span class="o">=</span> <span class="n">CHW</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">HW</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="n">W</span> <span class="o">*</span> <span class="n">h</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">w</span>
</pre></div>
</div>
<p>One merit of introducing <strong>stride</strong> is that it can express noncontiguous tensors, e.g. a slice of big tensor. For example, the ‘Xs’ in the following image have a stride of &lt;n1+n2, 1&gt;.</p>
<p><img alt="fig-3-pytorch-strided-layout" src="../../_images/figure3_strided_layout.png" /></p>
<p>Keep in mind that PyTorch Tensor does not have an attribute called ‘memory_format’ or something else. The memory format expression completely relies on <strong>size</strong> and <strong>stride</strong>. The design principle can be found at reference: <a class="reference external" href="https://github.com/pytorch/pytorch/issues/19092">RFC: Memory format (aka layout aka NHWC) support</a>. No matter what the tensor’s memory format is, we need a logical canonical order for the dimensions - that is <strong>NCHW</strong> on PyTorch. Thus, <strong>size</strong> and <strong>stride</strong> are ALWAYS described in the order of <strong>NCHW</strong>. Let’s now look at the Channels Last case of the previous question:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tensor</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="o">&gt;</span>
<span class="n">index</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">n</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span><span class="o">&gt;</span>
<span class="n">strides</span><span class="p">:</span> <span class="o">&lt;</span><span class="n">HWC</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">WC</span><span class="p">,</span> <span class="n">C</span><span class="o">&gt;</span>
<span class="n">offset</span><span class="p">(</span><span class="n">n</span><span class="p">,</span><span class="n">c</span><span class="p">,</span><span class="n">h</span><span class="p">,</span><span class="n">w</span><span class="p">)</span> <span class="o">=</span> <span class="n">stride_n</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="n">stride_c</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="n">stride_h</span> <span class="o">*</span> <span class="n">h</span> <span class="o">+</span> <span class="n">stride_w</span> <span class="o">*</span> <span class="n">w</span>
                <span class="o">=</span> <span class="n">HWC</span> <span class="o">*</span> <span class="n">n</span> <span class="o">+</span> <span class="mi">1</span> <span class="o">*</span> <span class="n">c</span> <span class="o">+</span> <span class="n">WC</span> <span class="o">*</span> <span class="n">h</span> <span class="o">+</span> <span class="n">C</span> <span class="o">*</span> <span class="n">w</span>
</pre></div>
</div>
<p>Actually, this pattern applies to ALL other memory formats as long as it is 4-dim, e.g. strides for CHWN would be &lt;1, HWN, WN, N&gt;.</p>
</section>
<section id="channels-last-memory-format-apis">
<h2>Channels Last Memory Format APIs<a class="headerlink" href="#channels-last-memory-format-apis" title="Link to this heading"></a></h2>
<section id="a-tensor-creation">
<h3>a. tensor creation<a class="headerlink" href="#a-tensor-creation" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span> <span class="c1"># or &#39;xpu&#39;</span>
<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s1">&#39;xpu&#39;</span><span class="p">:</span>
  <span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="b-tensor-conversion">
<h3>b. tensor conversion<a class="headerlink" href="#b-tensor-conversion" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span> <span class="c1"># or &#39;xpu&#39;</span>
<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s1">&#39;xpu&#39;</span><span class="p">:</span>
  <span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span>

<span class="c1">## .contiguous() transforms NHWC noncontiguous to NHWC contiguous.</span>
<span class="c1">## .to() converts NCHW tensor to NHWC one, it is outplace.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>

<span class="c1">## contiguous check</span>
<span class="n">x</span><span class="o">.</span><span class="n">is_contiguous</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="c-model-conversion">
<h3>c. model conversion<a class="headerlink" href="#c-model-conversion" title="Link to this heading"></a></h3>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span> <span class="c1"># or &#39;xpu&#39;</span>
<span class="k">if</span> <span class="n">device</span> <span class="o">==</span> <span class="s1">&#39;xpu&#39;</span><span class="p">:</span>
  <span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span>

<span class="c1">## NB: tensor.to() is an outplace operation</span>
<span class="c1">##   model.to() is inplace. It calls _apply() which is inplace.</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
<span class="nb">input</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">channels_last</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="d-operator-coverage-in-pytorch">
<h3>d. operator coverage in PyTorch<a class="headerlink" href="#d-operator-coverage-in-pytorch" title="Link to this heading"></a></h3>
<p>Detailed operator coverage information has been listed at reference <a class="reference external" href="https://github.com/pytorch/pytorch/wiki/Operators-with-Channels-Last-support">Operators-with-Channels-Last-support</a>.</p>
<p>Some spontaneous questions:</p>
<ul class="simple">
<li><p><strong>How to tell whether this model or operator support Channels Last?</strong> - This requires manual memory format check, aka. ‘torch.channels_last’ input and weight shall NOT generate ‘torch.contiguous_format’ output.</p></li>
<li><p><strong>What if the model comprises of operator not supported Channels Last?</strong> - No errors messages will be shown, the NHWC tensor will be handled by the operator as a non-contiguous NCHW tensor, so result might not be correct depending on the algorithm of this operator.</p></li>
</ul>
</section>
</section>
<section id="writing-channels-last-kernels-on-cpu">
<h2>Writing Channels Last Kernels on CPU<a class="headerlink" href="#writing-channels-last-kernels-on-cpu" title="Link to this heading"></a></h2>
<section id="a-register-channels-last-kernel-in-aten-native-manner">
<h3>a. Register Channels Last Kernel in ATen Native Manner<a class="headerlink" href="#a-register-channels-last-kernel-in-aten-native-manner" title="Link to this heading"></a></h3>
<p>The general guideline has been listed under reference <a class="reference external" href="https://github.com/pytorch/pytorch/wiki/Writing-memory-format-aware-operators">Writing-memory-format-aware-operators</a>, not to repeat here. You may take one of my recent PR <a class="reference external" href="https://github.com/pytorch/pytorch/pull/34864">optimize upsample performance linear mode on CPU</a> as an example, which also demonstrates NHWC performance advantage over NCHW because of the ease of vectorization.</p>
</section>
<section id="b-register-onednn-kernel-on-channels-last">
<h3>b. Register oneDNN Kernel on Channels Last<a class="headerlink" href="#b-register-onednn-kernel-on-channels-last" title="Link to this heading"></a></h3>
<p>Registering a oneDNN kernel under Channels Last memory format on CPU is no different from <a class="reference external" href="https://github.com/pytorch/pytorch/pull/23861">cuDNN</a>: Only very few upper level changes are needed, such as accommodate ‘contiguous()’ to ‘contiguous(suggested_memory_format)’. The automatic reorder of oneDNN weight shall have been hidden in ideep.</p>
</section>
</section>
<section id="onednn-nhwc-apis">
<h2>oneDNN NHWC APIs<a class="headerlink" href="#onednn-nhwc-apis" title="Link to this heading"></a></h2>
<p>Compared to NCHW interfaces, 2 parts need to be addressed on NHWC interfaces:</p>
<section id="a-create-nhwc-memory">
<h3>a. Create NHWC Memory<a class="headerlink" href="#a-create-nhwc-memory" title="Link to this heading"></a></h3>
<p>The logical size and stride description of oneDNN is always in NCHW, this is identical to PyTorch. Example code such as</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cm">/* create md from memory::format_tag */</span>
<span class="k">auto</span><span class="w"> </span><span class="n">src_md</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span><span class="p">(</span>
<span class="w">        </span><span class="p">{</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">H</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">},</span><span class="w"> </span><span class="c1">// logical dims, the order is defined by a primitive</span>
<span class="w">        </span><span class="n">memory</span><span class="o">::</span><span class="n">data_type</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span><span class="w"> </span><span class="c1">// tensor&#39;s data type</span>
<span class="w">        </span><span class="n">memory</span><span class="o">::</span><span class="n">format_tag</span><span class="o">::</span><span class="n">nhwc</span><span class="w"> </span><span class="c1">// memory format, NHWC in this case</span>
<span class="p">);</span>

<span class="cm">/* alternative: create md from strides */</span>
<span class="k">auto</span><span class="w"> </span><span class="n">src_md</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">memory</span><span class="o">::</span><span class="n">desc</span><span class="p">(</span>
<span class="w">        </span><span class="p">{</span><span class="n">N</span><span class="p">,</span><span class="w"> </span><span class="n">C</span><span class="p">,</span><span class="w"> </span><span class="n">H</span><span class="p">,</span><span class="w"> </span><span class="n">W</span><span class="p">},</span><span class="w"> </span><span class="c1">// logical dims, the order is defined by a primitive</span>
<span class="w">        </span><span class="n">memory</span><span class="o">::</span><span class="n">data_type</span><span class="o">::</span><span class="n">f32</span><span class="p">,</span><span class="w"> </span><span class="c1">// tensor&#39;s data type</span>
<span class="w">        </span><span class="p">{</span><span class="n">stride_N</span><span class="p">,</span><span class="w"> </span><span class="n">stride_C</span><span class="p">,</span><span class="w"> </span><span class="n">stride_H</span><span class="p">,</span><span class="w"> </span><span class="n">stride_W</span><span class="p">}</span><span class="w"> </span><span class="c1">// the strides</span>
<span class="p">);</span>

<span class="cm">/* create memory */</span>
<span class="k">auto</span><span class="w"> </span><span class="n">src_mem</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">memory</span><span class="p">(</span><span class="n">src_md</span><span class="p">,</span><span class="w"> </span><span class="n">src_data_ptr</span><span class="p">,</span><span class="w"> </span><span class="n">engine</span><span class="p">);</span>
</pre></div>
</div>
</section>
<section id="b-create-convolution-primitive">
<h3>b. Create Convolution Primitive<a class="headerlink" href="#b-create-convolution-primitive" title="Link to this heading"></a></h3>
<ul class="simple">
<li><p><strong>NCHW</strong> - create <code class="docutils literal notranslate"><span class="pre">memory::desc</span></code> with <em>any</em> card for ‘input’, ‘output’ and ‘weight’; query proposed <code class="docutils literal notranslate"><span class="pre">memory::desc</span></code> from convolution primitive;</p></li>
<li><p><strong>NHWC</strong> - create <code class="docutils literal notranslate"><span class="pre">memory::desc</span></code> with <code class="docutils literal notranslate"><span class="pre">format_tag::nhwc</span></code> for ‘input’ and ‘output’, use <em>any</em> for ‘weight’; if we use <code class="docutils literal notranslate"><span class="pre">hwio</span></code> for ‘weight’ convolution primitive will be created with gemm rather avx512.</p></li>
</ul>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="../features.html" class="btn btn-neutral float-left" title="Features" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="auto_channels_last.html" class="btn btn-neutral float-right" title="Auto Channels Last" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7e011d205720> 
<p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a><a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a><a href='/#' data-wap_ref='dns' id='wap_dns'><small>| Your Privacy Choices</small><span style='height:10px;width:28px;display:inline-block;position:relative;'><svg style='position:absolute;width:28px;bottom:-2px;' version='1.1' id='Layer_1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' x='0px' y='0px' viewBox='0 0 30 14' xml:space='preserve'><title>California Consumer Privacy Act (CCPA) Opt-Out Icon</title><style type='text/css'> .st0 { fill-rule: evenodd; clip-rule: evenodd; fill: #FFFFFF; } .st1 { fill-rule: evenodd; clip-rule: evenodd; fill: #0066FF; } .st2 { fill: #FFFFFF; } .st3 { fill: #0066FF; } </style><g><g id='final---dec.11-2020_1_'><g id='_x30_208-our-toggle_2_' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2_2_' transform='translate(1275.000000, 200.000000)'><path class='st0' d='M7.4,12.8h6.8l3.1-11.6H7.4C4.2,1.2,1.6,3.8,1.6,7S4.2,12.8,7.4,12.8z'></path></g></g></g><g id='final---dec.11-2020'><g id='_x30_208-our-toggle' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2' transform='translate(1275.000000, 200.000000)'><path class='st1' d='M22.6,0H7.4c-3.9,0-7,3.1-7,7s3.1,7,7,7h15.2c3.9,0,7-3.1,7-7S26.4,0,22.6,0z M1.6,7c0-3.2,2.6-5.8,5.8-5.8 h9.9l-3.1,11.6H7.4C4.2,12.8,1.6,10.2,1.6,7z'></path><path id='x' class='st2' d='M24.6,4c0.2,0.2,0.2,0.6,0,0.8l0,0L22.5,7l2.2,2.2c0.2,0.2,0.2,0.6,0,0.8c-0.2,0.2-0.6,0.2-0.8,0 l0,0l-2.2-2.2L19.5,10c-0.2,0.2-0.6,0.2-0.8,0c-0.2-0.2-0.2-0.6,0-0.8l0,0L20.8,7l-2.2-2.2c-0.2-0.2-0.2-0.6,0-0.8 c0.2-0.2,0.6-0.2,0.8,0l0,0l2.2,2.2L23.8,4C24,3.8,24.4,3.8,24.6,4z'></path><path id='y' class='st3' d='M12.7,4.1c0.2,0.2,0.3,0.6,0.1,0.8l0,0L8.6,9.8C8.5,9.9,8.4,10,8.3,10c-0.2,0.1-0.5,0.1-0.7-0.1l0,0 L5.4,7.7c-0.2-0.2-0.2-0.6,0-0.8c0.2-0.2,0.6-0.2,0.8,0l0,0L8,8.6l3.8-4.5C12,3.9,12.4,3.9,12.7,4.1z'></path></g></g></g></g></svg></span></a><a href=https://www.intel.com/content/www/us/en/privacy/privacy-residents-certain-states.html data-wap_ref='nac' id='wap_nac'><small>| Notice at Collection</small></a></div><p></p><div>&copy; Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others. No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (OBSD), <a href='http://opensource.org/licenses/0BSD'>http://opensource.org/licenses/0BSD</a>.</div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>