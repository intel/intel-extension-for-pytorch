

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>DPC++ Extension &mdash; Intel&amp;#174 Extension for PyTorch* 2.8.10+xpu documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=e59714d7" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=a95c1af4" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-pytorch"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Advanced Configuration" href="advanced_configuration.html" />
    <link rel="prev" title="DLPack Solution" href="DLPack.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel&#174 Extension for PyTorch*
          </a>
<div class="version">
            <a href="../../../../">2.8.10+xpu ▼</a>
            <p>Click link above to switch version</p>
          </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ABOUT</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../introduction.html">Introduction</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../features.html">Features</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../features.html#easy-to-use-python-api">Easy-to-use Python API</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#channels-last">Channels Last</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#auto-mixed-precision-amp">Auto Mixed Precision (AMP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#quantization">Quantization</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#distributed-training">Distributed Training</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#dlpack-solution">DLPack Solution</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../features.html#dpc-extension">DPC++ Extension</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">DPC++ Extension</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#introduction">Introduction</a></li>
<li class="toctree-l4"><a class="reference internal" href="#motivation-and-example">Motivation and Example</a></li>
<li class="toctree-l4"><a class="reference internal" href="#writing-a-dpc-extension">Writing a DPC++ Extension</a><ul>
<li class="toctree-l5"><a class="reference internal" href="#building-with-setuptools">Building with setuptools</a></li>
<li class="toctree-l5"><a class="reference internal" href="#jit-compiling-extensions">JIT Compiling Extensions</a></li>
<li class="toctree-l5"><a class="reference internal" href="#building-with-cmake">Building with CMake</a></li>
<li class="toctree-l5"><a class="reference internal" href="#requesting-the-current-c10-xpu-xpustream">Requesting the current c10::xpu::XPUStream</a></li>
<li class="toctree-l5"><a class="reference internal" href="#fetching-the-corresponding-sycl-queue">Fetching the corresponding sycl::queue</a></li>
<li class="toctree-l5"><a class="reference internal" href="#writing-the-dpc-op">Writing the DPC++ Op</a><ul>
<li class="toctree-l6"><a class="reference internal" href="#using-accessors">Using accessors</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#advanced-configuration">Advanced Configuration</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#fully-sharded-data-parallel-fsdp">Fully Sharded Data Parallel (FSDP)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#torch-compile-for-gpu-beta">torch.compile for GPU (Beta)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#kineto-supported-profiler-tool-prototype">Kineto Supported Profiler Tool (Prototype)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#compute-engine-prototype-feature-for-debug">Compute Engine (Prototype feature for debug)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../features.html#ipex-log-prototype-feature-for-debug"><code class="docutils literal notranslate"><span class="pre">IPEX_LOG</span></code> (Prototype feature for debug)</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../llm.html">Large Language Models (LLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../technical_details.html">Technical Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="../releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../known_issues.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/blogs.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu&amp;version=v2.3.110%2bxpu">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../examples.html">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../api_doc.html">API Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CONTRIBUTING GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../contribution.html">Contribution</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel&#174 Extension for PyTorch*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../features.html">Features</a></li>
      <li class="breadcrumb-item active">DPC++ Extension</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/tutorials/features/DPC++_Extension.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="dpc-extension">
<h1>DPC++ Extension<a class="headerlink" href="#dpc-extension" title="Link to this heading"></a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Link to this heading"></a></h2>
<p>C++ extension is a mechanism developed by PyTorch that lets you to create customized and highly efficient PyTorch operators defined out-of-source, i.e. separate from the PyTorch backend. (For more details, see https://pytorch.org/tutorials/advanced/cpp_extension.html). Based on the PyTorch C++ extension mechanism, Intel® Extension for PyTorch* lets you to create PyTorch operators with custom DPC++ kernels to run on the XPU device.</p>
<p><strong>Note:</strong> The current implementation of the DPC++ extension only supports Linux.</p>
</section>
<section id="motivation-and-example">
<h2>Motivation and Example<a class="headerlink" href="#motivation-and-example" title="Link to this heading"></a></h2>
<p>This tutorial walks through a practical example of writing and using a DPC++ extension on the XPU device with Intel® Extension for PyTorch*.</p>
</section>
<section id="writing-a-dpc-extension">
<h2>Writing a DPC++ Extension<a class="headerlink" href="#writing-a-dpc-extension" title="Link to this heading"></a></h2>
<p>DPC++ extensions come in two flavors: They can be built “ahead of time” (AOT) with <code class="docutils literal notranslate"><span class="pre">setuptools</span></code>, or “just in time” (JIT) via <code class="docutils literal notranslate"><span class="pre">torch.xpu.cpp_extension.load()</span></code>. We’ll begin with the first approach and discuss the latter one afterwards.</p>
<p>Besides, DPC++ extension also supports compilation with <code class="docutils literal notranslate"><span class="pre">CMake</span></code>. We’ll discuss the CMake methodology at last.</p>
<section id="building-with-setuptools">
<h3>Building with setuptools<a class="headerlink" href="#building-with-setuptools" title="Link to this heading"></a></h3>
<p>For building with <code class="docutils literal notranslate"><span class="pre">setuptools</span></code>, we build our DPC++ extension by writing a <code class="docutils literal notranslate"><span class="pre">setup.py</span></code> script that uses <code class="docutils literal notranslate"><span class="pre">setuptools</span></code> to compile our C++ code. For the Long-Long-Term-Memory unit (LLTM), it looks like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">setuptools</span><span class="w"> </span><span class="kn">import</span> <span class="n">setup</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.xpu.cpp_extension</span><span class="w"> </span><span class="kn">import</span> <span class="n">DPCPPExtension</span><span class="p">,</span> <span class="n">DpcppBuildExtension</span>

<span class="n">setup</span><span class="p">(</span>
    <span class="n">name</span><span class="o">=</span><span class="s1">&#39;lltm&#39;</span><span class="p">,</span>
    <span class="n">ext_modules</span><span class="o">=</span><span class="p">[</span>
        <span class="n">DPCPPExtension</span><span class="p">(</span><span class="s1">&#39;lltm_xpu&#39;</span><span class="p">,</span> <span class="p">[</span>
            <span class="s1">&#39;lltm_xpu.cpp&#39;</span><span class="p">,</span>
            <span class="s1">&#39;lltm_xpu_kernel.cpp&#39;</span><span class="p">,</span>
        <span class="p">])</span>
    <span class="p">],</span>
    <span class="n">cmdclass</span><span class="o">=</span><span class="p">{</span>
        <span class="s1">&#39;build_ext&#39;</span><span class="p">:</span> <span class="n">DpcppBuildExtension</span>
    <span class="p">})</span>
</pre></div>
</div>
<p>In this code, <code class="docutils literal notranslate"><span class="pre">DPCPPExtension</span></code> is a convenience wrapper around <code class="docutils literal notranslate"><span class="pre">setuptools.Extension</span></code> that passes the correct include paths and sets the language of the extension to C++. The equivalent vanilla <code class="docutils literal notranslate"><span class="pre">setuptools</span></code> code would simply be:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Extension</span><span class="p">(</span>
   <span class="n">name</span><span class="o">=</span><span class="s1">&#39;lltm_xpu&#39;</span><span class="p">,</span>
   <span class="n">sources</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;lltm_xpu.cpp&#39;</span><span class="p">,</span> <span class="s1">&#39;lltm_xpu_kernel.cpp&#39;</span><span class="p">,],</span>
   <span class="n">include_dirs</span><span class="o">=</span><span class="n">cpp_extension</span><span class="o">.</span><span class="n">include_paths</span><span class="p">(),</span>
   <span class="n">language</span><span class="o">=</span><span class="s1">&#39;c++&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">DpcppBuildExtension</span></code> performs a number of required configuration steps and checks and also manages compilation in the case of DPC++ extensions. And that’s all we really need to know about building DPC++ extensions for now.</p>
<p>Let’s take a look at the implementation of our DPC++ extension, which goes into <code class="docutils literal notranslate"><span class="pre">lltm_xpu.cpp</span></code> and <code class="docutils literal notranslate"><span class="pre">lltm_xpu_kernel.cpp</span></code>.
After building the Python module with DPC++ extension, the <code class="docutils literal notranslate"><span class="pre">lltm_xpu</span></code> is available for importing as an extension plug-in.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">lltm_xpu</span>
</pre></div>
</div>
</section>
<section id="jit-compiling-extensions">
<h3>JIT Compiling Extensions<a class="headerlink" href="#jit-compiling-extensions" title="Link to this heading"></a></h3>
<p>Previously, we mentioned that there were two ways of building DPC++ extensions: use setuptools as AOT or compile with JIT. Having the former one introduced, let’s elaborate on the latter one. The JIT compilation mechanism provides a methodology to compile and load your extensions on the fly by invoking a simple <code class="docutils literal notranslate"><span class="pre">torch</span></code> API function <code class="docutils literal notranslate"><span class="pre">torch.xpu.cpp_extension.load()</span></code>. For the LLTM, this would look as simple as this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.xpu.cpp_extension</span><span class="w"> </span><span class="kn">import</span> <span class="n">load</span>

<span class="n">lltm_xpu</span> <span class="o">=</span> <span class="n">load</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;lltm_xpu&quot;</span><span class="p">,</span> <span class="n">sources</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;lltm_xpu.cpp&#39;</span><span class="p">,</span> <span class="s1">&#39;lltm_xpu_kernel.cpp&#39;</span><span class="p">,])</span>
</pre></div>
</div>
<p>Here, we provide a function with the same information as those for <code class="docutils literal notranslate"><span class="pre">setuptools</span></code>. In the background, the function will do the followings:</p>
<ol class="simple">
<li><p>Create a temporary directory <code class="docutils literal notranslate"><span class="pre">/tmp/torch_extensions/py[ver]_xpu/lltm_xpu</span></code>,</p></li>
<li><p>Emit a <code class="docutils literal notranslate"><span class="pre">Ninja</span></code> build file into that temporary directory,</p></li>
<li><p>Compile your source files into a shared library,</p></li>
<li><p>Import this shared library as a Python module.</p></li>
</ol>
<p>In fact, if you pass <code class="docutils literal notranslate"><span class="pre">verbose=True</span></code> to <code class="docutils literal notranslate"><span class="pre">cpp_extension.load()</span></code>, you will be informed about the process:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Emitting</span> <span class="n">ninja</span> <span class="n">build</span> <span class="n">file</span> <span class="o">/</span><span class="n">home</span><span class="o">/</span><span class="p">[</span><span class="n">user_name</span><span class="p">]</span><span class="o">/.</span><span class="n">cache</span><span class="o">/</span><span class="n">torch_extensions</span><span class="o">/</span><span class="n">py</span><span class="p">[</span><span class="n">ver</span><span class="p">]</span><span class="n">_xpu</span><span class="o">/</span><span class="n">lltm_xpu</span><span class="o">/</span><span class="n">build</span><span class="o">.</span><span class="n">ninja</span><span class="o">...</span>
<span class="n">Building</span> <span class="n">extension</span> <span class="n">module</span> <span class="n">lltm_xpu</span><span class="o">...</span>
<span class="n">Loading</span> <span class="n">extension</span> <span class="n">module</span> <span class="n">lltm_xpu</span><span class="o">...</span>
</pre></div>
</div>
<p>The resulting Python module are exactly the same as the ones produced by <code class="docutils literal notranslate"><span class="pre">setuptools</span></code>. This avoids maintaining a separate <code class="docutils literal notranslate"><span class="pre">setup.py</span></code> build file. Generally this JIT technique will do the compilation just fine, however, if your setup is more complicated and you do need the full power of <code class="docutils literal notranslate"><span class="pre">setuptools</span></code>, you can still write your own <code class="docutils literal notranslate"><span class="pre">setup.py</span></code>. It will take some time at the first time when you run through this line, as the extension is compiling in the background. Since we use Ninja build system to build source codes, re-compilation is incremental and thus the compilation reloads the extension when you run your Python module from the second time. It is fast and has low overhead if there are no code changes in the extension’s source files.</p>
</section>
<section id="building-with-cmake">
<h3>Building with CMake<a class="headerlink" href="#building-with-cmake" title="Link to this heading"></a></h3>
<p>For building with <code class="docutils literal notranslate"><span class="pre">CMake</span></code>, we build our DPC++ extension by writing a <code class="docutils literal notranslate"><span class="pre">CMakeLists.txt</span></code> file that uses CMake to build our C++ code. For the same example we showed using <code class="docutils literal notranslate"><span class="pre">setuptools</span></code>, the <code class="docutils literal notranslate"><span class="pre">CMakeLists.txt</span></code> looks like this:
CMakeLists.txt</p>
<div class="highlight-cmake notranslate"><div class="highlight"><pre><span></span><span class="nb">cmake_minimum_required</span><span class="p">(</span><span class="s">VERSION</span><span class="w"> </span><span class="s">3.18</span><span class="w"> </span><span class="s">FATAL_ERROR</span><span class="p">)</span>
<span class="nb">project</span><span class="p">(</span><span class="s">lltm_xpu</span><span class="p">)</span>

<span class="nb">find_package</span><span class="p">(</span><span class="s">Python</span><span class="w"> </span><span class="s">COMPONENTS</span><span class="w"> </span><span class="s">Interpreter</span><span class="w"> </span><span class="s">Development</span><span class="p">)</span>
<span class="nb">find_package</span><span class="p">(</span><span class="s">Torch</span><span class="w"> </span><span class="s">REQUIRED</span><span class="p">)</span>
<span class="nb">find_package</span><span class="p">(</span><span class="s">IPEX</span><span class="w"> </span><span class="s">REQUIRED</span><span class="p">)</span>

<span class="c">#The SYCL kernel should be compiled with &quot;-fsycl&quot;</span>
<span class="nb">set_source_files_properties</span><span class="p">(</span><span class="s">lltm_xpu_kernel.cpp</span><span class="w"> </span><span class="s">PROPERTIES</span><span class="w"> </span><span class="s">COMPILE_FLAGS</span><span class="w"> </span><span class="s2">&quot;-fsycl&quot;</span><span class="p">)</span>

<span class="nb">add_library</span><span class="p">(</span><span class="s">lltm_xpu</span><span class="w"> </span><span class="s">SHARED</span><span class="w"> </span><span class="s">lltm_xpu.cpp</span><span class="w"> </span><span class="s">lltm_xpu_kernel.cpp</span><span class="p">)</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">lltm_xpu</span><span class="w"> </span><span class="s2">&quot;${TORCH_LIBRARIES}&quot;</span><span class="p">)</span>
<span class="nb">target_link_libraries</span><span class="p">(</span><span class="s">lltm_xpu</span><span class="w"> </span><span class="s2">&quot;${TORCH_IPEX_LIBRARIES}&quot;</span><span class="p">)</span>
<span class="nb">target_include_directories</span><span class="p">(</span><span class="s">lltm_xpu</span><span class="w"> </span><span class="s">PUBLIC</span><span class="w"> </span><span class="s2">&quot;${Python_INCLUDE_DIRS}&quot;</span><span class="p">)</span>
<span class="nb">target_include_directories</span><span class="p">(</span><span class="s">lltm_xpu</span><span class="w"> </span><span class="s">PUBLIC</span><span class="w"> </span><span class="s2">&quot;${TORCH_IPEX_INCLUDE_DIRS}&quot;</span><span class="p">)</span>

<span class="nb">set_property</span><span class="p">(</span><span class="s">TARGET</span><span class="w"> </span><span class="s">lltm_xpu</span><span class="w"> </span><span class="s">PROPERTY</span><span class="w"> </span><span class="s">CXX_STANDARD</span><span class="w"> </span><span class="s">17</span><span class="p">)</span>
<span class="c">#DPCPP need 17</span>
</pre></div>
</div>
<p>Find cmake_prefix_path of torch and ipex</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ python
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import intel_extension_for_pytorch
&gt;&gt;&gt; torch.utils.cmake_prefix_path
&#39;&lt;cmake_prefix_path for torch&gt;&#39;
&gt;&gt;&gt; intel_extension_for_pytorch.cmake_prefix_path
&#39;&lt;cmake_prefix_path for ipex&gt;&#39;
</pre></div>
</div>
<p>Commands for compilation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ cmake -DCMAKE_PREFIX_PATH=&lt;torch &amp; ipex cmake_prefix_path&gt; -DCMAKE_C_COMPILER=icx -DCMAKE_CXX_COMPILER=&lt;icpx|icx&gt; ..
$ make
</pre></div>
</div>
<p>After build the python module with CMake, the <code class="docutils literal notranslate"><span class="pre">lltm_xpu</span></code> is also avalible for importing as a extension plug-in like setuptools method.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$ python
&gt;&gt;&gt; import torch
&gt;&gt;&gt; import intel_extension_for_pytorch
&gt;&gt;&gt; import lltm_xpu
</pre></div>
</div>
</section>
<section id="requesting-the-current-c10-xpu-xpustream">
<h3>Requesting the current c10::xpu::XPUStream<a class="headerlink" href="#requesting-the-current-c10-xpu-xpustream" title="Link to this heading"></a></h3>
<p>If you need to get the current <code class="docutils literal notranslate"><span class="pre">c10::xpu::XPUStream</span></code> on the current XPU device to do synchronization. You can implement it as below.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">c10</span><span class="o">::</span><span class="n">xpu</span><span class="o">::</span><span class="n">XPUStream</span><span class="w"> </span><span class="n">stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">xpu</span><span class="o">::</span><span class="n">getCurrentXPUStream</span><span class="p">();</span>
<span class="n">stream</span><span class="p">.</span><span class="n">synchronize</span><span class="p">();</span>
</pre></div>
</div>
</section>
<section id="fetching-the-corresponding-sycl-queue">
<h3>Fetching the corresponding sycl::queue<a class="headerlink" href="#fetching-the-corresponding-sycl-queue" title="Link to this heading"></a></h3>
<p>We provide some APIs to fetch the corresponding <code class="docutils literal notranslate"><span class="pre">sycl::queue</span></code> associated with the
current <code class="docutils literal notranslate"><span class="pre">c10::xpu::XPUStream</span></code>.
In C++ code, you can fetch a <code class="docutils literal notranslate"><span class="pre">sycl::queue</span></code> reference as below.</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">c10</span><span class="o">::</span><span class="n">xpu</span><span class="o">::</span><span class="n">XPUStream</span><span class="w"> </span><span class="n">stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">xpu</span><span class="o">::</span><span class="n">getCurrentXPUStream</span><span class="p">();</span>
<span class="k">auto</span><span class="o">&amp;</span><span class="w"> </span><span class="n">queue</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">stream</span><span class="p">.</span><span class="n">queue</span><span class="p">();</span>
</pre></div>
</div>
<p>In python code, you can use the below codes to get a <code class="docutils literal notranslate"><span class="pre">void*</span></code>, which can cast to a <code class="docutils literal notranslate"><span class="pre">sycl::queue</span></code> pointer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">intel_extension_for_pytorch</span>
<span class="n">stream</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">xpu</span><span class="o">.</span><span class="n">current_stream</span><span class="p">()</span>
<span class="n">queue</span> <span class="o">=</span> <span class="n">stream</span><span class="o">.</span><span class="n">sycl_queue</span> <span class="c1"># queue is a ``void*`` which can cast to a sycl::queue pointer</span>
</pre></div>
</div>
<p>Subsequently, you can submit a customized kernel via <code class="docutils literal notranslate"><span class="pre">sycl::queue</span></code> by yourself. Refer to <a class="reference external" href="#writing-the-dpc-op">Writing the DPC++ Op</a> for more details.</p>
</section>
<section id="writing-the-dpc-op">
<h3>Writing the DPC++ Op<a class="headerlink" href="#writing-the-dpc-op" title="Link to this heading"></a></h3>
<p>The general strategy for writing a DPC++ extension is to write a C++ file that defines the functions that are called from Python, and binds those functions to Python with pybind11. The C++ functions do some checks and ultimately forward the calls to submit SYCL kernels. The <code class="docutils literal notranslate"><span class="pre">ipex.cpp_extension</span></code> package then takes care of compiling the C++ sources with a DPC++ compiler.</p>
<p>Let’s consider the PyTorch CUDA examples https://pytorch.org/tutorials/advanced/cpp_extension.html#writing-a-mixed-c-cuda-extension. Here is how we implement it in DPC++ style:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/extension.h&gt;</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;vector&gt;</span>

<span class="c1">// XPU forward declarations</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lltm_xpu_forward</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_cell</span><span class="p">);</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lltm_xpu_backward</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">grad_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">grad_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">X</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">gate_weights</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weights</span><span class="p">);</span>

<span class="c1">// C++ interface</span>

<span class="cp">#define CHECK_XPU(x) TORCH_CHECK(x.device().is_xpu(), #x &quot; must be a XPU tensor&quot;)</span>
<span class="cp">#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x &quot; must be contiguous&quot;)</span>
<span class="cp">#define CHECK_INPUT(x) CHECK_XPU(x); CHECK_CONTIGUOUS(x)</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lltm_forward</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_cell</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">input</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">weights</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">bias</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">old_h</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">lltm_xpu_forward</span><span class="p">(</span><span class="n">input</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="n">old_h</span><span class="p">,</span><span class="w"> </span><span class="n">old_cell</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lltm_backward</span><span class="p">(</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">grad_h</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">grad_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">X</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">gate_weights</span><span class="p">,</span>
<span class="w">    </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weights</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">grad_h</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">grad_cell</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">input_gate</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">output_gate</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">candidate_cell</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">gate_weights</span><span class="p">);</span>
<span class="w">  </span><span class="n">CHECK_INPUT</span><span class="p">(</span><span class="n">weights</span><span class="p">);</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">lltm_xpu_backward</span><span class="p">(</span>
<span class="w">      </span><span class="n">grad_h</span><span class="p">,</span>
<span class="w">      </span><span class="n">grad_cell</span><span class="p">,</span>
<span class="w">      </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">      </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">      </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">      </span><span class="n">candidate_cell</span><span class="p">,</span>
<span class="w">      </span><span class="n">X</span><span class="p">,</span>
<span class="w">      </span><span class="n">gate_weights</span><span class="p">,</span>
<span class="w">      </span><span class="n">weights</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">PYBIND11_MODULE</span><span class="p">(</span><span class="n">TORCH_EXTENSION_NAME</span><span class="p">,</span><span class="w"> </span><span class="n">m</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">&quot;forward&quot;</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">lltm_forward</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;LLTM forward (XPU)&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="n">m</span><span class="p">.</span><span class="n">def</span><span class="p">(</span><span class="s">&quot;backward&quot;</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">lltm_backward</span><span class="p">,</span><span class="w"> </span><span class="s">&quot;LLTM backward (XPU)&quot;</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The bridge code checks and forwards the calls to functions that we’ll define in the DPC++ code file <code class="docutils literal notranslate"><span class="pre">lltm_xpu_kernel.cpp</span></code>. DPC++ supports compiling C++ naturally, thus we still have ATen and the C++ standard library available to us.</p>
<p>Let’s go through the DPC++ code step by step:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;torch/extension.h&gt;</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ipex.h&gt;</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;vector&gt;</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">scalar_t</span><span class="o">&gt;</span>
<span class="n">scalar_t</span><span class="w"> </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">scalar_t</span><span class="w"> </span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mf">1.0f</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="p">(</span><span class="mf">1.0f</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">z</span><span class="p">));</span>
<span class="p">}</span>
</pre></div>
</div>
<p>At the beginning of the code, we include <code class="docutils literal notranslate"><span class="pre">&lt;torch/extension.h&gt;</span></code> that will introduce all the torch definitions into the code. After that, the <code class="docutils literal notranslate"><span class="pre">&lt;ipex.h&gt;</span></code> line includes the SYCL header in DPC++. With the <code class="docutils literal notranslate"><span class="pre">&lt;torch/extension.h&gt;</span></code> and <code class="docutils literal notranslate"><span class="pre">&lt;ipex.h&gt;</span></code>, all the essential declarations have been included for writing the DPC++ kernel to run on the XPU device. The helper function <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> does the math calculation with the more efficient C++ language. Next are some more helper functions for LLTM:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">scalar_t</span><span class="o">&gt;</span>
<span class="n">scalar_t</span><span class="w"> </span><span class="n">d_sigmoid</span><span class="p">(</span><span class="n">scalar_t</span><span class="w"> </span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">s</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">z</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">(</span><span class="mf">1.0f</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">s</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">s</span><span class="p">;</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">scalar_t</span><span class="o">&gt;</span>
<span class="n">scalar_t</span><span class="w"> </span><span class="n">d_tanh</span><span class="p">(</span><span class="n">scalar_t</span><span class="w"> </span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">t</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tanh</span><span class="p">(</span><span class="n">z</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="mf">1.0f</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="p">(</span><span class="n">t</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">t</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">scalar_t</span><span class="o">&gt;</span>
<span class="n">scalar_t</span><span class="w"> </span><span class="n">elu</span><span class="p">(</span><span class="n">scalar_t</span><span class="w"> </span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">scalar_t</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">fmax</span><span class="p">(</span><span class="mf">0.0f</span><span class="p">,</span><span class="w"> </span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">fmin</span><span class="p">(</span><span class="mf">0.0f</span><span class="p">,</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">));</span>
<span class="p">}</span>

<span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">scalar_t</span><span class="o">&gt;</span>
<span class="n">scalar_t</span><span class="w"> </span><span class="n">d_elu</span><span class="p">(</span><span class="n">scalar_t</span><span class="w"> </span><span class="n">z</span><span class="p">,</span><span class="w"> </span><span class="n">scalar_t</span><span class="w"> </span><span class="n">alpha</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">e</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">);</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">d_relu</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">z</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.0f</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="mf">0.0f</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">;</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">d_relu</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="p">(((</span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">e</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mf">1.0f</span><span class="p">))</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">)</span><span class="w"> </span><span class="o">?</span><span class="w"> </span><span class="p">(</span><span class="n">alpha</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">e</span><span class="p">)</span><span class="w"> </span><span class="o">:</span><span class="w"> </span><span class="mf">0.0f</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>Now we can implement the actual code for our extension with two functions in DPC++:</p>
<ul class="simple">
<li><p>a function that performs operations we don’t wish to explicitly write by hand and calls into the function to submit the SYCL kernel,</p></li>
<li><p>a function that actual submits the SYCL kernel to the XPU device for the parts we want to speed up.</p></li>
</ul>
<p>For the forward pass, the first function looks like this:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lltm_xpu_forward</span><span class="p">(</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input</span><span class="p">,</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weights</span><span class="p">,</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">bias</span><span class="p">,</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_h</span><span class="p">,</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">old_cell</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">cat</span><span class="p">({</span><span class="n">old_h</span><span class="p">,</span><span class="w"> </span><span class="n">input</span><span class="p">},</span><span class="w"> </span><span class="cm">/*dim=*/</span><span class="mi">1</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">gates</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">addmm</span><span class="p">(</span><span class="n">bias</span><span class="p">,</span><span class="w"> </span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">weights</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">));</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">old_cell</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">state_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">old_cell</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">new_h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">new_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">input_gate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">output_gate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">candidate_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">old_cell</span><span class="p">);</span>

<span class="w">  </span><span class="n">AT_DISPATCH_FLOATING_TYPES</span><span class="p">(</span><span class="n">gates</span><span class="p">.</span><span class="n">type</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;lltm_forward_xpu&quot;</span><span class="p">,</span><span class="w"> </span><span class="p">([</span><span class="o">&amp;</span><span class="p">]</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">lltm_xpu_forward_kernel</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(</span>
<span class="w">          </span><span class="n">gates</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">old_cell</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">new_h</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">new_cell</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">input_gate</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">output_gate</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">candidate_cell</span><span class="p">.</span><span class="n">data</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">state_size</span><span class="p">,</span>
<span class="w">                  </span><span class="n">batch_size</span><span class="p">);</span>
<span class="w">  </span><span class="p">}));</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="n">new_h</span><span class="p">,</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">,</span><span class="w"> </span><span class="n">X</span><span class="p">,</span><span class="w"> </span><span class="n">gates</span><span class="p">};</span>
<span class="p">}</span>
</pre></div>
</div>
<p>The purpose of <code class="docutils literal notranslate"><span class="pre">AT_DISPATCH_FLOATING_TYPES</span></code> is to take care of this dispatch for us. It takes a type (<code class="docutils literal notranslate"><span class="pre">gates.type()</span></code> in our case), a name (for error messages) and a lambda function. Inside this lambda function, the type alias <code class="docutils literal notranslate"><span class="pre">scalar_t</span></code> is available and is defined as the type that the tensor actually is at runtime in that context. As such, if we have a template function (which will submit the actual SYCL kernel), we can instantiate it with this <code class="docutils literal notranslate"><span class="pre">scalar_t</span></code> alias, and the correct function will be called. In this case, we also want to retrieve the data pointers of the tensors as pointers of that <code class="docutils literal notranslate"><span class="pre">scalar_t</span></code> type. If you wanted to dispatch over all types and not just floating point types (<code class="docutils literal notranslate"><span class="pre">Float</span></code> and <code class="docutils literal notranslate"><span class="pre">Double</span></code>), you can use <code class="docutils literal notranslate"><span class="pre">AT_DISPATCH_ALL_TYPES</span></code>.</p>
<p>Here’s how to submit the actual kernel to the XPU device:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">scalar_t</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">lltm_xpu_forward_kernel</span><span class="p">(</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="n">scalar_t</span><span class="o">*</span><span class="w"> </span><span class="n">gates</span><span class="p">,</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="n">scalar_t</span><span class="o">*</span><span class="w"> </span><span class="n">old_cell</span><span class="p">,</span>
<span class="w">        </span><span class="n">scalar_t</span><span class="o">*</span><span class="w"> </span><span class="n">new_h</span><span class="p">,</span>
<span class="w">        </span><span class="n">scalar_t</span><span class="o">*</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">        </span><span class="n">scalar_t</span><span class="o">*</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">        </span><span class="n">scalar_t</span><span class="o">*</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">        </span><span class="n">scalar_t</span><span class="o">*</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">,</span>
<span class="w">        </span><span class="kt">size_t</span><span class="w"> </span><span class="n">state_size</span><span class="p">,</span>
<span class="w">        </span><span class="kt">size_t</span><span class="w"> </span><span class="n">batch_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">threads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1024</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">work_groups</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">state_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threads</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">threads</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// define the kernel</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">cgf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">sycl</span><span class="o">::</span><span class="n">handler</span><span class="o">&amp;</span><span class="w"> </span><span class="n">cgh</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">kfn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="o">=</span><span class="p">](</span><span class="n">sycl</span><span class="o">::</span><span class="n">nd_item</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="w"> </span><span class="n">item</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">column</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">item</span><span class="p">.</span><span class="n">get_group</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">item</span><span class="p">.</span><span class="n">get_group_range</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">item</span><span class="p">.</span><span class="n">get_local_id</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">index</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">item</span><span class="p">.</span><span class="n">get_group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">state_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">column</span><span class="p">;</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">gates_row</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">item</span><span class="p">.</span><span class="n">get_group</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">state_size</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">3</span><span class="p">);</span>

<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">column</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">state_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="n">input_gate</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="n">gates_row</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">column</span><span class="p">]);</span>
<span class="w">        </span><span class="n">output_gate</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">sigmoid</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="n">gates_row</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">state_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">column</span><span class="p">]);</span>
<span class="w">        </span><span class="n">candidate_cell</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">elu</span><span class="p">(</span><span class="n">gates</span><span class="p">[</span><span class="n">gates_row</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">2</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">state_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">column</span><span class="p">]);</span>
<span class="w">        </span><span class="n">new_cell</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span>
<span class="w">                </span><span class="n">old_cell</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">input_gate</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
<span class="w">        </span><span class="n">new_h</span><span class="p">[</span><span class="n">index</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tanh</span><span class="p">(</span><span class="n">new_cell</span><span class="p">[</span><span class="n">index</span><span class="p">])</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">output_gate</span><span class="p">[</span><span class="n">index</span><span class="p">];</span>
<span class="w">      </span><span class="p">}</span>

<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="n">cgh</span><span class="p">.</span><span class="n">parallel_for</span><span class="p">(</span>
<span class="w">            </span><span class="n">sycl</span><span class="o">::</span><span class="n">nd_range</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(</span>
<span class="w">                    </span><span class="n">sycl</span><span class="o">::</span><span class="n">range</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(</span><span class="n">work_groups</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">threads</span><span class="p">,</span><span class="w"> </span><span class="n">batch_size</span><span class="p">),</span>
<span class="w">                    </span><span class="n">sycl</span><span class="o">::</span><span class="n">range</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(</span><span class="n">threads</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)),</span>
<span class="w">            </span><span class="n">kfn</span><span class="p">);</span>
<span class="w">  </span><span class="p">};</span>

<span class="w">  </span><span class="c1">// submit kernel</span>
<span class="w">  </span><span class="n">c10</span><span class="o">::</span><span class="n">xpu</span><span class="o">::</span><span class="n">XPUStream</span><span class="w"> </span><span class="n">stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">xpu</span><span class="o">::</span><span class="n">getCurrentXPUStream</span><span class="p">();</span>
<span class="w">  </span><span class="n">stream</span><span class="p">.</span><span class="n">queue</span><span class="p">().</span><span class="n">submit</span><span class="p">(</span><span class="n">cgf</span><span class="p">);</span>
<span class="p">}</span>
</pre></div>
</div>
<p>We’re specifying that each work group has 1024 threads and that the entire GPU grid is split into as many work groups of 1 x 1024 threads as are required to fill our matrices with one thread per component. For example, if our state size was 2048 and our batch size 4, we’d launch a total of 4 x 2 = 8 work groups with 1024 threads each. If you are not familiar with the SYCL “work groups”, an introductory read about SYCL may help.</p>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">c10::impl::VirtualGuardImpl</span></code> must get the current stream of the current XPU device and use the XPU API to get the corresponding SYCL underlaying queue. It can then submit the kernel to the queue for execution.</p>
<section id="using-accessors">
<h4>Using accessors<a class="headerlink" href="#using-accessors" title="Link to this heading"></a></h4>
<p>You can see in the SYCL kernel that we work directly on pointers with the right type. Indeed, working directly with high level type agnostic tensors inside SYCL kernels would be very inefficient.</p>
<p>However, this comes at a cost of ease of use and readability, especially for highly dimensional data. We can use torch’s C++ utils to abstract access to high dimension data in the SYCL kernel directly.</p>
<p>The backwards pass follows much the same pattern but with the <code class="docutils literal notranslate"><span class="pre">torch::PackedTensorAccessor32</span></code>. You can get more information about these utils in torch documents:</p>
<div class="highlight-cpp notranslate"><div class="highlight"><pre><span></span><span class="k">template</span><span class="w"> </span><span class="o">&lt;</span><span class="k">typename</span><span class="w"> </span><span class="nc">scalar_t</span><span class="o">&gt;</span>
<span class="kt">void</span><span class="w"> </span><span class="n">lltm_xpu_backward_kernel</span><span class="p">(</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="o">&gt;</span><span class="w"> </span><span class="n">d_old_cell</span><span class="p">,</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">3</span><span class="o">&gt;</span><span class="w"> </span><span class="n">d_gates</span><span class="p">,</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="o">&gt;</span><span class="w"> </span><span class="n">grad_h</span><span class="p">,</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="o">&gt;</span><span class="w"> </span><span class="n">grad_cell</span><span class="p">,</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="o">&gt;</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="o">&gt;</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="o">&gt;</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="o">&gt;</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">,</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">PackedTensorAccessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">3</span><span class="o">&gt;</span><span class="w"> </span><span class="n">gate_weights</span><span class="p">,</span>
<span class="w">        </span><span class="kt">size_t</span><span class="w"> </span><span class="n">state_size</span><span class="p">,</span>
<span class="w">        </span><span class="kt">size_t</span><span class="w"> </span><span class="n">batch_size</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">threads</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">1024</span><span class="p">;</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">work_groups</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="n">state_size</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">threads</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">threads</span><span class="p">;</span>

<span class="w">  </span><span class="c1">// define the kernel</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">cgf</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="o">&amp;</span><span class="p">](</span><span class="n">sycl</span><span class="o">::</span><span class="n">handler</span><span class="o">&amp;</span><span class="w"> </span><span class="n">cgh</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">kfn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">[</span><span class="o">=</span><span class="p">](</span><span class="n">sycl</span><span class="o">::</span><span class="n">nd_item</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="w"> </span><span class="n">item</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">      </span><span class="c1">//batch index</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">n</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">item</span><span class="p">.</span><span class="n">get_group</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>
<span class="w">      </span><span class="c1">// column index</span>
<span class="w">      </span><span class="k">const</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">c</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">item</span><span class="p">.</span><span class="n">get_group</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">item</span><span class="p">.</span><span class="n">get_group_range</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">item</span><span class="p">.</span><span class="n">get_local_id</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">d_gates_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_gates</span><span class="p">;</span>
<span class="w">      </span><span class="k">auto</span><span class="w"> </span><span class="n">d_old_cell_</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_old_cell</span><span class="p">;</span>
<span class="w">      </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">c</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">d_gates</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">)){</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">d_output_gate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">tanh</span><span class="p">(</span><span class="n">new_cell</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">])</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">grad_h</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">];</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">d_tanh_new_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">output_gate</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">grad_h</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">];</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">d_new_cell</span><span class="w"> </span><span class="o">=</span>
<span class="w">                </span><span class="n">d_tanh</span><span class="p">(</span><span class="n">new_cell</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">])</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d_tanh_new_cell</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">grad_cell</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">];</span>


<span class="w">        </span><span class="n">d_old_cell_</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_new_cell</span><span class="p">;</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">d_candidate_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">input_gate</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d_new_cell</span><span class="p">;</span>
<span class="w">        </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">d_input_gate</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d_new_cell</span><span class="p">;</span>

<span class="w">        </span><span class="n">d_gates_</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">=</span>
<span class="w">                </span><span class="n">d_input_gate</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d_sigmoid</span><span class="p">(</span><span class="n">gate_weights</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="mi">0</span><span class="p">][</span><span class="n">c</span><span class="p">]);</span>
<span class="w">        </span><span class="n">d_gates_</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">=</span>
<span class="w">                </span><span class="n">d_output_gate</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d_sigmoid</span><span class="p">(</span><span class="n">gate_weights</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="mi">1</span><span class="p">][</span><span class="n">c</span><span class="p">]);</span>
<span class="w">        </span><span class="n">d_gates_</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="n">c</span><span class="p">]</span><span class="w"> </span><span class="o">=</span>
<span class="w">                </span><span class="n">d_candidate_cell</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">d_elu</span><span class="p">(</span><span class="n">gate_weights</span><span class="p">[</span><span class="n">n</span><span class="p">][</span><span class="mi">2</span><span class="p">][</span><span class="n">c</span><span class="p">]);</span>
<span class="w">      </span><span class="p">}</span>
<span class="w">    </span><span class="p">};</span>

<span class="w">    </span><span class="n">cgh</span><span class="p">.</span><span class="n">parallel_for</span><span class="p">(</span>
<span class="w">            </span><span class="n">sycl</span><span class="o">::</span><span class="n">nd_range</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(</span>
<span class="w">                    </span><span class="n">sycl</span><span class="o">::</span><span class="n">range</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(</span><span class="n">work_groups</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">threads</span><span class="p">,</span><span class="w"> </span><span class="n">batch_size</span><span class="p">),</span>
<span class="w">                    </span><span class="n">sycl</span><span class="o">::</span><span class="n">range</span><span class="o">&lt;</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(</span><span class="n">threads</span><span class="p">,</span><span class="w"> </span><span class="mi">1</span><span class="p">)),</span>
<span class="w">            </span><span class="n">kfn</span><span class="p">);</span>
<span class="w">  </span><span class="p">};</span>

<span class="w">  </span><span class="c1">// submit kernel</span>
<span class="w">  </span><span class="n">c10</span><span class="o">::</span><span class="n">xpu</span><span class="o">::</span><span class="n">XPUStream</span><span class="w"> </span><span class="n">stream</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">c10</span><span class="o">::</span><span class="n">xpu</span><span class="o">::</span><span class="n">getCurrentXPUStream</span><span class="p">();</span>
<span class="w">  </span><span class="n">stream</span><span class="p">.</span><span class="n">queue</span><span class="p">().</span><span class="n">submit</span><span class="p">(</span><span class="n">cgf</span><span class="p">);</span>
<span class="p">}</span>

<span class="n">std</span><span class="o">::</span><span class="n">vector</span><span class="o">&lt;</span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="o">&gt;</span><span class="w"> </span><span class="n">lltm_xpu_backward</span><span class="p">(</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">grad_h</span><span class="p">,</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">grad_cell</span><span class="p">,</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">new_cell</span><span class="p">,</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">input_gate</span><span class="p">,</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">output_gate</span><span class="p">,</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">candidate_cell</span><span class="p">,</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">X</span><span class="p">,</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">gates</span><span class="p">,</span>
<span class="w">        </span><span class="n">torch</span><span class="o">::</span><span class="n">Tensor</span><span class="w"> </span><span class="n">weights</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_old_cell</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">new_cell</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_gates</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">torch</span><span class="o">::</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">gates</span><span class="p">);</span>

<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">batch_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_cell</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">);</span>
<span class="w">  </span><span class="k">const</span><span class="w"> </span><span class="k">auto</span><span class="w"> </span><span class="n">state_size</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">new_cell</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">);</span>

<span class="w">  </span><span class="n">AT_DISPATCH_FLOATING_TYPES</span><span class="p">(</span><span class="n">X</span><span class="p">.</span><span class="n">type</span><span class="p">(),</span><span class="w"> </span><span class="s">&quot;lltm_backward_xpu&quot;</span><span class="p">,</span><span class="w"> </span><span class="p">([</span><span class="o">&amp;</span><span class="p">]</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="n">lltm_xpu_backward_kernel</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="o">&gt;</span><span class="p">(</span>
<span class="w">          </span><span class="n">d_old_cell</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">d_gates</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">3</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">grad_h</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">grad_cell</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">new_cell</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">input_gate</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">output_gate</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">candidate_cell</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">2</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">gates</span><span class="p">.</span><span class="n">packed_accessor32</span><span class="o">&lt;</span><span class="n">scalar_t</span><span class="p">,</span><span class="mi">3</span><span class="o">&gt;</span><span class="p">(),</span>
<span class="w">                  </span><span class="n">state_size</span><span class="p">,</span>
<span class="w">                  </span><span class="n">batch_size</span><span class="p">);</span>
<span class="w">  </span><span class="p">}));</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_gate_weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_gates</span><span class="p">.</span><span class="n">reshape</span><span class="p">({</span><span class="n">batch_size</span><span class="p">,</span><span class="w"> </span><span class="mi">3</span><span class="o">*</span><span class="n">state_size</span><span class="p">});</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_weights</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_gate_weights</span><span class="p">.</span><span class="n">t</span><span class="p">().</span><span class="n">mm</span><span class="p">(</span><span class="n">X</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_gate_weights</span><span class="p">.</span><span class="n">sum</span><span class="p">(</span><span class="cm">/*dim=*/</span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="cm">/*keepdim=*/</span><span class="nb">true</span><span class="p">);</span>

<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_X</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_gate_weights</span><span class="p">.</span><span class="n">mm</span><span class="p">(</span><span class="n">weights</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_old_h</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_X</span><span class="p">.</span><span class="n">slice</span><span class="p">(</span><span class="cm">/*dim=*/</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="mi">0</span><span class="p">,</span><span class="w"> </span><span class="n">state_size</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">d_input</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">d_X</span><span class="p">.</span><span class="n">slice</span><span class="p">(</span><span class="cm">/*dim=*/</span><span class="mi">1</span><span class="p">,</span><span class="w"> </span><span class="n">state_size</span><span class="p">);</span>

<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="p">{</span><span class="n">d_old_h</span><span class="p">,</span><span class="w"> </span><span class="n">d_input</span><span class="p">,</span><span class="w"> </span><span class="n">d_weights</span><span class="p">,</span><span class="w"> </span><span class="n">d_bias</span><span class="p">,</span><span class="w"> </span><span class="n">d_old_cell</span><span class="p">,</span><span class="w"> </span><span class="n">d_gates</span><span class="p">};</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
</section>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="DLPack.html" class="btn btn-neutral float-left" title="DLPack Solution" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="advanced_configuration.html" class="btn btn-neutral float-right" title="Advanced Configuration" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x739386079e10> 
<p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a><a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a><a href='/#' data-wap_ref='dns' id='wap_dns'><small>| Your Privacy Choices</small><span style='height:10px;width:28px;display:inline-block;position:relative;'><svg style='position:absolute;width:28px;bottom:-2px;' version='1.1' id='Layer_1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' x='0px' y='0px' viewBox='0 0 30 14' xml:space='preserve'><title>California Consumer Privacy Act (CCPA) Opt-Out Icon</title><style type='text/css'> .st0 { fill-rule: evenodd; clip-rule: evenodd; fill: #FFFFFF; } .st1 { fill-rule: evenodd; clip-rule: evenodd; fill: #0066FF; } .st2 { fill: #FFFFFF; } .st3 { fill: #0066FF; } </style><g><g id='final---dec.11-2020_1_'><g id='_x30_208-our-toggle_2_' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2_2_' transform='translate(1275.000000, 200.000000)'><path class='st0' d='M7.4,12.8h6.8l3.1-11.6H7.4C4.2,1.2,1.6,3.8,1.6,7S4.2,12.8,7.4,12.8z'></path></g></g></g><g id='final---dec.11-2020'><g id='_x30_208-our-toggle' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2' transform='translate(1275.000000, 200.000000)'><path class='st1' d='M22.6,0H7.4c-3.9,0-7,3.1-7,7s3.1,7,7,7h15.2c3.9,0,7-3.1,7-7S26.4,0,22.6,0z M1.6,7c0-3.2,2.6-5.8,5.8-5.8 h9.9l-3.1,11.6H7.4C4.2,12.8,1.6,10.2,1.6,7z'></path><path id='x' class='st2' d='M24.6,4c0.2,0.2,0.2,0.6,0,0.8l0,0L22.5,7l2.2,2.2c0.2,0.2,0.2,0.6,0,0.8c-0.2,0.2-0.6,0.2-0.8,0 l0,0l-2.2-2.2L19.5,10c-0.2,0.2-0.6,0.2-0.8,0c-0.2-0.2-0.2-0.6,0-0.8l0,0L20.8,7l-2.2-2.2c-0.2-0.2-0.2-0.6,0-0.8 c0.2-0.2,0.6-0.2,0.8,0l0,0l2.2,2.2L23.8,4C24,3.8,24.4,3.8,24.6,4z'></path><path id='y' class='st3' d='M12.7,4.1c0.2,0.2,0.3,0.6,0.1,0.8l0,0L8.6,9.8C8.5,9.9,8.4,10,8.3,10c-0.2,0.1-0.5,0.1-0.7-0.1l0,0 L5.4,7.7c-0.2-0.2-0.2-0.6,0-0.8c0.2-0.2,0.6-0.2,0.8,0l0,0L8,8.6l3.8-4.5C12,3.9,12.4,3.9,12.7,4.1z'></path></g></g></g></g></svg></span></a><a href=https://www.intel.com/content/www/us/en/privacy/privacy-residents-certain-states.html data-wap_ref='nac' id='wap_nac'><small>| Notice at Collection</small></a></div><p></p><div>&copy; Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others. No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (OBSD), <a href='http://opensource.org/licenses/0BSD'>http://opensource.org/licenses/0BSD</a>.</div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>