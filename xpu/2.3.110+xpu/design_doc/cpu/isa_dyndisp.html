<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="../../">
<head>
  <meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Intel® Extension for PyTorch* CPU ISA Dynamic Dispatch Design Doc &mdash; Intel&amp;#174 Extension for PyTorch* 2.3.110+xpu documentation</title>
      <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=80d5e7a1" />
      <link rel="stylesheet" type="text/css" href="../../_static/css/theme.css?v=19f00094" />
      <link rel="stylesheet" type="text/css" href="../../_static/custom.css?v=a95c1af4" />

  
<script type="text/javascript">
  // Configure TMS settings
  window.wapProfile = 'profile-microsite'; // This is mapped by WAP authorize value
  window.wapLocalCode = 'us-en'; // Dynamically set per localized site, see mapping table for values
  window.wapSection = "intel-extension-for-pytorch"; // WAP team will give you a unique section for your site
  window.wapEnv = 'prod'; // environment to be use in Adobe Tags.
  // Load TMS
  (() => {
        let url = 'https://www.intel.com/content/dam/www/global/wap/main/wap-microsite.js';
        let po = document.createElement('script'); po.type = 'text/javascript'; po.async = true; po.src = url;
        let s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  }) ();
</script>

    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../index.html" class="icon icon-home">
            Intel&#174 Extension for PyTorch*
          </a>
              <div class="version">
                <a href="../../../../">2.3.110+xpu ▼</a>
                <p>Click link above to switch version</p>
              </div>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">ABOUT</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/introduction.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/features.html">Features</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/llm.html">Large Language Models (LLM)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/performance.html">Performance</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/technical_details.html">Technical Details</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/releases.html">Releases</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/known_issues.html">Troubleshooting</a></li>
<li class="toctree-l1"><a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/blogs.html">Blogs &amp; Publications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/license.html">License</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">GET STARTED</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://intel.github.io/intel-extension-for-pytorch/index.html#installation?platform=gpu&amp;version=v2.3.110%2bxpu">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/getting_started.html">Quick Start</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/examples.html">Examples</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">DEVELOPER REFERENCE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/api_doc.html">API Documentation</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">CONTRIBUTING GUIDE</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../tutorials/contribution.html">Contribution</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">Intel&#174 Extension for PyTorch*</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="../../index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Intel® Extension for PyTorch* CPU ISA Dynamic Dispatch Design Doc</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../_sources/design_doc/cpu/isa_dyndisp.md.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="intel-extension-for-pytorch-cpu-isa-dynamic-dispatch-design-doc">
<h1>Intel® Extension for PyTorch* CPU ISA Dynamic Dispatch Design Doc<a class="headerlink" href="#intel-extension-for-pytorch-cpu-isa-dynamic-dispatch-design-doc" title="Link to this heading"></a></h1>
<p>This document explains the dynamic kernel dispatch mechanism for Intel® Extension for PyTorch* (IPEX) based on CPU ISA. It is an extension to the similar mechanism in PyTorch.</p>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Link to this heading"></a></h2>
<p>IPEX dyndisp is forked from <strong>PyTorch:</strong> <code class="docutils literal notranslate"><span class="pre">ATen/native/DispatchStub.h</span></code> and <code class="docutils literal notranslate"><span class="pre">ATen/native/DispatchStub.cpp</span></code>. IPEX adds additional CPU ISA level support, such as <code class="docutils literal notranslate"><span class="pre">AVX512_VNNI</span></code>, <code class="docutils literal notranslate"><span class="pre">AVX512_BF16</span></code> and <code class="docutils literal notranslate"><span class="pre">AMX</span></code>.</p>
<p>PyTorch &amp; IPEX CPU ISA support statement:</p>
<table border="1" class="docutils">
<thead>
<tr>
<th></th>
<th>DEFAULT</th>
<th>AVX2</th>
<th>AVX2_VNNI</th>
<th>AVX512</th>
<th>AVX512_VNNI</th>
<th>AVX512_BF16</th>
<th>AMX</th>
<th>AVX512_FP16</th>
</tr>
</thead>
<tbody>
<tr>
<td>PyTorch</td>
<td>✔</td>
<td>✔</td>
<td>✘</td>
<td>✔</td>
<td>✘</td>
<td>✘</td>
<td>✘</td>
<td>✘</td>
</tr>
<tr>
<td>IPEX-1.11</td>
<td>✘</td>
<td>✔</td>
<td>✘</td>
<td>✔</td>
<td>✘</td>
<td>✘</td>
<td>✘</td>
<td>✘</td>
</tr>
<tr>
<td>IPEX-1.12</td>
<td>✘</td>
<td>✔</td>
<td>✘</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✘</td>
</tr>
<tr>
<td>IPEX-1.13</td>
<td>✘</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✘</td>
</tr>
<tr>
<td>IPEX-2.1</td>
<td>✘</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
</tr>
<tr>
<td>IPEX-2.2</td>
<td>✘</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
<td>✔</td>
</tr>
</tbody>
</table><p>* Current IPEX DEFAULT level implemented as same as AVX2 level.</p>
<section id="cpu-isa-build-compiler-requirement">
<h3>CPU ISA build compiler requirement<a class="headerlink" href="#cpu-isa-build-compiler-requirement" title="Link to this heading"></a></h3>
<p>| ISA Level | GCC requirement |
| —- | —- |
| AVX2 | Any |
| AVX512 | GCC 9.2+ |
| AVX512_VNNI | GCC 9.2+ |
| AVX512_BF16 | GCC 10.3+ |
| AVX2_VNNI | GCC 11.2+ |
| AMX | GCC 11.2+ |
| AVX512_FP16 | GCC 12.1+ |</p>
<p>* Check with <code class="docutils literal notranslate"><span class="pre">cmake/Modules/FindAVX.cmake</span></code> for detailed compiler checks.</p>
</section>
</section>
<section id="dynamic-dispatch-design">
<h2>Dynamic Dispatch Design<a class="headerlink" href="#dynamic-dispatch-design" title="Link to this heading"></a></h2>
<p>Dynamic dispatch copies the kernel implementation source files to multiple folders for each ISA level. It then builds each file using its ISA specific parameters. Each generated object file will contain its function body (<strong>Kernel Implementation</strong>).</p>
<p>Kernel Implementation uses an anonymous namespace so that different CPU versions won’t conflict.</p>
<p><strong>Kernel Stub</strong> is a “virtual function” with polymorphic kernel implementations pertaining to ISA levels.</p>
<p>At the runtime, <strong>Dispatch Stub implementation</strong> will check CPUIDs and OS status to determins which ISA level pointer best matches the function body.</p>
<section id="code-folder-struct">
<h3>Code Folder Struct<a class="headerlink" href="#code-folder-struct" title="Link to this heading"></a></h3>
<section id="kernel-implementation-csrc-cpu-aten-kernels-xyzkrnl-cpp">
<h4><strong>Kernel implementation:</strong> <code class="docutils literal notranslate"><span class="pre">csrc/cpu/aten/kernels/xyzKrnl.cpp</span></code><a class="headerlink" href="#kernel-implementation-csrc-cpu-aten-kernels-xyzkrnl-cpp" title="Link to this heading"></a></h4>
</section>
<section id="kernel-stub-csrc-cpu-aten-xyz-cpp-and-csrc-cpu-aten-xyz-h">
<h4><strong>Kernel Stub:</strong> <code class="docutils literal notranslate"><span class="pre">csrc/cpu/aten/xyz.cpp</span></code> and <code class="docutils literal notranslate"><span class="pre">csrc/cpu/aten/xyz.h</span></code><a class="headerlink" href="#kernel-stub-csrc-cpu-aten-xyz-cpp-and-csrc-cpu-aten-xyz-h" title="Link to this heading"></a></h4>
</section>
<section id="dispatch-stub-implementation-csrc-cpu-dyndisp-dispatchstub-cpp-and-csrc-cpu-dyndisp-dispatchstub-h">
<h4><strong>Dispatch Stub implementation:</strong> <code class="docutils literal notranslate"><span class="pre">csrc/cpu/dyndisp/DispatchStub.cpp</span></code> and <code class="docutils literal notranslate"><span class="pre">csrc/cpu/dyndisp/DispatchStub.h</span></code><a class="headerlink" href="#dispatch-stub-implementation-csrc-cpu-dyndisp-dispatchstub-cpp-and-csrc-cpu-dyndisp-dispatchstub-h" title="Link to this heading"></a></h4>
</section>
</section>
<section id="codegen-process">
<h3>CodeGen Process<a class="headerlink" href="#codegen-process" title="Link to this heading"></a></h3>
<p>IPEX build system will generate code for each ISA level with specifiy complier parameters. The CodeGen script is located at <code class="docutils literal notranslate"><span class="pre">cmake/cpu/IsaCodegen.cmake</span></code>.</p>
<p>The CodeGen will copy each cpp files from <strong>Kernel implementation</strong>, and then add ISA level as new file suffix.</p>
<blockquote>
<div><p><strong>Sample:</strong></p>
<hr class="docutils" />
<p><strong>Origin file:</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">csrc/cpu/aten/kernels/AdaptiveAveragePoolingKrnl.cpp</span></code></p>
<p><strong>Generate files:</strong></p>
<p>DEFAULT: <code class="docutils literal notranslate"><span class="pre">build/Release/csrc/isa_codegen/cpu/aten/kernels/AdaptiveAveragePoolingKrnl.cpp.DEFAULT.cpp</span> <span class="pre">-O3</span> <span class="pre">-D__AVX__</span> <span class="pre">-DCPU_CAPABILITY_AVX2</span> <span class="pre">-mavx2</span> <span class="pre">-mfma</span> <span class="pre">-mno-avx256-split-unaligned-load</span> <span class="pre">-mno-avx256-split-unaligned-store</span> <span class="pre">-DCPU_CAPABILITY=DEFAULT</span> <span class="pre">-DCPU_CAPABILITY_DEFAULT</span></code></p>
<p>AVX2: <code class="docutils literal notranslate"><span class="pre">build/Release/csrc/isa_codegen/cpu/aten/kernels/AdaptiveAveragePoolingKrnl.cpp.AVX2.cpp</span> <span class="pre">-O3</span> <span class="pre">-D__AVX__</span> <span class="pre">-mavx2</span> <span class="pre">-mfma</span> <span class="pre">-mno-avx256-split-unaligned-load</span> <span class="pre">-mno-avx256-split-unaligned-store</span> <span class="pre">-DCPU_CAPABILITY=AVX2</span> <span class="pre">-DCPU_CAPABILITY_AVX2</span></code></p>
<p>AVX512: <code class="docutils literal notranslate"><span class="pre">build/Release/csrc/isa_codegen/cpu/aten/kernels/AdaptiveAveragePoolingKrnl.cpp.AVX512.cpp</span> <span class="pre">-O3</span> <span class="pre">-D__AVX512F__</span> <span class="pre">-mavx512f</span> <span class="pre">-mavx512bw</span> <span class="pre">-mavx512vl</span> <span class="pre">-mavx512dq</span> <span class="pre">-mfma</span> <span class="pre">-DCPU_CAPABILITY=AVX512</span> <span class="pre">-DCPU_CAPABILITY_AVX512</span></code></p>
<p>AVX512_VNNI: <code class="docutils literal notranslate"><span class="pre">build/Release/csrc/isa_codegen/cpu/aten/kernels/AdaptiveAveragePoolingKrnl.cpp.AVX512_VNNI.cpp</span> <span class="pre">-O3</span> <span class="pre">-D__AVX512F__</span> <span class="pre">-DCPU_CAPABILITY_AVX512</span> <span class="pre">-mavx512f</span> <span class="pre">-mavx512bw</span> <span class="pre">-mavx512vl</span> <span class="pre">-mavx512dq</span> <span class="pre">-mavx512vnni</span> <span class="pre">-mfma</span> <span class="pre">-DCPU_CAPABILITY=AVX512_VNNI</span> <span class="pre">-DCPU_CAPABILITY_AVX512_VNNI</span></code></p>
<p>AVX512_BF16: <code class="docutils literal notranslate"><span class="pre">build/Release/csrc/isa_codegen/cpu/aten/kernels/AdaptiveAveragePoolingKrnl.cpp.AVX512_BF16.cpp</span> <span class="pre">-O3</span> <span class="pre">-D__AVX512F__</span> <span class="pre">-DCPU_CAPABILITY_AVX512</span> <span class="pre">-DCPU_CAPABILITY_AVX512_VNNI</span> <span class="pre">-mavx512f</span> <span class="pre">-mavx512bw</span> <span class="pre">-mavx512vl</span> <span class="pre">-mavx512dq</span> <span class="pre">-mavx512vnni</span> <span class="pre">-mavx512bf16</span> <span class="pre">-mfma</span> <span class="pre">-DCPU_CAPABILITY=AVX512_BF16</span> <span class="pre">-DCPU_CAPABILITY_AVX512_BF16</span></code></p>
<p>AMX: <code class="docutils literal notranslate"><span class="pre">build/Release/csrc/isa_codegen/cpu/aten/kernels/AdaptiveAveragePoolingKrnl.cpp.AMX.cpp</span> <span class="pre">-O3</span>&#160; <span class="pre">-D__AVX512F__</span> <span class="pre">-DCPU_CAPABILITY_AVX512</span> <span class="pre">-DCPU_CAPABILITY_AVX512_VNNI</span> <span class="pre">-DCPU_CAPABILITY_AVX512_BF16</span> <span class="pre">-mavx512f</span> <span class="pre">-mavx512bw</span> <span class="pre">-mavx512vl</span> <span class="pre">-mavx512dq</span> <span class="pre">-mavx512vnni</span> <span class="pre">-mavx512bf16</span> <span class="pre">-mfma</span> <span class="pre">-mamx-tile</span> <span class="pre">-mamx-int8</span> <span class="pre">-mamx-bf16</span> <span class="pre">-DCPU_CAPABILITY=AMX</span> <span class="pre">-DCPU_CAPABILITY_AMX</span></code></p>
<p>AVX512_FP16: <code class="docutils literal notranslate"><span class="pre">build/Release/csrc/isa_codegen/cpu/aten/kernels/AdaptiveAveragePoolingKrnl.cpp.AVX512_FP16.cpp</span> <span class="pre">-O3</span>&#160; <span class="pre">-D__AVX512F__</span> <span class="pre">-DCPU_CAPABILITY_AVX512</span> <span class="pre">-DCPU_CAPABILITY_AVX512_VNNI</span> <span class="pre">-DCPU_CAPABILITY_AVX512_BF16</span> <span class="pre">-mavx512f</span> <span class="pre">-mavx512bw</span> <span class="pre">-mavx512vl</span> <span class="pre">-mavx512dq</span> <span class="pre">-mavx512vnni</span> <span class="pre">-mavx512bf16</span> <span class="pre">-mfma</span> <span class="pre">-mamx-tile</span> <span class="pre">-mamx-int8</span> <span class="pre">-mamx-bf16</span> <span class="pre">-mavx512fp16</span> <span class="pre">-DCPU_CAPABILITY_AMX</span> <span class="pre">-DCPU_CAPABILITY=AVX512_FP16</span> <span class="pre">-DCPU_CAPABILITY_AVX512_FP16</span></code></p>
</div></blockquote>
<hr class="docutils" />
<blockquote>
<div><p><strong>Note:</strong></p>
<ol class="simple">
<li><p>DEFAULT level kernels is not fully implemented in IPEX. In order to align to PyTorch, we build default use AVX2 parameters in stead of that. So, IPEX minimal required executing machine support AVX2.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-D__AVX__</span></code> and <code class="docutils literal notranslate"><span class="pre">-D__AVX512F__</span></code> is defined for depends library <a class="reference external" href="https://sleef.org/">sleef</a> .</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-DCPU_CAPABILITY_AVX512</span></code> and <code class="docutils literal notranslate"><span class="pre">-DCPU_CAPABILITY_AVX2</span></code> are must to be defined for <strong>PyTorch:</strong> <code class="docutils literal notranslate"><span class="pre">aten/src/ATen/cpu/vec</span></code>, it determins vec register width.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">-DCPU_CAPABILITY=[ISA_NAME]</span></code> is must to be defined for <strong>PyTorch:</strong> <code class="docutils literal notranslate"><span class="pre">aten/src/ATen/cpu/vec</span></code>, it is used as inline namespace name.</p></li>
<li><p>Higher ISA level is compatible to lower ISA levels, so it needs to contains level ISA feature definitions. Such as AVX512_BF16 need contains <code class="docutils literal notranslate"><span class="pre">-DCPU_CAPABILITY_AVX512</span></code> <code class="docutils literal notranslate"><span class="pre">-DCPU_CAPABILITY_AVX512_VNNI</span></code>. But AVX512 don’t contains AVX2 definitions, due to there are different vec register width.</p></li>
</ol>
</div></blockquote>
</section>
</section>
<section id="add-custom-kernel">
<h2>Add Custom Kernel<a class="headerlink" href="#add-custom-kernel" title="Link to this heading"></a></h2>
<p>If you want to add a new custom kernel, and the kernel uses CPU ISA instructions, refer to these tips:</p>
<ol class="simple">
<li><p>Add CPU ISA related kernel implementation to the folder:  <code class="docutils literal notranslate"><span class="pre">csrc/cpu/aten/kernels/NewKernelKrnl.cpp</span></code></p></li>
<li><p>Add kernel stub to the folder: <code class="docutils literal notranslate"><span class="pre">csrc/cpu/aten/NewKernel.cpp</span></code></p></li>
<li><p>Include header file: <code class="docutils literal notranslate"><span class="pre">csrc/cpu/dyndisp/DispatchStub.h</span></code>, and reference to the comment in the header file.</p></li>
</ol>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">// Implements instruction set specific function dispatch.</span>
<span class="c1">//</span>
<span class="c1">// Kernels that may make use of specialized instruction sets (e.g. AVX2) are</span>
<span class="c1">// compiled multiple times with different compiler flags (e.g. -mavx2). A</span>
<span class="c1">// DispatchStub contains a table of function pointers for a kernel. At runtime,</span>
<span class="c1">// the fastest available kernel is chosen based on the features reported by</span>
<span class="c1">// cpuinfo.</span>
<span class="c1">//</span>
<span class="c1">// Example:</span>
<span class="c1">//</span>
<span class="c1">// In csrc/cpu/aten/MyKernel.h:</span>
<span class="c1">//   using fn_type = void(*)(const Tensor&amp; x);</span>
<span class="c1">//   IPEX_DECLARE_DISPATCH(fn_type, stub);</span>
<span class="c1">//</span>
<span class="c1">// In csrc/cpu/aten/MyKernel.cpp</span>
<span class="c1">//   IPEX_DEFINE_DISPATCH(stub);</span>
<span class="c1">//</span>
<span class="c1">// In csrc/cpu/aten/kernels/MyKernel.cpp:</span>
<span class="c1">//   namespace {</span>
<span class="c1">//     // use anonymous namespace so that different cpu versions won&#39;t conflict</span>
<span class="c1">//     void kernel(const Tensor&amp; x) { ... }</span>
<span class="c1">//   }</span>
<span class="c1">//   IPEX_REGISTER_DISPATCH(stub, &amp;kernel);</span>
<span class="c1">//</span>
<span class="c1">// To call:</span>
<span class="c1">//   stub(kCPU, tensor);</span>
</pre></div>
</div>
<ol class="simple">
<li><p>Write the kernel follow the guide. It contains: declare function type, register stub, call stub, etc.</p></li>
</ol>
<blockquote>
<div><p><strong>Note:</strong></p>
<ol class="simple">
<li><p>Some kernels only call <strong>oneDNN</strong> or <strong>iDeep</strong> implementation, or other backend implementation, which is not needed to add kernel implementations. (Refer: <code class="docutils literal notranslate"><span class="pre">BatchNorm.cpp</span></code>)</p></li>
<li><p>Vec related header file must be included in kernel implementation files, but can not be included in kernel stub. Kernel stub is common code for all ISA level, and can’t pass ISA related compiler parameters.</p></li>
<li><p>For more intrinsics, check the <a class="reference external" href="https://www.intel.com/content/www/us/en/docs/intrinsics-guide/index.html">Intel® Intrinsics Guide</a>.</p></li>
</ol>
</div></blockquote>
<section id="isa-intrinics-specific-kernel-example">
<h3>ISA intrinics specific kernel example:<a class="headerlink" href="#isa-intrinics-specific-kernel-example" title="Link to this heading"></a></h3>
<p>This is a FP32 convert to BF16 function example, and it is implemented for <code class="docutils literal notranslate"><span class="pre">AVX512_BF16</span></code>, <code class="docutils literal notranslate"><span class="pre">AVX512</span></code> and <code class="docutils literal notranslate"><span class="pre">DEFAULT</span></code> ISA levels.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">//csrc/cpu/aten/CvtFp32ToBf16.h</span>

<span class="cp">#pragma once</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;dyndisp/DispatchStub.h&gt;</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">torch_ipex</span><span class="w"> </span><span class="p">{</span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">cpu</span><span class="w"> </span><span class="p">{</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">cvt_fp32_to_bf16</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">BFloat16</span><span class="o">*</span><span class="w"> </span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">len</span><span class="p">);</span>

<span class="k">namespace</span><span class="w"> </span><span class="p">{</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">cvt_fp32_to_bf16_kernel_impl</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">BFloat16</span><span class="o">*</span><span class="w"> </span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">len</span><span class="p">);</span>

<span class="p">}</span>

<span class="k">using</span><span class="w"> </span><span class="n">cvt_fp32_to_bf16_kernel_fn</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kt">void</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="p">)(</span><span class="n">at</span><span class="o">::</span><span class="n">BFloat16</span><span class="o">*</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="p">);</span>
<span class="n">IPEX_DECLARE_DISPATCH</span><span class="p">(</span><span class="n">cvt_fp32_to_bf16_kernel_fn</span><span class="p">,</span><span class="w"> </span><span class="n">cvt_fp32_to_bf16_kernel_stub</span><span class="p">);</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace cpu</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace torch_ipex</span>
</pre></div>
</div>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">//csrc/cpu/aten/CvtFp32ToBf16.cpp</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;CvtFp32ToBf16.h&quot;</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">torch_ipex</span><span class="w"> </span><span class="p">{</span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">cpu</span><span class="w"> </span><span class="p">{</span>

<span class="n">IPEX_DEFINE_DISPATCH</span><span class="p">(</span><span class="n">cvt_fp32_to_bf16_kernel_stub</span><span class="p">);</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">cvt_fp32_to_bf16</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">BFloat16</span><span class="o">*</span><span class="w"> </span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="w"> </span><span class="n">len</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">cvt_fp32_to_bf16_kernel_stub</span><span class="p">(</span><span class="n">kCPU</span><span class="p">,</span><span class="w"> </span><span class="n">dst</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">len</span><span class="p">);</span>
<span class="p">}</span>

<span class="p">}</span><span class="w"> </span><span class="c1">// namespace cpu</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace torch_ipex</span>
</pre></div>
</div>
<p>Macro <code class="docutils literal notranslate"><span class="pre">CPU_CAPABILITY_AVX512</span></code> and <code class="docutils literal notranslate"><span class="pre">CPU_CAPABILITY_AVX512_BF16</span></code> are defined by compiler check, it is means that current compiler havs capability to generate defined ISA level code.</p>
<p>Because of <code class="docutils literal notranslate"><span class="pre">AVX512_BF16</span></code> is higher level than <code class="docutils literal notranslate"><span class="pre">AVX512</span></code>, and it compatible to <code class="docutils literal notranslate"><span class="pre">AVX512</span></code>. <code class="docutils literal notranslate"><span class="pre">CPU_CAPABILITY_AVX512_BF16</span></code> can be contained in <code class="docutils literal notranslate"><span class="pre">CPU_CAPABILITY_AVX512</span></code> region.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">//csrc/cpu/aten/kernels/CvtFp32ToBf16Krnl.cpp</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/cpu/vec/vec.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;csrc/aten/cpu/CvtFp32ToBf16.h&quot;</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">torch_ipex</span><span class="w"> </span><span class="p">{</span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">cpu</span><span class="w"> </span><span class="p">{</span>

<span class="k">namespace</span><span class="w"> </span><span class="p">{</span>

<span class="cp">#if defined(CPU_CAPABILITY_AVX512)</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/cpu/vec/vec512/vec512.h&gt;</span>
<span class="cp">#else</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/cpu/vec/vec256/vec256.h&gt;</span>
<span class="cp">#endif</span>
<span class="k">using</span><span class="w"> </span><span class="k">namespace</span><span class="w"> </span><span class="nn">at</span><span class="o">::</span><span class="nn">vec</span><span class="p">;</span>

<span class="cp">#if defined(CPU_CAPABILITY_AVX512)</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;immintrin.h&gt;</span>

<span class="kr">inline</span><span class="w"> </span><span class="n">__m256i</span><span class="w"> </span><span class="nf">_cvt_fp32_to_bf16</span><span class="p">(</span><span class="k">const</span><span class="w"> </span><span class="n">__m512</span><span class="w"> </span><span class="n">src</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="cp">#if (defined CPU_CAPABILITY_AVX512_BF16) </span><span class="c1">// AVX512_BF16 ISA implementation.</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="k">reinterpret_cast</span><span class="o">&lt;</span><span class="n">__m256i</span><span class="o">&gt;</span><span class="p">(</span><span class="n">_mm512_cvtneps_pbh</span><span class="p">(</span><span class="n">src</span><span class="p">));</span>
<span class="cp">#else  </span><span class="c1">// AVX512 ISA implementation.</span>
<span class="w">  </span><span class="n">__m512i</span><span class="w"> </span><span class="n">value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm512_castps_si512</span><span class="p">(</span><span class="n">src</span><span class="p">);</span>
<span class="w">  </span><span class="n">__m512i</span><span class="w"> </span><span class="n">nan</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm512_set1_epi32</span><span class="p">(</span><span class="mh">0xffff</span><span class="p">);</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">mask_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm512_cmp_ps_mask</span><span class="p">(</span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="p">,</span><span class="w"> </span><span class="n">_CMP_ORD_Q</span><span class="p">);</span>
<span class="w">  </span><span class="n">__m512i</span><span class="w"> </span><span class="n">ones</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm512_set1_epi32</span><span class="p">(</span><span class="mh">0x1</span><span class="p">);</span>
<span class="w">  </span><span class="n">__m512i</span><span class="w"> </span><span class="n">vec_bias</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm512_set1_epi32</span><span class="p">(</span><span class="mh">0x7fff</span><span class="p">);</span>
<span class="w">  </span><span class="c1">// uint32_t lsb = (input &gt;&gt; 16) &amp; 1;</span>
<span class="w">  </span><span class="k">auto</span><span class="w"> </span><span class="n">t_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm512_and_si512</span><span class="p">(</span><span class="n">_mm512_srli_epi32</span><span class="p">(</span><span class="n">value</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">),</span><span class="w"> </span><span class="n">ones</span><span class="p">);</span>
<span class="w">  </span><span class="c1">// uint32_t rounding_bias = 0x7fff + lsb;</span>
<span class="w">  </span><span class="n">t_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm512_add_epi32</span><span class="p">(</span><span class="n">t_value</span><span class="p">,</span><span class="w"> </span><span class="n">vec_bias</span><span class="p">);</span>
<span class="w">  </span><span class="c1">// input += rounding_bias;</span>
<span class="w">  </span><span class="n">t_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm512_add_epi32</span><span class="p">(</span><span class="n">t_value</span><span class="p">,</span><span class="w"> </span><span class="n">value</span><span class="p">);</span>
<span class="w">  </span><span class="c1">// input = input &gt;&gt; 16;</span>
<span class="w">  </span><span class="n">t_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm512_srli_epi32</span><span class="p">(</span><span class="n">t_value</span><span class="p">,</span><span class="w"> </span><span class="mi">16</span><span class="p">);</span>
<span class="w">  </span><span class="c1">// Check NaN before converting back to bf16</span>
<span class="w">  </span><span class="n">t_value</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm512_mask_blend_epi32</span><span class="p">(</span><span class="n">mask_value</span><span class="p">,</span><span class="w"> </span><span class="n">nan</span><span class="p">,</span><span class="w"> </span><span class="n">t_value</span><span class="p">);</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">_mm512_cvtusepi32_epi16</span><span class="p">(</span><span class="n">t_value</span><span class="p">);</span>
<span class="cp">#endif</span>
<span class="p">}</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">cvt_fp32_to_bf16_kernel_impl</span><span class="p">(</span>
<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">BFloat16</span><span class="o">*</span><span class="w"> </span><span class="n">dst</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">src</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">len</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="kt">int</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">len</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">15</span><span class="p">;</span><span class="w"> </span><span class="n">i</span><span class="w"> </span><span class="o">+=</span><span class="w"> </span><span class="mi">16</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">f32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm512_loadu_ps</span><span class="p">(</span><span class="n">src</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">);</span>
<span class="w">    </span><span class="n">_mm256_storeu_si256</span><span class="p">((</span><span class="n">__m256i</span><span class="o">*</span><span class="p">)(</span><span class="n">dst</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">),</span><span class="w"> </span><span class="n">_cvt_fp32_to_bf16</span><span class="p">(</span><span class="n">f32</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>
<span class="w">  </span><span class="k">if</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">len</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">mask</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="p">(</span><span class="mi">1</span><span class="w"> </span><span class="o">&lt;&lt;</span><span class="w"> </span><span class="p">(</span><span class="n">len</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">i</span><span class="p">))</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="mi">1</span><span class="p">;</span>
<span class="w">    </span><span class="k">auto</span><span class="w"> </span><span class="n">f32</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">_mm512_maskz_loadu_ps</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="n">src</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">);</span>
<span class="w">    </span><span class="n">_mm256_mask_storeu_epi16</span><span class="p">(</span><span class="n">dst</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">i</span><span class="p">,</span><span class="w"> </span><span class="n">mask</span><span class="p">,</span><span class="w"> </span><span class="n">_cvt_fp32_to_bf16</span><span class="p">(</span><span class="n">f32</span><span class="p">));</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="cp">#else </span><span class="c1">// DEFAULT ISA implementation.</span>

<span class="kt">void</span><span class="w"> </span><span class="nf">cvt_fp32_to_bf16_kernel_impl</span><span class="p">(</span>
<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">BFloat16</span><span class="o">*</span><span class="w"> </span><span class="n">dst</span><span class="p">,</span>
<span class="w">    </span><span class="k">const</span><span class="w"> </span><span class="kt">float</span><span class="o">*</span><span class="w"> </span><span class="n">src</span><span class="p">,</span>
<span class="w">    </span><span class="kt">int</span><span class="w"> </span><span class="n">len</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="kt">int</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="mi">0</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="w"> </span><span class="o">&lt;</span><span class="w"> </span><span class="n">len</span><span class="p">;</span><span class="w"> </span><span class="n">j</span><span class="o">++</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="o">*</span><span class="p">(</span><span class="n">dst</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">j</span><span class="p">)</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="o">*</span><span class="p">(</span><span class="n">src</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">j</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="cp">#endif</span>

<span class="p">}</span><span class="w"> </span><span class="c1">// anonymous namespace</span>

<span class="n">IPEX_REGISTER_DISPATCH</span><span class="p">(</span><span class="n">cvt_fp32_to_bf16_kernel_stub</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">cvt_fp32_to_bf16_kernel_impl</span><span class="p">);</span>

<span class="p">}</span><span class="w"> </span><span class="c1">// namespace cpu</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace torch_ipex</span>
</pre></div>
</div>
</section>
<section id="vec-specific-kernel-example">
<h3>Vec specific kernel example:<a class="headerlink" href="#vec-specific-kernel-example" title="Link to this heading"></a></h3>
<p>This example shows how to get the data type size and its Vec size. In different ISA, Vec has a different register width and a different Vec size.</p>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">//csrc/cpu/aten/GetVecLength.h</span>
<span class="cp">#pragma once</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;dyndisp/DispatchStub.h&gt;</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">torch_ipex</span><span class="w"> </span><span class="p">{</span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">cpu</span><span class="w"> </span><span class="p">{</span>

<span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">get_cpp_typesize_and_vecsize</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>

<span class="k">namespace</span><span class="w"> </span><span class="p">{</span>

<span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">get_cpp_typesize_and_vecsize_kernel_impl</span><span class="p">(</span>
<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="k">using</span><span class="w"> </span><span class="n">get_cpp_typesize_and_vecsize_kernel_fn</span><span class="w"> </span><span class="o">=</span>
<span class="w">    </span><span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="p">(</span><span class="o">*</span><span class="p">)(</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="p">);</span>
<span class="n">IPEX_DECLARE_DISPATCH</span><span class="p">(</span>
<span class="w">    </span><span class="n">get_cpp_typesize_and_vecsize_kernel_fn</span><span class="p">,</span>
<span class="w">    </span><span class="n">get_cpp_typesize_and_vecsize_kernel_stub</span><span class="p">);</span>

<span class="p">}</span><span class="w"> </span><span class="c1">// namespace cpu</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace torch_ipex</span>
</pre></div>
</div>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">//csrc/cpu/aten/GetVecLength.cpp</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;GetVecLength.h&quot;</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">torch_ipex</span><span class="w"> </span><span class="p">{</span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">cpu</span><span class="w"> </span><span class="p">{</span>

<span class="n">IPEX_DEFINE_DISPATCH</span><span class="p">(</span><span class="n">get_cpp_typesize_and_vecsize_kernel_stub</span><span class="p">);</span>

<span class="c1">// get cpp typesize and vectorsize by at::ScalarType</span>
<span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">get_cpp_typesize_and_vecsize</span><span class="p">(</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">return</span><span class="w"> </span><span class="n">get_cpp_typesize_and_vecsize_kernel_stub</span><span class="p">(</span><span class="n">kCPU</span><span class="p">,</span><span class="w"> </span><span class="n">dtype</span><span class="p">);</span>
<span class="p">}</span>

<span class="p">}</span><span class="w"> </span><span class="c1">// namespace cpu</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace torch_ipex</span>
</pre></div>
</div>
<div class="highlight-c++ notranslate"><div class="highlight"><pre><span></span><span class="c1">//csrc/cpu/aten/kernels/GetVecLengthKrnl.cpp</span>

<span class="cp">#include</span><span class="w"> </span><span class="cpf">&lt;ATen/cpu/vec/vec.h&gt;</span>
<span class="cp">#include</span><span class="w"> </span><span class="cpf">&quot;csrc/cpu/aten/GetVecLength.h&quot;</span>

<span class="k">namespace</span><span class="w"> </span><span class="nn">torch_ipex</span><span class="w"> </span><span class="p">{</span>
<span class="k">namespace</span><span class="w"> </span><span class="nn">cpu</span><span class="w"> </span><span class="p">{</span>

<span class="k">namespace</span><span class="w"> </span><span class="p">{</span>

<span class="n">std</span><span class="o">::</span><span class="n">tuple</span><span class="o">&lt;</span><span class="kt">int</span><span class="p">,</span><span class="w"> </span><span class="kt">int</span><span class="o">&gt;</span><span class="w"> </span><span class="n">get_cpp_typesize_and_vecsize_kernel_impl</span><span class="p">(</span>
<span class="w">    </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="w"> </span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">  </span><span class="k">switch</span><span class="w"> </span><span class="p">(</span><span class="n">dtype</span><span class="p">)</span><span class="w"> </span><span class="p">{</span>
<span class="w">    </span><span class="k">case</span><span class="w"> </span><span class="no">at</span><span class="o">::</span><span class="no">ScalarType</span><span class="o">::</span><span class="no">Double</span><span class="p">:</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_tuple</span><span class="p">(</span>
<span class="w">          </span><span class="k">sizeof</span><span class="p">(</span><span class="kt">double</span><span class="p">),</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">vec</span><span class="o">::</span><span class="n">Vectorized</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;::</span><span class="n">size</span><span class="p">());</span>
<span class="w">    </span><span class="k">case</span><span class="w"> </span><span class="no">at</span><span class="o">::</span><span class="no">ScalarType</span><span class="o">::</span><span class="no">Float</span><span class="p">:</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_tuple</span><span class="p">(</span><span class="k">sizeof</span><span class="p">(</span><span class="kt">float</span><span class="p">),</span><span class="w"> </span><span class="n">at</span><span class="o">::</span><span class="n">vec</span><span class="o">::</span><span class="n">Vectorized</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;::</span><span class="n">size</span><span class="p">());</span>
<span class="w">    </span><span class="k">case</span><span class="w"> </span><span class="no">at</span><span class="o">::</span><span class="no">ScalarType</span><span class="o">::</span><span class="no">ComplexDouble</span><span class="p">:</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_tuple</span><span class="p">(</span>
<span class="w">          </span><span class="k">sizeof</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;</span><span class="p">),</span>
<span class="w">          </span><span class="n">at</span><span class="o">::</span><span class="n">vec</span><span class="o">::</span><span class="n">Vectorized</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="kt">double</span><span class="o">&gt;&gt;::</span><span class="n">size</span><span class="p">());</span>
<span class="w">    </span><span class="k">case</span><span class="w"> </span><span class="no">at</span><span class="o">::</span><span class="no">ScalarType</span><span class="o">::</span><span class="no">ComplexFloat</span><span class="p">:</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_tuple</span><span class="p">(</span>
<span class="w">          </span><span class="k">sizeof</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;</span><span class="p">),</span>
<span class="w">          </span><span class="n">at</span><span class="o">::</span><span class="n">vec</span><span class="o">::</span><span class="n">Vectorized</span><span class="o">&lt;</span><span class="n">c10</span><span class="o">::</span><span class="n">complex</span><span class="o">&lt;</span><span class="kt">float</span><span class="o">&gt;&gt;::</span><span class="n">size</span><span class="p">());</span>
<span class="w">    </span><span class="k">case</span><span class="w"> </span><span class="no">at</span><span class="o">::</span><span class="no">ScalarType</span><span class="o">::</span><span class="no">BFloat16</span><span class="p">:</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_tuple</span><span class="p">(</span>
<span class="w">          </span><span class="k">sizeof</span><span class="p">(</span><span class="k">decltype</span><span class="p">(</span>
<span class="w">              </span><span class="n">c10</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">ScalarTypeToCPPType</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">::</span><span class="n">BFloat16</span><span class="o">&gt;::</span><span class="n">t</span><span class="p">)),</span>
<span class="w">          </span><span class="n">at</span><span class="o">::</span><span class="n">vec</span><span class="o">::</span><span class="n">Vectorized</span><span class="o">&lt;</span><span class="k">decltype</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">ScalarTypeToCPPType</span><span class="o">&lt;</span>
<span class="w">                                       </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">::</span><span class="n">BFloat16</span><span class="o">&gt;::</span><span class="n">t</span><span class="p">)</span><span class="o">&gt;::</span><span class="n">size</span><span class="p">());</span>
<span class="w">    </span><span class="k">case</span><span class="w"> </span><span class="no">at</span><span class="o">::</span><span class="no">ScalarType</span><span class="o">::</span><span class="no">Half</span><span class="p">:</span>
<span class="w">      </span><span class="k">return</span><span class="w"> </span><span class="n">std</span><span class="o">::</span><span class="n">make_tuple</span><span class="p">(</span>
<span class="w">          </span><span class="k">sizeof</span><span class="p">(</span><span class="k">decltype</span><span class="p">(</span>
<span class="w">              </span><span class="n">c10</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">ScalarTypeToCPPType</span><span class="o">&lt;</span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">::</span><span class="n">Half</span><span class="o">&gt;::</span><span class="n">t</span><span class="p">)),</span>
<span class="w">          </span><span class="n">at</span><span class="o">::</span><span class="n">vec</span><span class="o">::</span><span class="n">Vectorized</span><span class="o">&lt;</span><span class="k">decltype</span><span class="p">(</span><span class="n">c10</span><span class="o">::</span><span class="n">impl</span><span class="o">::</span><span class="n">ScalarTypeToCPPType</span><span class="o">&lt;</span>
<span class="w">                                       </span><span class="n">at</span><span class="o">::</span><span class="n">ScalarType</span><span class="o">::</span><span class="n">Half</span><span class="o">&gt;::</span><span class="n">t</span><span class="p">)</span><span class="o">&gt;::</span><span class="n">size</span><span class="p">());</span>
<span class="w">    </span><span class="k">default</span><span class="o">:</span>
<span class="w">      </span><span class="n">TORCH_CHECK</span><span class="p">(</span>
<span class="w">          </span><span class="nb">false</span><span class="p">,</span>
<span class="w">          </span><span class="s">&quot;Currently only floating and complex ScalarType are supported.&quot;</span><span class="p">);</span>
<span class="w">  </span><span class="p">}</span>
<span class="p">}</span>

<span class="p">}</span><span class="w"> </span><span class="c1">// anonymous namespace</span>

<span class="n">IPEX_REGISTER_DISPATCH</span><span class="p">(</span>
<span class="w">    </span><span class="n">get_cpp_typesize_and_vecsize_kernel_stub</span><span class="p">,</span>
<span class="w">    </span><span class="o">&amp;</span><span class="n">get_cpp_typesize_and_vecsize_kernel_impl</span><span class="p">);</span>

<span class="p">}</span><span class="w"> </span><span class="c1">// namespace cpu</span>
<span class="p">}</span><span class="w"> </span><span class="c1">// namespace torch_ipex</span>
</pre></div>
</div>
</section>
</section>
<section id="private-debug-apis">
<h2>Private Debug APIs<a class="headerlink" href="#private-debug-apis" title="Link to this heading"></a></h2>
<p>Here are three ISA-related private APIs that can help debugging::</p>
<ol class="simple">
<li><p>Query current ISA level.</p></li>
<li><p>Query max CPU supported ISA level.</p></li>
<li><p>Query max binary supported ISA level.</p></li>
</ol>
<blockquote>
<div><p><strong>Note:</strong></p>
<ol class="simple">
<li><p>Max CPU supported ISA level only depends on CPU features.</p></li>
<li><p>Max binary supported ISA level only depends on built complier version.</p></li>
<li><p>Current ISA level, it is the smaller of <code class="docutils literal notranslate"><span class="pre">max</span> <span class="pre">CPU</span> <span class="pre">ISA</span> <span class="pre">level</span></code> and <code class="docutils literal notranslate"><span class="pre">max</span> <span class="pre">binary</span> <span class="pre">ISA</span> <span class="pre">level</span></code>.</p></li>
</ol>
</div></blockquote>
<section id="example">
<h3>Example:<a class="headerlink" href="#example" title="Link to this heading"></a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>python
Python<span class="w"> </span><span class="m">3</span>.9.7<span class="w"> </span><span class="o">(</span>default,<span class="w"> </span>Sep<span class="w"> </span><span class="m">16</span><span class="w"> </span><span class="m">2021</span>,<span class="w"> </span><span class="m">13</span>:09:58<span class="o">)</span>
<span class="o">[</span>GCC<span class="w"> </span><span class="m">7</span>.5.0<span class="o">]</span><span class="w"> </span>::<span class="w"> </span>Anaconda,<span class="w"> </span>Inc.<span class="w"> </span>on<span class="w"> </span>linux
Type<span class="w"> </span><span class="s2">&quot;help&quot;</span>,<span class="w"> </span><span class="s2">&quot;copyright&quot;</span>,<span class="w"> </span><span class="s2">&quot;credits&quot;</span><span class="w"> </span>or<span class="w"> </span><span class="s2">&quot;license&quot;</span><span class="w"> </span><span class="k">for</span><span class="w"> </span>more<span class="w"> </span>information.
&gt;&gt;&gt;<span class="w"> </span>import<span class="w"> </span>intel_extension_for_pytorch._C<span class="w"> </span>as<span class="w"> </span>core
&gt;&gt;&gt;<span class="w"> </span>core._get_current_isa_level<span class="o">()</span>
<span class="s1">&#39;AMX&#39;</span>
&gt;&gt;&gt;<span class="w"> </span>core._get_highest_cpu_support_isa_level<span class="o">()</span>
<span class="s1">&#39;AMX&#39;</span>
&gt;&gt;&gt;<span class="w"> </span>core._get_highest_binary_support_isa_level<span class="o">()</span>
<span class="s1">&#39;AMX&#39;</span>
&gt;&gt;&gt;<span class="w"> </span>quit<span class="o">()</span>
</pre></div>
</div>
</section>
</section>
<section id="select-isa-level-manually">
<h2>Select ISA level manually.<a class="headerlink" href="#select-isa-level-manually" title="Link to this heading"></a></h2>
<p>By default, IPEX dispatches to the kernels with the maximum ISA level supported by the underlying CPU hardware. This ISA level can be overridden by the environment variable <code class="docutils literal notranslate"><span class="pre">ATEN_CPU_CAPABILITY</span></code> (same environment variable as PyTorch). The available values are {<code class="docutils literal notranslate"><span class="pre">avx2</span></code>, <code class="docutils literal notranslate"><span class="pre">avx512</span></code>, <code class="docutils literal notranslate"><span class="pre">avx512_vnni</span></code>, <code class="docutils literal notranslate"><span class="pre">avx512_bf16</span></code>, <code class="docutils literal notranslate"><span class="pre">amx</span></code>, <code class="docutils literal notranslate"><span class="pre">avx512_fp16</span></code>}. The effective ISA level would be the minimal level between <code class="docutils literal notranslate"><span class="pre">ATEN_CPU_CAPABILITY</span></code> and the maximum level supported by the hardware.</p>
<section id="id1">
<h3>Example:<a class="headerlink" href="#id1" title="Link to this heading"></a></h3>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>python<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;import intel_extension_for_pytorch._C as core;print(core._get_current_isa_level())&#39;</span>
AMX
$<span class="w"> </span><span class="nv">ATEN_CPU_CAPABILITY</span><span class="o">=</span>avx2<span class="w"> </span>python<span class="w"> </span>-c<span class="w"> </span><span class="s1">&#39;import intel_extension_for_pytorch._C as core;print(core._get_current_isa_level())&#39;</span>
AVX2
</pre></div>
</div>
<blockquote>
<div><p><strong>Note:</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">core._get_current_isa_level()</span></code> is an IPEX internal function used for checking the current effective ISA level. It is used for debugging purpose only and subject to change.</p>
</div></blockquote>
</section>
</section>
<section id="cpu-feature-check">
<h2>CPU feature check<a class="headerlink" href="#cpu-feature-check" title="Link to this heading"></a></h2>
<p>An addtional CPU feature check tool in the subfolder: <code class="docutils literal notranslate"><span class="pre">tests/cpu/isa</span></code></p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$<span class="w"> </span>cmake<span class="w"> </span>.
--<span class="w"> </span>The<span class="w"> </span>C<span class="w"> </span>compiler<span class="w"> </span>identification<span class="w"> </span>is<span class="w"> </span>GNU<span class="w"> </span><span class="m">11</span>.2.1
--<span class="w"> </span>The<span class="w"> </span>CXX<span class="w"> </span>compiler<span class="w"> </span>identification<span class="w"> </span>is<span class="w"> </span>GNU<span class="w"> </span><span class="m">11</span>.2.1
--<span class="w"> </span>Detecting<span class="w"> </span>C<span class="w"> </span>compiler<span class="w"> </span>ABI<span class="w"> </span>info
--<span class="w"> </span>Detecting<span class="w"> </span>C<span class="w"> </span>compiler<span class="w"> </span>ABI<span class="w"> </span>info<span class="w"> </span>-<span class="w"> </span><span class="k">done</span>
--<span class="w"> </span>Check<span class="w"> </span><span class="k">for</span><span class="w"> </span>working<span class="w"> </span>C<span class="w"> </span>compiler:<span class="w"> </span>/opt/rh/gcc-toolset-11/root/usr/bin/cc<span class="w"> </span>-<span class="w"> </span>skipped
--<span class="w"> </span>Detecting<span class="w"> </span>C<span class="w"> </span>compile<span class="w"> </span>features
--<span class="w"> </span>Detecting<span class="w"> </span>C<span class="w"> </span>compile<span class="w"> </span>features<span class="w"> </span>-<span class="w"> </span><span class="k">done</span>
--<span class="w"> </span>Detecting<span class="w"> </span>CXX<span class="w"> </span>compiler<span class="w"> </span>ABI<span class="w"> </span>info
--<span class="w"> </span>Detecting<span class="w"> </span>CXX<span class="w"> </span>compiler<span class="w"> </span>ABI<span class="w"> </span>info<span class="w"> </span>-<span class="w"> </span><span class="k">done</span>
--<span class="w"> </span>Check<span class="w"> </span><span class="k">for</span><span class="w"> </span>working<span class="w"> </span>CXX<span class="w"> </span>compiler:<span class="w"> </span>/opt/rh/gcc-toolset-11/root/usr/bin/c++<span class="w"> </span>-<span class="w"> </span>skipped
--<span class="w"> </span>Detecting<span class="w"> </span>CXX<span class="w"> </span>compile<span class="w"> </span>features
--<span class="w"> </span>Detecting<span class="w"> </span>CXX<span class="w"> </span>compile<span class="w"> </span>features<span class="w"> </span>-<span class="w"> </span><span class="k">done</span>
--<span class="w"> </span>Configuring<span class="w"> </span><span class="k">done</span>
--<span class="w"> </span>Generating<span class="w"> </span><span class="k">done</span>
--<span class="w"> </span>Build<span class="w"> </span>files<span class="w"> </span>have<span class="w"> </span>been<span class="w"> </span>written<span class="w"> </span>to:<span class="w"> </span>tests/cpu/isa
$<span class="w"> </span>make
<span class="o">[</span><span class="w"> </span><span class="m">33</span>%<span class="o">]</span><span class="w"> </span>Building<span class="w"> </span>CXX<span class="w"> </span>object<span class="w"> </span>CMakeFiles/cpu_features.dir/intel_extension_for_pytorch/csrc/cpu/isa/cpu_feature.cpp.o
<span class="o">[</span><span class="w"> </span><span class="m">66</span>%<span class="o">]</span><span class="w"> </span>Building<span class="w"> </span>CXX<span class="w"> </span>object<span class="w"> </span>CMakeFiles/cpu_features.dir/intel_extension_for_pytorch/csrc/cpu/isa/cpu_feature_main.cpp.o
<span class="o">[</span><span class="m">100</span>%<span class="o">]</span><span class="w"> </span>Linking<span class="w"> </span>CXX<span class="w"> </span>executable<span class="w"> </span>cpu_features
<span class="o">[</span><span class="m">100</span>%<span class="o">]</span><span class="w"> </span>Built<span class="w"> </span>target<span class="w"> </span>cpu_features
$<span class="w"> </span>./cpu_features
XCR0:<span class="w"> </span>00000000000602e7
os<span class="w"> </span>--&gt;<span class="w"> </span>avx:<span class="w"> </span><span class="nb">true</span>
os<span class="w"> </span>--&gt;<span class="w"> </span>avx2:<span class="w"> </span><span class="nb">true</span>
os<span class="w"> </span>--&gt;<span class="w"> </span>avx512:<span class="w"> </span><span class="nb">true</span>
os<span class="w"> </span>--&gt;<span class="w"> </span>amx:<span class="w"> </span><span class="nb">true</span>
mmx:<span class="w">                    </span><span class="nb">true</span>
sse:<span class="w">                    </span><span class="nb">true</span>
sse2:<span class="w">                   </span><span class="nb">true</span>
sse3:<span class="w">                   </span><span class="nb">true</span>
ssse3:<span class="w">                  </span><span class="nb">true</span>
sse4_1:<span class="w">                 </span><span class="nb">true</span>
sse4_2:<span class="w">                 </span><span class="nb">true</span>
aes_ni:<span class="w">                 </span><span class="nb">true</span>
sha:<span class="w">                    </span><span class="nb">true</span>
xsave:<span class="w">                  </span><span class="nb">true</span>
fma:<span class="w">                    </span><span class="nb">true</span>
f16c:<span class="w">                   </span><span class="nb">true</span>
avx:<span class="w">                    </span><span class="nb">true</span>
avx2:<span class="w">                   </span><span class="nb">true</span>
avx_vnni:<span class="w">                       </span><span class="nb">true</span>
avx512_f:<span class="w">                       </span><span class="nb">true</span>
avx512_cd:<span class="w">                      </span><span class="nb">true</span>
avx512_pf:<span class="w">                      </span><span class="nb">false</span>
avx512_er:<span class="w">                      </span><span class="nb">false</span>
avx512_vl:<span class="w">                      </span><span class="nb">true</span>
avx512_bw:<span class="w">                      </span><span class="nb">true</span>
avx512_dq:<span class="w">                      </span><span class="nb">true</span>
avx512_ifma:<span class="w">                    </span><span class="nb">true</span>
avx512_vbmi:<span class="w">                    </span><span class="nb">true</span>
avx512_vpopcntdq:<span class="w">                       </span><span class="nb">true</span>
avx512_4fmaps:<span class="w">                  </span><span class="nb">false</span>
avx512_4vnniw:<span class="w">                  </span><span class="nb">false</span>
avx512_vbmi2:<span class="w">                   </span><span class="nb">true</span>
avx512_vpclmul:<span class="w">                 </span><span class="nb">true</span>
avx512_vnni:<span class="w">                    </span><span class="nb">true</span>
avx512_bitalg:<span class="w">                  </span><span class="nb">true</span>
avx512_fp16:<span class="w">                    </span><span class="nb">true</span>
avx512_bf16:<span class="w">                    </span><span class="nb">true</span>
avx512_vp2intersect:<span class="w">                    </span><span class="nb">true</span>
amx_bf16:<span class="w">                       </span><span class="nb">true</span>
amx_tile:<span class="w">                       </span><span class="nb">true</span>
amx_int8:<span class="w">                       </span><span class="nb">true</span>
prefetchw:<span class="w">                      </span><span class="nb">true</span>
prefetchwt1:<span class="w">                    </span><span class="nb">false</span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright .</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   <jinja2.runtime.BlockReference object at 0x7f2c09336bc0> 
<p></p><div><a href='https://www.intel.com/content/www/us/en/privacy/intel-cookie-notice.html' data-cookie-notice='true'>Cookies</a><a href='https://www.intel.com/content/www/us/en/privacy/intel-privacy-notice.html'>| Privacy</a><a href='/#' data-wap_ref='dns' id='wap_dns'><small>| Your Privacy Choices</small><span style='height:10px;width:28px;display:inline-block;position:relative;'><svg style='position:absolute;width:28px;bottom:-2px;' version='1.1' id='Layer_1' xmlns='http://www.w3.org/2000/svg' xmlns:xlink='http://www.w3.org/1999/xlink' x='0px' y='0px' viewBox='0 0 30 14' xml:space='preserve'><title>California Consumer Privacy Act (CCPA) Opt-Out Icon</title><style type='text/css'> .st0 { fill-rule: evenodd; clip-rule: evenodd; fill: #FFFFFF; } .st1 { fill-rule: evenodd; clip-rule: evenodd; fill: #0066FF; } .st2 { fill: #FFFFFF; } .st3 { fill: #0066FF; } </style><g><g id='final---dec.11-2020_1_'><g id='_x30_208-our-toggle_2_' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2_2_' transform='translate(1275.000000, 200.000000)'><path class='st0' d='M7.4,12.8h6.8l3.1-11.6H7.4C4.2,1.2,1.6,3.8,1.6,7S4.2,12.8,7.4,12.8z'></path></g></g></g><g id='final---dec.11-2020'><g id='_x30_208-our-toggle' transform='translate(-1275.000000, -200.000000)'><g id='Final-Copy-2' transform='translate(1275.000000, 200.000000)'><path class='st1' d='M22.6,0H7.4c-3.9,0-7,3.1-7,7s3.1,7,7,7h15.2c3.9,0,7-3.1,7-7S26.4,0,22.6,0z M1.6,7c0-3.2,2.6-5.8,5.8-5.8 h9.9l-3.1,11.6H7.4C4.2,12.8,1.6,10.2,1.6,7z'></path><path id='x' class='st2' d='M24.6,4c0.2,0.2,0.2,0.6,0,0.8l0,0L22.5,7l2.2,2.2c0.2,0.2,0.2,0.6,0,0.8c-0.2,0.2-0.6,0.2-0.8,0 l0,0l-2.2-2.2L19.5,10c-0.2,0.2-0.6,0.2-0.8,0c-0.2-0.2-0.2-0.6,0-0.8l0,0L20.8,7l-2.2-2.2c-0.2-0.2-0.2-0.6,0-0.8 c0.2-0.2,0.6-0.2,0.8,0l0,0l2.2,2.2L23.8,4C24,3.8,24.4,3.8,24.6,4z'></path><path id='y' class='st3' d='M12.7,4.1c0.2,0.2,0.3,0.6,0.1,0.8l0,0L8.6,9.8C8.5,9.9,8.4,10,8.3,10c-0.2,0.1-0.5,0.1-0.7-0.1l0,0 L5.4,7.7c-0.2-0.2-0.2-0.6,0-0.8c0.2-0.2,0.6-0.2,0.8,0l0,0L8,8.6l3.8-4.5C12,3.9,12.4,3.9,12.7,4.1z'></path></g></g></g></g></svg></span></a><a href=https://www.intel.com/content/www/us/en/privacy/privacy-residents-certain-states.html data-wap_ref='nac' id='wap_nac'><small>| Notice at Collection</small></a></div><p></p><div>&copy; Intel Corporation. Intel, the Intel logo, and other Intel marks are trademarks of Intel Corporation or its subsidiaries. Other names and brands may be claimed as the property of others. No license (express or implied, by estoppel or otherwise) to any intellectual property rights is granted by this document, with the sole exception that code included in this document is licensed subject to the Zero-Clause BSD open source license (OBSD), <a href='http://opensource.org/licenses/0BSD'>http://opensource.org/licenses/0BSD</a>.</div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>