
/*
Fused Multi-Head Attention Forward

This is an implementation of the Flash Attention algorithm
(see: Dao et al., https://arxiv.org/pdf/2205.14135v2.pdf)
*/

#include "SDP/fmha_forward.hpp"
#include "SDP/fmha_forward_kernel.hpp"
#include "SDP/fmha_forward_v2.hpp"
#include "SDP/fmha_forward_v3.hpp"
#include "SDP/fmha_forward_v3_fp8kv.hpp"

// clang-format off
// macros to be filled in CMake
#define IMPL_T ${IMPL_T}
#define IMPL_KV_T ${IMPL_KV_T}
#define IMPL_ARCH_TAG gpu_arch::${IMPL_ARCH_TAG}
#define GPU_ARCH_${IMPL_ARCH_TAG}
#cmakedefine01 IMPL_KUSEALIBI
#cmakedefine01 IMPL_KUSEBIAS
#cmakedefine01 IMPL_KISCAUSAL
#cmakedefine01 IMPL_KSEQLAST
#cmakedefine01 IMPL_KISTRAINING
#cmakedefine01 IMPL_KISDROPOUT
#cmakedefine01 IMPL_KISVARLEN
#cmakedefine01 IMPL_KISLOCAL
#cmakedefine01 IMPL_KISFP8KV
// clang-format on

#define LAUNCH_V3_KERNEL_IMPL(kHeadPerKv)                                     \
  do {                                                                        \
    if constexpr (std::is_same_v<KV_T, uint8_t>) {                                                 \
      if (args.fp8_fmt_kv == fp8_format::E4M3) {                              \
        using fmha_forward_op_t = fmha_forward_v3_fp8kv_t<                    \
            fmha_policy,                                                      \
            T,                                                                \
            arch_tag,                                                         \
            kUseAlibi,                                                        \
            kUseBias,                                                         \
            kIsCausal,                                                        \
            kSeqLast,                                                         \
            kIsTraining,                                                      \
            kIsDropout,                                                       \
            kVarlen,                                                          \
            kIsLocal,                                                         \
            kHeadPerKv,                                                       \
            fp8_format::E4M3>;                                                \
                                                                              \
        sycl::nd_range<3> NdRange = fmha_forward_op_t::get_nd_range(          \
            args.num_batches * args.num_kv_heads, args.num_queries);          \
                                                                              \
        FmhaForwardKernelFunctor<fmha_forward_op_t, T, KV_T, false> kfn(   \
            args);                                                            \
                                                                              \
        return {[=](sycl::handler& cgh) { cgh.parallel_for(NdRange, kfn); }}; \
      } else if (args.fp8_fmt_kv == fp8_format::E5M2) {                       \
        using fmha_forward_op_t = fmha_forward_v3_fp8kv_t<                    \
            fmha_policy,                                                      \
            T,                                                                \
            arch_tag,                                                         \
            kUseAlibi,                                                        \
            kUseBias,                                                         \
            kIsCausal,                                                        \
            kSeqLast,                                                         \
            kIsTraining,                                                      \
            kIsDropout,                                                       \
            kVarlen,                                                          \
            kIsLocal,                                                         \
            kHeadPerKv,                                                       \
            fp8_format::E5M2>;                                                \
                                                                              \
        sycl::nd_range<3> NdRange = fmha_forward_op_t::get_nd_range(          \
            args.num_batches * args.num_kv_heads, args.num_queries);          \
                                                                              \
        FmhaForwardKernelFunctor<fmha_forward_op_t, T, KV_T, false> kfn(   \
            args);                                                            \
                                                                              \
        return {[=](sycl::handler& cgh) { cgh.parallel_for(NdRange, kfn); }}; \
      }                                                                       \
    } else {                                                                  \
      using fmha_forward_op_t = fmha_forward_v3_t<                            \
          fmha_policy,                                                        \
          T,                                                                  \
          arch_tag,                                                           \
          kUseAlibi,                                                          \
          kUseBias,                                                           \
          kIsCausal,                                                          \
          kSeqLast,                                                           \
          kIsTraining,                                                        \
          kIsDropout,                                                         \
          kVarlen,                                                            \
          kIsLocal,                                                           \
          kHeadPerKv>;                                                        \
                                                                              \
      sycl::nd_range<3> NdRange = fmha_forward_op_t::get_nd_range(            \
          args.num_batches * args.num_kv_heads, args.num_queries);            \
                                                                              \
      FmhaForwardKernelFunctor<fmha_forward_op_t, T, KV_T, false> kfn(args);        \
                                                                              \
      return {[=](sycl::handler& cgh) { cgh.parallel_for(NdRange, kfn); }};   \
    }                                                                         \
  } while (0)

namespace gpu::xetla {

namespace fmha {

// The launcher of fmha forward kernel
template <
    typename fmha_policy,
    typename T,
    typename KV_T,
    gpu_arch arch_tag,
    bool kUseAlibi,
    bool kUseBias,
    bool kIsCausal,
    bool kSeqLast,
    bool kIsTraining,
    bool kIsDropout,
    bool kVarlen,
    bool kIsLocal>
cgfs_t xetla_fmha_forward_kernel(
    const dispatch_fmha_forward_args_t<
        T,
        KV_T>& args) {
#ifdef SDP_DBG
  printf(
      "B, N, Nkv, F, T, H: %u, %u, %u, %u, %u, %u, UseAlibi: %d, UseBias: %d, IsCausal: %d, IsTraining: %d,"
      "IsDropout: %d, IsVarlen: %d, IsLocal: %d, alibi @ 0x%llx, uAT %d, uMT %d, strideB %d, strideN %d, strideF %d, dropout_prob %f, kSeqLast %d, isFP8kv %d\n",
      args.num_batches,
      args.num_heads,
      args.num_kv_heads,
      args.num_queries,
      args.num_keys,
      args.head_size,
      kUseAlibi,
      kUseBias,
      kIsCausal,
      kIsTraining,
      kIsDropout,
      kVarlen,
      kIsLocal,
      (unsigned long long)args.alibi,
      args.alibi_padded_block_size,
      args.attn_mask_padded_block_size,
      args.bias_strideB,
      args.bias_strideN,
      args.bias_strideF,
      args.dropout_prob,
      kSeqLast,
      args.is_fp8_kv);
#endif
  // fmha forward kernel

  if constexpr (
      std::is_same_v<fmha_policy, fmha_policy_1x64x64> ||
      std::is_same_v<fmha_policy, fmha_policy_1x128x64> ||
      std::is_same_v<fmha_policy, fmha_policy_1x64x128> ||
      std::is_same_v<fmha_policy, fmha_policy_1x128x128>) {
    // current 16xXXX policy will stuck on some shapes
    // std::is_same_v<fmha_policy, fmha_policy_16x64x64> ||
    // std::is_same_v<fmha_policy, fmha_policy_16x128x64> ||
    // std::is_same_v<fmha_policy, fmha_policy_16x64x128> ||
    // std::is_same_v<fmha_policy, fmha_policy_16x128x128>
    auto kHeadPerKv = args.num_heads / args.num_kv_heads;
    switch (kHeadPerKv) {
      case 1:
        LAUNCH_V3_KERNEL_IMPL(1);
      case 2:
        LAUNCH_V3_KERNEL_IMPL(2);
      case 3:
        LAUNCH_V3_KERNEL_IMPL(3);
      case 4:
        LAUNCH_V3_KERNEL_IMPL(4);
      case 5:
        LAUNCH_V3_KERNEL_IMPL(5);
      case 6:
        LAUNCH_V3_KERNEL_IMPL(6);
      case 7:
        LAUNCH_V3_KERNEL_IMPL(7);
      case 8:
        LAUNCH_V3_KERNEL_IMPL(8);
      default: {
        throw std::runtime_error("Unsupported kHeadPerKv value");
      }
    }
  } else if constexpr (!std::is_same_v<KV_T, uint8_t>) {
    using fmha_forward_op_t = fmha_forward_t<
        fmha_policy,
        T,
        KV_T,
        arch_tag,
        kUseAlibi,
        kUseBias,
        kIsCausal,
        kSeqLast,
        kIsTraining,
        kIsDropout,
        kVarlen,
        kIsLocal>;

    sycl::nd_range<3> NdRange = fmha_forward_op_t::get_nd_range(
        args.num_batches * args.num_heads, args.num_queries);

    FmhaForwardKernelFunctor<fmha_forward_op_t, T, KV_T, false> kfn(args);
    return {[=](sycl::handler& cgh) { cgh.parallel_for(NdRange, kfn); }};
  } else if constexpr (arch_tag == gpu_arch::XeHpc) {
    if (args.fp8_fmt_kv == fp8_format::E4M3) {
      using fmha_forward_op_t = fmha_forward_t<
          fmha_policy,
          T,
          KV_T,
          arch_tag,
          kUseAlibi,
          kUseBias,
          kIsCausal,
          kSeqLast,
          kIsTraining,
          kIsDropout,
          kVarlen,
          kIsLocal,
          fp8_format::E4M3>;

      sycl::nd_range<3> NdRange = fmha_forward_op_t::get_nd_range(
          args.num_batches * args.num_heads, args.num_queries);

      FmhaForwardKernelFunctor<fmha_forward_op_t, T, KV_T, false> kfn(args);
      return {[=](sycl::handler& cgh) { cgh.parallel_for(NdRange, kfn); }};
    } else if (args.fp8_fmt_kv == fp8_format::E5M2) {
      using fmha_forward_op_t = fmha_forward_t<
          fmha_policy,
          T,
          KV_T,
          arch_tag,
          kUseAlibi,
          kUseBias,
          kIsCausal,
          kSeqLast,
          kIsTraining,
          kIsDropout,
          kVarlen,
          kIsLocal,
          fp8_format::E5M2>;

      sycl::nd_range<3> NdRange = fmha_forward_op_t::get_nd_range(
          args.num_batches * args.num_heads, args.num_queries);

      FmhaForwardKernelFunctor<fmha_forward_op_t, T, KV_T, false> kfn(args);
      return {[=](sycl::handler& cgh) { cgh.parallel_for(NdRange, kfn); }};
    } else {
      throw std::runtime_error("Unsupported FP8 format for KV tensors");
    }
  }
}

#define INSTANTIATE_POLICY(policy)                                          \
  template cgfs_t xetla_fmha_forward_kernel<                                \
      policy,                                                               \
      IMPL_T,                                                               \
      IMPL_KV_T,                                                      \
      IMPL_ARCH_TAG,                                                        \
      IMPL_KUSEALIBI,                                                       \
      IMPL_KUSEBIAS,                                                        \
      IMPL_KISCAUSAL,                                                       \
      IMPL_KSEQLAST,                                                        \
      IMPL_KISTRAINING,                                                     \
      IMPL_KISDROPOUT,                                                      \
      IMPL_KISVARLEN,                                                       \
      IMPL_KISLOCAL>(                                                       \
      const dispatch_fmha_forward_args_t<                                   \
          IMPL_T,                                                           \
          IMPL_KV_T>& \
          args)

#if (IMPL_KISTRAINING && IMPL_KISDROPOUT && defined(GPU_ARCH_XeHpc))
INSTANTIATE_POLICY(fmha_policy_128x128x64);
INSTANTIATE_POLICY(fmha_policy_128x128x128);
INSTANTIATE_POLICY(fmha_policy_128x128x256);
INSTANTIATE_POLICY(fmha_policy_64x128x512);
#endif

// chunked prefill policy support only pvc now
#if defined(GPU_ARCH_XeHpc)
INSTANTIATE_POLICY(fmha_policy_1x64x64);
INSTANTIATE_POLICY(fmha_policy_1x64x128);
INSTANTIATE_POLICY(fmha_policy_1x128x64);
INSTANTIATE_POLICY(fmha_policy_1x128x128);
INSTANTIATE_POLICY(fmha_policy_64x64x64);
INSTANTIATE_POLICY(fmha_policy_64x64x128);
INSTANTIATE_POLICY(fmha_policy_64x64x256);
INSTANTIATE_POLICY(fmha_policy_64x64x512);
#endif

INSTANTIATE_POLICY(fmha_policy_8x128x64);
INSTANTIATE_POLICY(fmha_policy_64x128x64);

#if (!IMPL_KISDROPOUT && defined(GPU_ARCH_XeLpg))
#define COMMA ,
INSTANTIATE_POLICY(fmha_policy_1x256x128);
INSTANTIATE_POLICY(fmha_policy_1x512x128);
// INSTANTIATE_POLICY(std::integral_constant<int COMMA 64>);
// INSTANTIATE_POLICY(std::integral_constant<int COMMA 128>);
#endif

INSTANTIATE_POLICY(fmha_policy_8x256x128);
INSTANTIATE_POLICY(fmha_policy_8x512x128);

#if defined(GPU_ARCH_XeLpg)
INSTANTIATE_POLICY(fmha_policy_32x128x128);
#else
INSTANTIATE_POLICY(fmha_policy_64x128x128);
#endif

#if defined(GPU_ARCH_XeLpg)
#else
INSTANTIATE_POLICY(fmha_policy_8x256x256);
INSTANTIATE_POLICY(fmha_policy_64x128x256);
#if defined(GPU_ARCH_XeHpc)
INSTANTIATE_POLICY(fmha_policy_64x256x256);
#if !(IMPL_KISTRAINING && IMPL_KISDROPOUT)
INSTANTIATE_POLICY(fmha_policy_64x128x512);
#endif
#endif
#endif
} // namespace fmha
} // namespace gpu::xetla
